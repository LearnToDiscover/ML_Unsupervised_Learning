<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><title>Machine Learning - Unsupervised: Dimensionality Reduction</title><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="stylesheet" type="text/css" href="assets/styles.css"><script src="assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png"><link rel="manifest" href="site.webmanifest"><link rel="mask-icon" href="safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" content="#ffffff"></head><body>
    <header id="top" class="navbar navbar-expand-md navbar-light bg-white top-nav l2d"><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-6">
      <div class="large-logo">
        <img alt="l2d" src="assets/images/l2d-logo.svg"></div>
    </div>
    <div class="selector-container">


      <div class="dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Learner View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1"><li><button class="dropdown-item" type="button" onclick="window.location.href='instructor/03-dimensionality.html';">Instructor View</button></li>
        </ul></div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl navbar-light bg-white bottom-nav l2d" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="l2d" src="assets/images/l2d-logo-sm.svg"></div>
    <div class="lesson-title-md">
      Machine Learning - Unsupervised
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0"><li class="nav-item">
          <span class="lesson-title">
            Machine Learning - Unsupervised
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="reference.html#glossary">Glossary</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="profiles.html">Learner Profiles</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown"></ul></li>
      </ul></div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a class="btn btn-primary" href="aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div><!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Machine Learning - Unsupervised
</div>

<aside class="col-md-12 lesson-progress"><div style="width: 67%" class="percentage">
    67%
  </div>
  <div class="progress l2d">
    <div class="progress-bar l2d" role="progressbar" style="width: 67%" aria-valuenow="67" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar" style="display: flex">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle sticky-top" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner sticky-top">
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Learner View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="instructor/03-dimensionality.html">Instructor View</a>
                      </div>
                    </div>
                  </div><!--/div.accordion-item-->
                </div><!--/div.accordion-flush-->
              </div><!--div.sidenav-view-selector -->
            </div><!--/div.col -->

            <hr></div><!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Setup</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="01-clustering-intro.html">1. Clustering Introduction</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="02-clustering-image.html">2. Clustering Images</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlushcurrent">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-headingcurrent">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapsecurrent" aria-expanded="true" aria-controls="flush-collapsecurrent">
        <span class="visually-hidden">Current Chapter</span>
        <span class="current-chapter">
        3. Dimensionality Reduction
        </span>
      </button>
    </div><!--/div.accordion-header-->

    <div id="flush-collapsecurrent" class="accordion-collapse collapse show" aria-labelledby="flush-headingcurrent" data-bs-parent="#accordionFlushcurrent">
      <div class="accordion-body">
        <ul><li><a href="#introduction">Introduction</a></li>
<li><a href="#example-predicting-outcome-from-large-scale-genetic-or-molecular-studies">Example: Predicting outcome from large-scale genetic or molecular
studies</a></li>
<li><a href="#principal-component-analysis">Principal Component Analysis</a></li>
<li><a href="#pca-of-the-gene-expression-data">PCA of the Gene Expression Data</a></li>
<li><a href="#when-does-pca-fail">When does PCA fail?</a></li>
<li><a href="#resources">Resources</a></li>
        </ul></div><!--/div.accordion-body-->
    </div><!--/div.accordion-collapse-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width"><div class="accordion accordion-flush resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul><li>
                        <a href="key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="reference.html#glossary">Glossary</a>
                      </li>
                      <li>
                        <a href="profiles.html">Learner Profiles</a>
                      </li>

                    </ul></div>
                </div>
              </div>
            </div>
            <hr class="half-width resources"><a href="aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none"><div class="d-grid gap-1">

            </div>
          </div><!-- /div.accordion -->
        </div><!-- /div.sidebar-inner -->
      </nav></div><!-- /div.sidebar -->
  </div><!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-instructor.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <nav class="lesson-content mx-md-4" aria-label="Previous and Next Chapter"><!-- content for small screens --><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="02-clustering-image.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="index.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="02-clustering-image.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Clustering Images
        </a>
        <a class="chapter-link float-end" href="index.html" rel="next">
          Home
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
      <hr></nav><main id="main-content" class="main-content"><div class="container lesson-content">
        <h1>Dimensionality Reduction</h1>
        <p>Last updated on 2025-01-27 |

        <a href="https://github.com/carpentries/workbench-template-md/edit/main/episodes/03-dimensionality.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>



        <div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>

        

<p><a href="https://drive.usercontent.google.com/u/1/uc?id=1QpuoibEsIfbllix2dElYpfl5TBe700EP&amp;export=download" class="external-link"><strong>Download
Chapter notebook (ipynb)</strong></a></p>
<p><a href="https://drive.usercontent.google.com/u/1/uc?id=1vGeUNnFSlcAOVNkfB5OEFMCWL3BQY6Mq&amp;export=download" class="external-link"><strong>Download
Chapter PDF</strong></a></p>
<p><a href="https://docs.google.com/forms/d/e/1FAIpQLSdr0capF7jloJhPH3Pki1B3LZoKOG16poOpuVJ7SL2LkwLHQA/viewform?pli=1" class="external-link"><span style="color: rgb(255, 0, 0);"><strong>Mandatory Lesson Feedback
Survey</strong></span></a></p>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul><li>Why is it important to perform dimensionality reduction?</li>
<li>How is dimensionality reduction performed?</li>
<li>How PCA is used to determine variance?</li>
<li>When does PCA fail?</li>
</ul></div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul><li>Understanding high-dimensional datasets.</li>
<li>Applying dimensionality reduction to extract features.</li>
<li>Learning to use PCA for dimensionality reduction.</li>
<li>Estimating the optimal number of features.</li>
</ul></div>
</div>
</div>
</div>
</div>
<br><p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/ql80ZoyiWKY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</p>
<br><p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/yLjzGlZ19pM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</p>
<br><p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/E0YytUSJsQY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</p>
<p><br></p>
<div id="prereq1" class="callout prereq">
<div class="callout-square">
<i class="callout-icon" data-feather="check"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Prerequisite</h3>
<div class="callout-content">
<ul><li><a href="01-clustering-intro.html">Clustering Introduction</a></li>
<li><a href="02-clustering-image.html">Clustering Images</a></li>
</ul></div>
</div>
</div>
<div class="section level3">
<h3 id="code-preparation"><strong>Code Preparation</strong><a class="anchor" aria-label="anchor" href="#code-preparation"></a></h3>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> reshape, append, mean, pi, linspace, var, array, cumsum, arange</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">from</span> numpy.random <span class="im">import</span> seed, multivariate_normal, binomial</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">from</span> numpy.linalg <span class="im">import</span> norm</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="im">from</span> math <span class="im">import</span> cos, sin, atan2</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="im">from</span> matplotlib.pyplot <span class="im">import</span> subplots, xlabel, ylabel, scatter, title</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="im">from</span> matplotlib.pyplot <span class="im">import</span> figure, plot, xticks, xlabel, ylabel, suptitle, show</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="im">import</span> glob</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a><span class="im">from</span> pandas <span class="im">import</span> read_csv, DataFrame</span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a><span class="im">import</span> csv</span></code></pre>
</div>
</div>
<section><h2 class="section-heading" id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a></h2>
<hr class="half-width"><p style="text-align: justify;">
Clinical and scientific datasets often contain measurements of many
physiological variables, expression of many genes or metabolic markers.
These datasets, where each individual observation/sample is described by
many variables are referred to as <strong>high-dimensional</strong>.
However, some of these features can be highly correlated or redundant,
and we sometimes want a more condensed description of the data. This
process of describing the data by a reduced number of new combined
features that reduces redundancy, is called <strong>dimensionality
reduction</strong>.
</p>
<figure><img src="fig/goal.png" alt="Goal of decomposition" class="figure mx-auto d-block"><div class="figcaption">Goal of decomposition</div>
</figure><p>There are several reasons for performing dimensionality reduction,
both for data exploration and for pre-processing:</p>
<ul><li><p>Data compression, to optimize memory usage/loading speed and
visualisation of “important features”.</p></li>
<li><p>To find the individual variable or combinations of variables that
give a condensed but accurate description of the data (sometimes also
called feature extraction).</p></li>
<li><p>It can also be used for “denoising” - removing features that only
account for small variability.</p></li>
<li><p>It is also useful to perform this step before other supervised
(regression, classification) or unsupervised learning procedures (like
clustering), by reducing data to “main features” to improve robustness
of learning, especially with a limited number of independent
observations.</p></li>
</ul></section><section><h2 class="section-heading" id="example-predicting-outcome-from-large-scale-genetic-or-molecular-studies">Example: Predicting outcome from large-scale genetic or molecular
studies<a class="anchor" aria-label="anchor" href="#example-predicting-outcome-from-large-scale-genetic-or-molecular-studies"></a></h2>
<hr class="half-width"><p style="text-align: justify;">
Clinical research can involve doing an exhaustive measurement of all
tentative diagnostic markers, for example levels of various cytokines
and hormones, or searching for genetic markers of a disease. There is
good reason to expect that these share regulatory/signalling mechanisms,
and that their interaction determines disease outcome. Building a
regression model including all measurements is problematic because of
multicollinearity and the demand for a much larger number of
observations.
</p>
<p style="text-align: justify;">
By performing dimensionality reduction, we first find features that
capture the major patterns of covariation of these factors in the sample
population. Then we will use these compact features, rather than
individual measurements, to train our classifier or regression model, to
study outcomes. With fewer features, we train models with less data.
</p>
<div class="section level3">
<h3 id="loading-example-data-childhood-onset-rheumatic-disease-gene-expression-profile"><strong>Loading example data: Childhood Onset Rheumatic Disease gene
expression profile</strong><a class="anchor" aria-label="anchor" href="#loading-example-data-childhood-onset-rheumatic-disease-gene-expression-profile"></a></h3>
<p style="text-align: justify;">
<a href="https://www.ebi.ac.uk/arrayexpress/experiments/E-GEOD-11083/" class="external-link">Gene
expression data</a> (for selected markers) is available for 83
individuals, including some patients with varying forms of early-onset
rheumatic disease (available at EMBL-Bioinformatics database, also
included in folder). This is a typical example of high dimensional
datasets, with transcript levels for &gt;50,000 genes measured!
</p>
<p>Here, the data are read from individual files in the ‘Dataset’ folder
and combined into a Pandas dataframe called ‘geneData’.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="co"># Load all data</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>nGenes <span class="op">=</span> <span class="dv">1000</span> <span class="co"># keep only these genes for now</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>filectr<span class="op">=</span><span class="dv">0</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>datapath <span class="op">=</span> Path(<span class="st">"data"</span>)    <span class="co">#/relative/path/to/folder/with/datsets</span></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a><span class="cf">for</span> txt <span class="kw">in</span> datapath.glob(<span class="st">"*.txt"</span>):</span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a>    b<span class="op">=</span>read_csv(txt, sep<span class="op">=</span><span class="st">'</span><span class="ch">\t</span><span class="st">'</span>, usecols<span class="op">=</span>[<span class="dv">1</span>])</span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>    b<span class="op">=</span>reshape(b.values, (<span class="dv">1</span>,<span class="dv">54675</span>)) <span class="co"># each file is 1 observation sample i.e. 1 row.</span></span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a>    <span class="cf">if</span> filectr<span class="op">==</span><span class="dv">0</span>:</span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a>        coln <span class="op">=</span>  read_csv(txt, sep<span class="op">=</span><span class="st">'</span><span class="ch">\t</span><span class="st">'</span>, usecols<span class="op">=</span>[<span class="dv">0</span>])</span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a>        a <span class="op">=</span> b</span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb2-15"><a href="#cb2-15" tabindex="-1"></a>        a<span class="op">=</span>append( a, b, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb2-16"><a href="#cb2-16" tabindex="-1"></a>    filectr<span class="op">+=</span><span class="dv">1</span></span>
<span id="cb2-17"><a href="#cb2-17" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" tabindex="-1"></a><span class="co"># convert 2D array to dataframe</span></span>
<span id="cb2-19"><a href="#cb2-19" tabindex="-1"></a>geneData <span class="op">=</span> DataFrame( data<span class="op">=</span>a[:,<span class="bu">range</span>(nGenes)]<span class="op">-</span> mean(a[:,<span class="bu">range</span>(nGenes)], axis<span class="op">=</span><span class="dv">0</span>), columns <span class="op">=</span> coln.values[<span class="bu">range</span>(nGenes)])</span></code></pre>
</div>
The dataframe has 80 samples and 1000 features.
<p style="text-align: justify;">
Apart from the large number of variables (compared to number of
samples), we also have strongly correlated expression of various
markers. We can see that by plotting the correlation matrix of the
dataframe.
</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>fig, ax <span class="op">=</span> subplots(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">8</span>))</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>im <span class="op">=</span> ax.matshow(geneData.corr())</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>cbar <span class="op">=</span> fig.colorbar(im, shrink<span class="op">=</span><span class="fl">0.82</span>)</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>xlabel(<span class="st">'Locus #'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>ylabel(<span class="st">'Locus #'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span></span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>show()</span></code></pre>
</div>
<figure><img src="fig/03-dimensionality-rendered-unnamed-chunk-3-1.png" width="768" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p style="text-align: justify;">
The little yellow blocks indicate that various features are strongly
correlation (values around 1). Further analyses (e.g. checking if we
find clusters mapping to healthy individuals and patients, respectively,
or whether patients with different patterns of marker expression can be
identified for future diagnostic purposes) can be thus be improved if we
perform a dimensionality reduction, i.e. decrease the number of features
while maintaining relevant information.
</p>
</div>
</section><section><h2 class="section-heading" id="principal-component-analysis">Principal Component Analysis<a class="anchor" aria-label="anchor" href="#principal-component-analysis"></a></h2>
<hr class="half-width"><p style="text-align: justify;">
Principal component analysis (PCA) is one of the most commonly used
dimensionality reduction methods. Instead of describing the data using
the original measurements, it finds a new set of ranked features (the
“basis”). The new “features” are linear combinations of the original
variables, such that they are all <em>orthogonal</em> to each other, and
the “importance” of a feature (called the <strong>principal
component</strong>) is generally given by how much of the variability in
the dataset it captures (the so-called <strong>explained
variance</strong>). Thus, we first transform the data from the original
variables into a new set of features or principal components (PCs).
</p>
<p>To get a lower dimensional description of the data, we retain only
the top features (PCs) such that the largest variations or ‘patterns’ in
the dataset are preserved.</p>
<p style="text-align: justify;">
To apply <strong>PCA</strong> from <code>Scikit-learn</code>, we need to
specify the number of components (<code>n_components</code>) we want to
reduce the data to. <code>n_components</code> must be less than the
<em>rank</em> of the dataset, i.e. less than the smaller of the number
of samples and number of variables measured.
</p>
<div class="section level3">
<h3 id="simple-example-with-2d-generated-dataset"><strong>Simple example with 2D generated dataset</strong><a class="anchor" aria-label="anchor" href="#simple-example-with-2d-generated-dataset"></a></h3>
<p>To first explore what dimensionality reduction with PCA looks like,
we will work with a simple 2-dimensional (2D) dataset, where we want to
find a 1D description instead.</p>
<p>We start with generating some observations from a two-dimensional
(multivariate) Gaussian distribution, with mean and covariance
specified.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>RND_SEED <span class="op">=</span> <span class="dv">7890</span></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>seed(RND_SEED)</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a><span class="co"># Set up 2D multivariate Gaussians</span></span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>means <span class="op">=</span> [<span class="dv">20</span>, <span class="dv">20</span>]</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>cov_mat <span class="op">=</span> [[<span class="dv">1</span>, <span class="fl">.85</span>], [<span class="fl">.85</span>, <span class="fl">1.5</span>]] <span class="co"># 2x2 covariance matrix must be symmetric and positive-semidefinite(&gt;=0)</span></span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a><span class="co"># Generate data</span></span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a>nSamples <span class="op">=</span> <span class="dv">300</span></span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a>data <span class="op">=</span> multivariate_normal(means, cov_mat, nSamples)</span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a><span class="bu">print</span>(data.shape)</span></code></pre>
</div>
<p>(300, 2)</p>
<p>We can check the distribution of the two features in histograms, and
we can see how the two data dimensions relate to each other in the
scatter plot.</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="co"># Center data</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>zdata <span class="op">=</span> data <span class="op">-</span> mean(data, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>fig, ax <span class="op">=</span> subplots(ncols<span class="op">=</span><span class="dv">3</span>, nrows<span class="op">=</span><span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">4</span>))</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>ax[<span class="dv">0</span>].hist(zdata[:, <span class="dv">0</span>])<span class="op">;</span></span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">'Dim 1'</span>, fontsize<span class="op">=</span><span class="dv">16</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)<span class="op">;</span></span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">'Values'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span></span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="st">'Frequency'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span></span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a>ax[<span class="dv">1</span>].hist(zdata[:, <span class="dv">1</span>])<span class="op">;</span></span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">'Dim 2'</span>, fontsize<span class="op">=</span><span class="dv">16</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)<span class="op">;</span></span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">'Values'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span></span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" tabindex="-1"></a>ax[<span class="dv">2</span>].scatter(zdata[:, <span class="dv">0</span>], zdata[:, <span class="dv">1</span>])<span class="op">;</span></span>
<span id="cb5-17"><a href="#cb5-17" tabindex="-1"></a>ax[<span class="dv">2</span>].set_title(<span class="st">'Sample Data'</span>, fontsize<span class="op">=</span><span class="dv">16</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb5-18"><a href="#cb5-18" tabindex="-1"></a>ax[<span class="dv">2</span>].set_xlabel(<span class="st">'Dim 1'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span></span>
<span id="cb5-19"><a href="#cb5-19" tabindex="-1"></a>ax[<span class="dv">2</span>].set_ylabel(<span class="st">'Dim 2'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span></span>
<span id="cb5-20"><a href="#cb5-20" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" tabindex="-1"></a>show()</span></code></pre>
</div>
<figure><img src="fig/03-dimensionality-rendered-unnamed-chunk-5-3.png" width="1440" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><div id="note" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="note" class="callout-inner">
<h3 class="callout-title">Note</h3>
<div class="callout-content">
<p><em>The last plot, with the samples plotted as scatter in the space
of measured variables is called a “state space plot”. The main axes
correspond to individual variables.</em></p>
</div>
</div>
</div>
<p style="text-align: justify;">
Now if we want to represent data by a single feature, we can see that
the individual variables are not ideal. If we only keep Dim 1, we lose
substantial variability along Dim 2. We see that the data points are
stretched along the main diagonal. This implies that the two variables
co-vary to some extent. Thus, a linear combination of the two might
capture this relevant pattern.
</p>
A linear combination corresponds to a vector (or direction) in this 2D
state space.
<p style="text-align: justify;">
For example, the x-axis vector (1, 0) = [1 x Dim1 + 0 x Dim2], i.e. it
contains Dim 1 only. More generally, (a, b) represents the linear
combination [a x Dim1 + b x Dim2], i.e. a mixture of the two variables.
Typically, we ensure that the length of these vectors is 1, to avoid
artefactual rescaling of data.
</p>
</div>
<div class="section level3">
<h3 id="pca-as-projection-or-rotation-of-basis"><strong>PCA as projection or rotation of basis</strong><a class="anchor" aria-label="anchor" href="#pca-as-projection-or-rotation-of-basis"></a></h3>
<p style="text-align: justify;">
Instead of representing data by their values along Dim 1 and Dim 2, we
now try to describe them along different directions. We do so by
<em>projecting</em> 2D data onto that vector. To quantify the importance
of each direction, we measure the variance of the projected samples.
</p>
<p style="text-align: justify;">
Let us examine different vectors that point at various angles compared
to the x-axis. Remember that the vector denoted by the angle = <em>(
cos(angle), sin(angle) )</em> denotes the following linear combination
of the 2D sample d=[d1, d2]:
</p>
<p>f(d1,d2) = cos(angle) * d1 + sin(angle) * d2</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="co">## Centre data at origin by subtracting mean</span></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>zdata <span class="op">=</span> data <span class="op">-</span> mean(data, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a><span class="co"># Define a feature as a direction in the 2D space using the angle</span></span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>angle <span class="op">=</span> pi<span class="op">/</span><span class="dv">4</span></span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a><span class="co"># Obtain the vector</span></span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>vec <span class="op">=</span> [cos(angle), sin(angle)] <span class="co"># has norm=1 by definition, otherwise:  vec = vec/norm(vec)</span></span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a><span class="co"># Project 2D data onto this single dimension</span></span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a>dim1_data <span class="op">=</span> zdata.dot(vec)</span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" tabindex="-1"></a>fig, ax <span class="op">=</span> subplots(ncols<span class="op">=</span><span class="dv">2</span>, nrows<span class="op">=</span><span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">6</span>))</span>
<span id="cb6-15"><a href="#cb6-15" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" tabindex="-1"></a><span class="co"># Plot data and the feature direction</span></span>
<span id="cb6-17"><a href="#cb6-17" tabindex="-1"></a>ax[<span class="dv">0</span>].scatter(zdata[:, <span class="dv">0</span>], zdata[:, <span class="dv">1</span>])<span class="op">;</span></span>
<span id="cb6-18"><a href="#cb6-18" tabindex="-1"></a>ax[<span class="dv">0</span>].quiver([<span class="dv">0</span>], [<span class="dv">0</span>], vec[<span class="dv">0</span>], vec[<span class="dv">1</span>], scale<span class="op">=</span><span class="dv">3</span>)<span class="op">;</span></span>
<span id="cb6-19"><a href="#cb6-19" tabindex="-1"></a></span>
<span id="cb6-20"><a href="#cb6-20" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">'Sample Data'</span>, fontsize<span class="op">=</span><span class="dv">16</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb6-21"><a href="#cb6-21" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">'Dim 1'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span> ax[<span class="dv">0</span>].set_xlim([<span class="op">-</span><span class="dv">4</span>,<span class="dv">4</span>])<span class="op">;</span></span>
<span id="cb6-22"><a href="#cb6-22" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="st">'Dim 2'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span> ax[<span class="dv">0</span>].set_ylim([<span class="op">-</span><span class="dv">4</span>,<span class="dv">4</span>])<span class="op">;</span></span>
<span id="cb6-23"><a href="#cb6-23" tabindex="-1"></a></span>
<span id="cb6-24"><a href="#cb6-24" tabindex="-1"></a><span class="co"># Plot histogram of new projected data</span></span>
<span id="cb6-25"><a href="#cb6-25" tabindex="-1"></a>ax[<span class="dv">1</span>].hist(dim1_data, bins <span class="op">=</span> linspace(<span class="op">-</span><span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">9</span>))<span class="op">;</span></span>
<span id="cb6-26"><a href="#cb6-26" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">'Along vec'</span>, fontsize<span class="op">=</span><span class="dv">16</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)<span class="op">;</span></span>
<span id="cb6-27"><a href="#cb6-27" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">'Values'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span></span>
<span id="cb6-28"><a href="#cb6-28" tabindex="-1"></a>ax[<span class="dv">1</span>].set_ylabel(<span class="st">'Frequency'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span></span>
<span id="cb6-29"><a href="#cb6-29" tabindex="-1"></a></span>
<span id="cb6-30"><a href="#cb6-30" tabindex="-1"></a>show()</span></code></pre>
</div>
<figure><img src="fig/03-dimensionality-rendered-unnamed-chunk-6-5.png" width="1440" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><div id="do-it-yourself" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="do-it-yourself" class="callout-inner">
<h3 class="callout-title">Do it Yourself</h3>
<div class="callout-content">
<p>Change the angle in the code above and see how distribution of
projected data changes. Use e.g. 0, <span class="math inline">\(\pi\)</span> /2, 2/3*<span class="math inline">\(\pi\)</span>, and <span class="math inline">\(\pi\)</span>. Try to estimate how much the
variance changes.</p>
</div>
</div>
</div>
<p style="text-align: justify;">
Now let’s formally calculate the variance by projecting along a set of
different directions. We use angles between 0 and <span class="math inline">\(\pi\)</span> (i.e. between 0 and 180 degrees). For
each direction (projection) we calculate the variance and plot it
against the angle.
</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>allAngles <span class="op">=</span> linspace( start<span class="op">=</span><span class="dv">0</span>, stop<span class="op">=</span>pi, num<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>allVar <span class="op">=</span> []</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a><span class="cf">for</span> angle <span class="kw">in</span> allAngles:</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>    vec <span class="op">=</span> array([cos(angle), sin(angle)]) <span class="co">#Unit length vector on which data will be projected</span></span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>    dim1_data <span class="op">=</span> data.dot(vec)</span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a>    allVar.append( var(dim1_data))</span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a><span class="co"># Plot variance captured by different features</span></span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a>scatter( allAngles, allVar )<span class="op">;</span></span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a>title(<span class="st">'Variance along different directions'</span>, fontsize<span class="op">=</span><span class="dv">16</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a>xlabel(<span class="st">'Direction (angle in radians)'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span></span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a>ylabel(<span class="st">'Variance of projected data'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span></span></code></pre>
</div>
<figure><img src="fig/03-dimensionality-rendered-unnamed-chunk-7-7.png" width="672" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p style="text-align: justify;">
It can be seen there is a particular direction that retains maximal
variance. This would be the optimal feature to give a 1D description of
the 2D data while retaining maximum variability. PCA is the method that
directly find this optimal direction. This makes it especially powerful
for high dimensional datasets.
</p>
</div>
<div class="section level3">
<h3 id="find-1-d-description-using-pca"><strong>Find 1-D description using PCA</strong><a class="anchor" aria-label="anchor" href="#find-1-d-description-using-pca"></a></h3>
<p style="text-align: justify;">
Let us now use dimensionality reduction <code>PCA</code> to directly
find the direction that captures maximal variance. For an n-dimensional
dataset, PCA finds a set of <em>n</em> new features, ranked by the
variance along each feature. See the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" class="external-link">Scikit-learn
documentation</a> for information.
</p>
<p>To get the best 1D representation of data, we instantiate the class
with one component (<em>n_components</em> = 1) to be returned, and
transform the data to the feature space of that component.</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a><span class="co"># Initialize the PCA model</span></span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>pcaResults <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">1</span>, whiten<span class="op">=</span><span class="va">False</span>) <span class="co"># specify no. of components, and whether to standardize</span></span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a><span class="co"># Fit to data</span></span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a>pcaResults <span class="op">=</span> pcaResults.fit(data)</span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a><span class="co"># Transform the data, using our learned PCA representation</span></span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a>dim1_data <span class="op">=</span> pcaResults.transform(data)</span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a></span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a><span class="bu">print</span>( <span class="st">"Size of new dataset: "</span>, dim1_data.shape )</span></code></pre>
</div>
<p>Size of new dataset: (300, 1)</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="ex">Size</span> of new dataset:  <span class="er">(</span><span class="ex">300,</span> 1<span class="kw">)</span></span></code></pre>
</div>
<p>From the fitted model ‘pcaResults’ we can now get:</p>
<ul><li><p>The fractional variance using
<code>explained_variance_ratio_</code>. It tells us what fraction of the
total variance is retained by th reduced 1D description of the
data.</p></li>
<li><p>The direction (angle) of this first PC as calculate from the
arcus tangens <code>atan</code> of the vector. You can compare it
directly to the angle in the above plot.</p></li>
</ul><div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="co"># How much variance was captured compared to original data?</span></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Fractional variance captured by first PC: </span><span class="sc">{:1.4f}</span><span class="st">."</span>.<span class="bu">format</span>(pcaResults.explained_variance_ratio_[<span class="dv">0</span>]))</span></code></pre>
</div>
<p>Fractional variance captured by first PC: 0.8722.</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>vec1 <span class="op">=</span> pcaResults.components_[<span class="dv">0</span>,:]</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Direction of PC1 is at angle = </span><span class="sc">{:1.2f}</span><span class="st"> radians"</span>.<span class="bu">format</span>(atan2(vec1[<span class="dv">1</span>],vec1[<span class="dv">0</span>])))</span></code></pre>
</div>
<p>Direction of PC1 is at angle = 0.99 radians</p>
<div id="do-it-yourself-1" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="do-it-yourself-1" class="callout-inner">
<h3 class="callout-title">Do it Yourself</h3>
<div class="callout-content">
<p>Exercise (DIY): Change the covariance of the synthetic data, and
repeat the above steps. Try to infer the relationship between the shape
of the scatter, the strength of correlation between the two dimensions,
and the variance captured by the first PC.</p>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="find-2-d-description-using-pca"><strong>Find 2-D description using PCA</strong><a class="anchor" aria-label="anchor" href="#find-2-d-description-using-pca"></a></h3>
<p style="text-align: justify;">
Our dataset has two features. We can thus obtain two principal
components. The two components must be orthogonal to each other (form a
right angle). Let us get the corresponding vectors and plot them on the
scatterplot.
</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="co"># Initialize the PCA model</span></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>pcaResults <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">2</span>, whiten<span class="op">=</span><span class="va">False</span>) <span class="co"># specify no. of components, and whether to standardize</span></span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a><span class="co"># Fit to data</span></span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a>pcaResults <span class="op">=</span> pcaResults.fit(data)</span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a><span class="co"># Transform the data, using our learned PCA representation</span></span>
<span id="cb12-8"><a href="#cb12-8" tabindex="-1"></a>dim1_data <span class="op">=</span> pcaResults.transform(data)</span>
<span id="cb12-9"><a href="#cb12-9" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" tabindex="-1"></a><span class="bu">print</span>( <span class="st">"Size of new dataset: "</span>, dim1_data.shape )</span></code></pre>
</div>
<p>Size of new dataset: (300, 2)</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="bu">print</span>(pcaResults.components_)</span></code></pre>
</div>
<p>[[ 0.54864625 0.8360546 ] [ 0.8360546 -0.54864625]]</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>vec1 <span class="op">=</span> pcaResults.components_[<span class="dv">0</span>,:]</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a>vec2 <span class="op">=</span> pcaResults.components_[<span class="dv">1</span>,:]</span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a>fig, ax <span class="op">=</span> subplots(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>))</span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a><span class="co"># Plot data and the feature direction</span></span>
<span id="cb14-8"><a href="#cb14-8" tabindex="-1"></a>ax.scatter(zdata[:, <span class="dv">0</span>], zdata[:, <span class="dv">1</span>])<span class="op">;</span></span>
<span id="cb14-9"><a href="#cb14-9" tabindex="-1"></a>ax.quiver([<span class="dv">0</span>], [<span class="dv">0</span>], vec1[<span class="dv">0</span>], vec1[<span class="dv">1</span>], scale<span class="op">=</span><span class="dv">3</span>, color<span class="op">=</span><span class="st">'r'</span>)<span class="op">;</span></span>
<span id="cb14-10"><a href="#cb14-10" tabindex="-1"></a>ax.quiver([<span class="dv">0</span>], [<span class="dv">0</span>], vec2[<span class="dv">0</span>], vec2[<span class="dv">1</span>], scale<span class="op">=</span><span class="dv">3</span>, color<span class="op">=</span><span class="st">'r'</span>)<span class="op">;</span></span>
<span id="cb14-11"><a href="#cb14-11" tabindex="-1"></a></span>
<span id="cb14-12"><a href="#cb14-12" tabindex="-1"></a>ax.set_title(<span class="st">'Sample Data'</span>, fontsize<span class="op">=</span><span class="dv">16</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb14-13"><a href="#cb14-13" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Dim 1'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span> ax.set_xlim([<span class="op">-</span><span class="dv">4</span>,<span class="dv">4</span>])<span class="op">;</span></span>
<span id="cb14-14"><a href="#cb14-14" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Dim 2'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span> ax.set_ylim([<span class="op">-</span><span class="dv">4</span>,<span class="dv">4</span>])<span class="op">;</span></span>
<span id="cb14-15"><a href="#cb14-15" tabindex="-1"></a></span>
<span id="cb14-16"><a href="#cb14-16" tabindex="-1"></a>show()</span></code></pre>
</div>
<figure><img src="fig/03-dimensionality-rendered-unnamed-chunk-10-9.png" width="576" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p style="text-align: justify;">
The red arrows show the new coordinate system, defined by the principal
components. It also shows that the more correlated the data are, the
less information we lose by reducing the data representation using PCA.
</p>
<div id="important" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="important" class="callout-inner">
<h3 class="callout-title">Important</h3>
<div class="callout-content">
<p>When reading about PCA you will come across the term <strong>Singular
Value Decomposition</strong> (SVD). As mentioned in the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" class="external-link">Scikit-learn
documentation for PCA</a>, when you apply <code>PCA</code> the algorithm
in the background uses SVD to return the results. The difference between
the two is technical but SVD is the more general method. Wikipedia has
as <a href="https://en.wikipedia.org/wiki/Principal_component_analysis#Singular_value_decomposition" class="external-link">mathematical
introduction to PCA</a> which also discusses <a href="https://en.wikipedia.org/wiki/Principal_component_analysis#Singular_value_decomposition" class="external-link">the
relationship to SVD</a>.</p>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="pca-of-the-gene-expression-data">PCA of the Gene Expression Data<a class="anchor" aria-label="anchor" href="#pca-of-the-gene-expression-data"></a></h2>
<hr class="half-width"><p style="text-align: justify;">
Now, we will use PCA to find a few features that capture most of the
variability in the gene expression dataset. we arbitrarily start with 50
components assuming that this will include the important features.
</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>nComp <span class="op">=</span> <span class="dv">50</span> <span class="co"># Number of PCs to be returned</span></span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a><span class="co">#trainIndx = binomial(1,0.9,size=filectr)</span></span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a>GenePCA <span class="op">=</span> PCA(n_components<span class="op">=</span>nComp, whiten<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a>GenePCA <span class="op">=</span> GenePCA.fit(geneData) <span class="co">#.values[trainIndx==1,:]</span></span></code></pre>
</div>
<p style="text-align: justify;">
How many features should we retain? To investigate this question, we
plot the total (cumulative) variance for retaining the top <em>k</em>
modes. The more we use, the higher the cumulative variance.
</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a>cumExpVar <span class="op">=</span> cumsum(GenePCA.explained_variance_ratio_)</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a>fig <span class="op">=</span> figure(figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">5</span>))</span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a>im  <span class="op">=</span> plot( <span class="bu">range</span>(nComp), cumExpVar )</span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a>xticks(arange(<span class="dv">0</span>,nComp,<span class="dv">5</span>))<span class="op">;</span></span>
<span id="cb16-5"><a href="#cb16-5" tabindex="-1"></a>xlabel( <span class="st">'Number of PCs'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span></span>
<span id="cb16-6"><a href="#cb16-6" tabindex="-1"></a>ylabel(<span class="st">'Cumulative explained variance'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span></span>
<span id="cb16-7"><a href="#cb16-7" tabindex="-1"></a></span>
<span id="cb16-8"><a href="#cb16-8" tabindex="-1"></a>show()</span></code></pre>
</div>
<figure><img src="fig/03-dimensionality-rendered-unnamed-chunk-12-11.png" width="480" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p style="text-align: justify;">
A common heuristic for choosing the number of components is by defining
a set threshold for the total explained variance. Thresholds commonly
vary between 0.8-0.9, depending on the structure of the PCs,
e.g. depending on whether there are a few top PCs or many small PCs, but
also depending on the expected noise in data, and on the desirable
accuracy of the reduced data set. While a dimensionality reduction is
convenient it always loses some information.
</p>
<p>As an example, we can check how many PCs we need to retain 99 % of
the variance.</p>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a>threshold <span class="op">=</span> <span class="fl">0.99</span></span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a>keepPC <span class="op">=</span> [pc <span class="cf">for</span> pc <span class="kw">in</span> <span class="bu">range</span>(nComp) <span class="cf">if</span> cumExpVar[pc]<span class="op">&gt;=</span>threshold][<span class="dv">0</span>]</span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Number of features needed to explain </span><span class="sc">{:1.2f}</span><span class="st"> fraction of total variance is </span><span class="sc">{:2d}</span><span class="st">. '</span>.<span class="bu">format</span>(threshold, keepPC) )</span></code></pre>
</div>
<p>Number of features needed to explain 0.99 fraction of total variance
is 19.</p>
<p>There are many alternative methods for estimating the number of
features. These include:</p>
<ul><li><p>Plot explained variance of individual PCs and cut off when there
is a sharp drop/saturation in the values.</p></li>
<li><p>Cross-validation - Find number of PCs that maximize explained
variance on heldout data (using a bi-cross validation scheme).</p></li>
<li><p>Visual inspection or interpretable PCs</p></li>
</ul><div class="section level3">
<h3 id="visualization-of-reduced-dataset"><strong>Visualization of reduced dataset</strong><a class="anchor" aria-label="anchor" href="#visualization-of-reduced-dataset"></a></h3>
<p>Now that we have selected the number of features to be used, we can
see what the data along those features looks like.</p>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a>newGeneData <span class="op">=</span> GenePCA.transform(geneData)[:,<span class="bu">range</span>(keepPC)]</span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a>fig, ax <span class="op">=</span> subplots(ncols<span class="op">=</span><span class="dv">3</span>, nrows<span class="op">=</span><span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">18</span>, <span class="dv">8</span>))</span>
<span id="cb18-4"><a href="#cb18-4" tabindex="-1"></a>suptitle(<span class="st">'Data in new Feature space'</span>, fontsize<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb18-5"><a href="#cb18-5" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">0</span>].scatter(newGeneData[:,<span class="dv">0</span>], newGeneData[:,<span class="dv">1</span>], c<span class="op">=</span><span class="bu">range</span>(filectr), cmap<span class="op">=</span><span class="st">'summer'</span>)<span class="op">;</span></span>
<span id="cb18-7"><a href="#cb18-7" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">0</span>].set_xlabel(<span class="st">'PC1'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span> ax[<span class="dv">0</span>, <span class="dv">0</span>].set_xlim([<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>])<span class="op">;</span></span>
<span id="cb18-8"><a href="#cb18-8" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">0</span>].set_ylabel(<span class="st">'PC2'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span> ax[<span class="dv">0</span>, <span class="dv">0</span>].set_ylim([<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>])<span class="op">;</span></span>
<span id="cb18-9"><a href="#cb18-9" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" tabindex="-1"></a></span>
<span id="cb18-11"><a href="#cb18-11" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">1</span>].scatter(newGeneData[:,<span class="dv">0</span>], newGeneData[:,<span class="dv">2</span>], c<span class="op">=</span><span class="bu">range</span>(filectr), cmap<span class="op">=</span><span class="st">'summer'</span>)<span class="op">;</span></span>
<span id="cb18-12"><a href="#cb18-12" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">1</span>].set_xlabel(<span class="st">'PC1'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span> ax[<span class="dv">0</span>, <span class="dv">1</span>].set_xlim([<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>])<span class="op">;</span></span>
<span id="cb18-13"><a href="#cb18-13" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">1</span>].set_ylabel(<span class="st">'PC3'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span> ax[<span class="dv">0</span>, <span class="dv">1</span>].set_ylim([<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>])<span class="op">;</span></span>
<span id="cb18-14"><a href="#cb18-14" tabindex="-1"></a></span>
<span id="cb18-15"><a href="#cb18-15" tabindex="-1"></a></span>
<span id="cb18-16"><a href="#cb18-16" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">2</span>].scatter(newGeneData[:,<span class="dv">0</span>], newGeneData[:,<span class="dv">3</span>], c<span class="op">=</span><span class="bu">range</span>(filectr), cmap<span class="op">=</span><span class="st">'summer'</span>)<span class="op">;</span></span>
<span id="cb18-17"><a href="#cb18-17" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">2</span>].set_xlabel(<span class="st">'PC1'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span> ax[<span class="dv">0</span>, <span class="dv">2</span>].set_xlim([<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>])<span class="op">;</span></span>
<span id="cb18-18"><a href="#cb18-18" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">2</span>].set_ylabel(<span class="st">'PC4'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span> ax[<span class="dv">0</span>, <span class="dv">2</span>].set_ylim([<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>])<span class="op">;</span></span>
<span id="cb18-19"><a href="#cb18-19" tabindex="-1"></a></span>
<span id="cb18-20"><a href="#cb18-20" tabindex="-1"></a></span>
<span id="cb18-21"><a href="#cb18-21" tabindex="-1"></a></span>
<span id="cb18-22"><a href="#cb18-22" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">0</span>].scatter(newGeneData[:,<span class="dv">1</span>], newGeneData[:,<span class="dv">2</span>], c<span class="op">=</span><span class="bu">range</span>(filectr), cmap<span class="op">=</span><span class="st">'summer'</span>)<span class="op">;</span></span>
<span id="cb18-23"><a href="#cb18-23" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">0</span>].set_xlabel(<span class="st">'PC2'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span> ax[<span class="dv">1</span>, <span class="dv">0</span>].set_xlim([<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>])<span class="op">;</span></span>
<span id="cb18-24"><a href="#cb18-24" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">0</span>].set_ylabel(<span class="st">'PC3'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span> ax[<span class="dv">1</span>, <span class="dv">0</span>].set_ylim([<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>])<span class="op">;</span></span>
<span id="cb18-25"><a href="#cb18-25" tabindex="-1"></a></span>
<span id="cb18-26"><a href="#cb18-26" tabindex="-1"></a></span>
<span id="cb18-27"><a href="#cb18-27" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">1</span>].scatter(newGeneData[:,<span class="dv">1</span>], newGeneData[:,<span class="dv">3</span>], c<span class="op">=</span><span class="bu">range</span>(filectr), cmap<span class="op">=</span><span class="st">'summer'</span>)<span class="op">;</span></span>
<span id="cb18-28"><a href="#cb18-28" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">1</span>].set_xlabel(<span class="st">'PC2'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span> ax[<span class="dv">1</span>, <span class="dv">1</span>].set_xlim([<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>])<span class="op">;</span></span>
<span id="cb18-29"><a href="#cb18-29" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">1</span>].set_ylabel(<span class="st">'PC4'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span> ax[<span class="dv">1</span>, <span class="dv">1</span>].set_ylim([<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>])<span class="op">;</span></span>
<span id="cb18-30"><a href="#cb18-30" tabindex="-1"></a></span>
<span id="cb18-31"><a href="#cb18-31" tabindex="-1"></a></span>
<span id="cb18-32"><a href="#cb18-32" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">2</span>].scatter(newGeneData[:,<span class="dv">3</span>], newGeneData[:,<span class="dv">2</span>], c<span class="op">=</span><span class="bu">range</span>(filectr), cmap<span class="op">=</span><span class="st">'summer'</span>)<span class="op">;</span></span>
<span id="cb18-33"><a href="#cb18-33" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">2</span>].set_xlabel(<span class="st">'PC4'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span> ax[<span class="dv">1</span>, <span class="dv">2</span>].set_xlim([<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>])<span class="op">;</span></span>
<span id="cb18-34"><a href="#cb18-34" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">2</span>].set_ylabel(<span class="st">'PC3'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span> ax[<span class="dv">1</span>, <span class="dv">2</span>].set_ylim([<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>])<span class="op">;</span></span>
<span id="cb18-35"><a href="#cb18-35" tabindex="-1"></a></span>
<span id="cb18-36"><a href="#cb18-36" tabindex="-1"></a>show()</span></code></pre>
</div>
<figure><img src="fig/03-dimensionality-rendered-unnamed-chunk-14-13.png" width="1728" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure></div>
<div class="section level3">
<h3 id="beyond-dimensionality-reduction"><strong>Beyond dimensionality reduction</strong><a class="anchor" aria-label="anchor" href="#beyond-dimensionality-reduction"></a></h3>
<p>After reducing dataset from hundreds or thousands of genes to fewer
features (PCs), it can be used:</p>
<ul><li><p>To find groups of genes that seem to be co-regulated (using
clustering).</p></li>
<li><p>To build classifiers to predict the likelihood of juvenile or
late-onset rheumatoid disease. For example, one feature can be highly
variable in the sampled population and therefore has high explained
variance, but it may not predictive for the current objective of
predicting disease phenotype (e.g. eye pigmentation genes).</p></li>
<li><p>To find what combination of genes the top features correspond
to.</p></li>
<li><p>To suggest what individual genes are most variable.</p></li>
<li><p>To discover co-expression patterns of multiple genes.</p></li>
</ul><div id="optional-exercises" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="optional-exercises" class="callout-inner">
<h3 class="callout-title">Optional Exercises</h3>
<div class="callout-content">
<ol style="list-style-type: decimal"><li><p>The dataset contains two types of samples, expression in
neutrophil and expression in peripheral blood mononuclear cells (PBMC).
Download the <a href="https://www.ebi.ac.uk/arrayexpress/experiments/E-GEOD-11083/samples/" class="external-link">sample-data
table</a> to get the category of each sample. Make a scatter plot of two
(original) features and set the colour of the scatter based on the
sample’s origin. Do a PCA, scatter the top scoring components and colour
the samples in the same way. Do the top PCs capture gene expression
differences in the two types of cells?</p></li>
<li><p>The dataset similarly contains samples from individuals with
differing phenotypes - healthy, as well as 2 different disease
characteristics. Similar to the previous exercise, colour the scatter
based on the disease phenotypes. Do the top PCs capture variability in
gene expression for different phenotypes?</p></li>
</ol></div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="when-does-pca-fail">When does PCA fail?<a class="anchor" aria-label="anchor" href="#when-does-pca-fail"></a></h2>
<hr class="half-width"><p>Although PCA is the first and simplest method of exploring
high-dimensional datasets, there are important caveats to keep in
mind:</p>
<ol style="list-style-type: decimal"><li><em>PCA is a linear method</em></li>
</ol><p>Nonlinear patterns of co-expression will be seemingly broken up into
many features. Thus, the ability to find whether genes are co-regulated
may be reduced if the effects are nonlinear.</p>
<ol start="2" style="list-style-type: decimal"><li><em>PCA depends on scaling of individual measurements.</em></li>
</ol><p>As PCA measures variance along different directions, changing the
scale of one variable (for example from cm to mm, or fold-change to
actual levels) may drastically change the dominant “features”
extracted.</p>
<p>The lesson on clustering of images with Gaussian mixture models
contained instructions to scale data. You can explore the original
dataset of this lesson, to investigate how scaling of variables impacts
the results.</p>
<ol start="3" style="list-style-type: decimal"><li><em>Estimating number of features is heuristic, and depends on the
purpose.</em></li>
</ol><p>The definition of what is the ‘relevant’ number of features can
depend on:</p>
<ul><li>the signal-to-noise ratio in data. For example, noisier or smaller
datasets make it harder to accurately estimate smaller PCs. In such
cases, a more conservative threshold should be used.</li>
<li>the classifier or regression performance following the
dimensionality reduction.</li>
</ul></section><section><h2 class="section-heading" id="resources">Resources<a class="anchor" aria-label="anchor" href="#resources"></a></h2>
<hr class="half-width"><ul><li><p><a href="http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html" class="external-link">Detailed
tutorial on PCA</a></p></li>
<li>
<p><a href="https://towardsdatascience.com/dimension-reduction-techniques-with-python-f36ca7009e5c" class="external-link">Other
methods for dimensionality reduction</a>:</p>
<ul><li><p><a href="https://scikit-learn.org/stable/modules/decomposition.html" class="external-link">Matrix
factorization methods</a></p></li>
<li><p><a href="https://scikit-learn.org/stable/modules/manifold.html" class="external-link">Manifold
learning or topological methods</a></p></li>
<li><p><a href="https://scikit-learn.org/stable/modules/unsupervised_reduction.html" class="external-link">Example
pipelines with Scikit-learn</a></p></li>
</ul></li>
</ul><p>Dataset from:</p>
<p>Frank MB, Wang S, Aggarwal A, et al. <strong>Disease-associated
pathophysiologic structures in pediatric rheumatic diseases show
characteristics of scale-free networks seen in physiologic systems:
implications for pathogenesis and treatment.</strong> <em>BMC Med
Genomics. 2009;2:9.</em> Published 2009 Feb 23. <a href="doi:10.1186/1755-8794-2-9" class="uri">doi:10.1186/1755-8794-2-9</a></p>
<p>Download data from: <a href="https://www.ebi.ac.uk/arrayexpress/experiments/E-GEOD-11083/" class="external-link uri">https://www.ebi.ac.uk/arrayexpress/experiments/E-GEOD-11083/</a></p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul><li>Reduced features in a dataset reduce redundancy and process is
called dimensionality reduction.</li>
<li>Principal component analysis (PCA) is one of the commonly used
dimensionality reduction methods.</li>
<li>
<code>n_components</code> is used specify the number of components
in <code>Scikit-learn</code>.</li>
<li>PCA can be helpful to find groups of genes that seem to be
co-regulated.</li>
</ul></div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</section></div> <!-- / div.lesson-content -->
    </main><!-- / main#main-content.main-content --><nav class="bottom-pagination mx-md-4" aria-label="Previous and Next Chapter"><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="02-clustering-image.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="index.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="02-clustering-image.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Clustering Images
        </a>
        <a class="chapter-link float-end" href="index.html" rel="next">
          Home
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
    </nav></div> <!-- / div.primary-content.col-xs-12 -->
<!-- END:   inst/pkgdown/templates/content-instructor.html-->

      </div><!--/div.row-->
      		<footer class="row footer mx-md-3"><hr><div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/carpentries/workbench-template-md/edit/main/episodes/03-dimensionality.Rmd" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/carpentries/workbench-template-md/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/carpentries/workbench-template-md/" class="external-link">Source</a></p>
				<p><a href="https://github.com/carpentries/workbench-template-md/blob/main/CITATION" class="external-link">Cite</a> | <a href="mailto:team@carpentries.org">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/69aaefee46a17928e2a1694825599e169b9793e3" class="external-link">sandpaper (0.16.10)</a>, <a href="https://github.com/carpentries/pegboard/tree/bad0be19a12f0c6545801b276ddf26c945f8bfd1" class="external-link">pegboard (0.7.7)</a>, and <a href="https://github.com/milanmlft/varnish/tree/milanmlft/sticky-sidebar" class="external-link">varnish (1.0.3.9000)</a></p>
			</div>
		</footer></div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "TrainingMaterial",
  "@id": "https://carpentries.github.io/workbench-template-md/03-dimensionality.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries",
  "name": "Dimensionality Reduction",
  "creativeWorkStatus": "active",
  "url": "https://carpentries.github.io/workbench-template-md/03-dimensionality.html",
  "identifier": "https://carpentries.github.io/workbench-template-md/03-dimensionality.html",
  "dateCreated": "2022-07-05",
  "dateModified": "2025-01-27",
  "datePublished": "2025-01-27"
}

  </script><script>
		feather.replace();
	</script></body></html><!-- END:   inst/pkgdown/templates/layout.html-->

