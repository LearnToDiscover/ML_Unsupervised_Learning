<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<title>Machine Learning - Unsupervised: All in One View</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" type="text/css" href="assets/styles.css">
<script src="assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="site.webmanifest">
<link rel="mask-icon" href="safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">
</head>
<body>
    <header id="top" class="navbar navbar-expand-md navbar-light bg-white top-nav l2d"><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-6">
      <div class="large-logo">
        <img alt="l2d" src="assets/images/l2d-logo.svg">
</div>
    </div>
    <div class="selector-container">


      <div class="dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Learner View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1">
<li><button class="dropdown-item" type="button" onclick="window.location.href='instructor/aio.html';">Instructor View</button></li>
        </ul>
</div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl navbar-light bg-white bottom-nav l2d" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="l2d" src="assets/images/l2d-logo-sm.svg">
</div>
    <div class="lesson-title-md">
      Machine Learning - Unsupervised
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0">
<li class="nav-item">
          <span class="lesson-title">
            Machine Learning - Unsupervised
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="reference.html#glossary">Glossary</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="profiles.html">Learner Profiles</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown"></ul>
</li>
      </ul>
</div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a class="btn btn-primary" href="aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div>
<!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Machine Learning - Unsupervised
</div>

<aside class="col-md-12 lesson-progress"><div style="width: %" class="percentage">
    %
  </div>
  <div class="progress l2d">
    <div class="progress-bar l2d" role="progressbar" style="width: %" aria-valuenow="" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar" style="display: flex">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle sticky-top" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner sticky-top">
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Learner View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="instructor/aio.html">Instructor View</a>
                      </div>
                    </div>
                  </div>
<!--/div.accordion-item-->
                </div>
<!--/div.accordion-flush-->
              </div>
<!--div.sidenav-view-selector -->
            </div>
<!--/div.col -->

            <hr>
</div>
<!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Setup</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="01-clustering-intro.html">1. Clustering Introduction</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="02-clustering-image.html">2. Clustering Images</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="03-dimensionality.html">3. Dimensionality Reduction</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width">
<div class="accordion accordion-flush resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul>
<li>
                        <a href="key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="reference.html#glossary">Glossary</a>
                      </li>
                      <li>
                        <a href="profiles.html">Learner Profiles</a>
                      </li>

                    </ul>
</div>
                </div>
              </div>
            </div>
            <hr class="half-width resources">
<a href="aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none">
<div class="d-grid gap-1">

            </div>
          </div>
<!-- /div.accordion -->
        </div>
<!-- /div.sidebar-inner -->
      </nav>
</div>
<!-- /div.sidebar -->
  </div>
<!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-extra.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <main id="main-content" class="main-content"><div class="container lesson-content">
        
        
<section id="aio-01-clustering-intro"><p>Content from <a href="01-clustering-intro.html">Clustering Introduction</a></p>
<hr>
<p>Last updated on 2025-01-27 |

        <a href="https://github.com/carpentries/workbench-template-md/edit/main/episodes/01-clustering-intro.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<p><a href="https://drive.usercontent.google.com/u/1/uc?id=1o4rsr8av-DaGtNILCXNRLnkhCUfoipht&amp;export=download" class="external-link"><strong>Download
Chapter notebook (ipynb)</strong></a></p>
<p><a href="https://drive.usercontent.google.com/u/1/uc?id=1d25T9eqO1vbgIFzKNu3d7UeLSdFHW8i7&amp;export=download" class="external-link"><strong>Download
Chapter PDF</strong></a></p>
<p><a href="https://docs.google.com/forms/d/e/1FAIpQLSdr0capF7jloJhPH3Pki1B3LZoKOG16poOpuVJ7SL2LkwLHQA/viewform?pli=1" class="external-link"><span style="color: rgb(255, 0, 0);"><strong>Mandatory Lesson Feedback
Survey</strong></span></a></p>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How to search for multiple distributions in a dataset?</li>
<li>How to use Scikit-learn to perform clustering?</li>
<li>How is data labelled in unsupervised learning?</li>
<li>How can we score clustering predictions?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Understanding Multiple Gaussian distributions in a dataset.</li>
<li>Demonstrating Scikit-learn functionality for Gaussian Mixture
Models.</li>
<li>Learning automated labelling of dataset.</li>
<li>Obtaining a basic quality score using a ground truth.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<br><p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/skuOor8jjlU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</p>
<br><p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/czt2rKrfIzw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</p>
<p><br></p>
<div id="prereq1" class="callout prereq">
<div class="callout-square">
<i class="callout-icon" data-feather="check"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Prerequisite</h3>
<div class="callout-content">
<ul>
<li><a href="https://learntodiscover.github.io/ML_supervised/01-classification_intro.html" class="external-link">Classification
Introduction</a></li>
<li><a href="https://learntodiscover.github.io/ML_supervised/02-improvement.html" class="external-link">Classification
Improvement</a></li>
</ul>
</div>
</div>
</div>
<div class="section level3">
<h3 id="import-functions">
<strong>Import functions</strong><a class="anchor" aria-label="anchor" href="#import-functions"></a>
</h3>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> arange, asarray, linspace, zeros, c_, mgrid, meshgrid, array, dot, percentile</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> histogram, cumsum, around</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> vstack, sqrt, logspace, amin, amax, equal, invert, count_nonzero</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="im">from</span> numpy.random <span class="im">import</span> uniform, seed, randint, randn, multivariate_normal</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="im">from</span> matplotlib.pyplot <span class="im">import</span> subplots, scatter, xlabel, ylabel, axis, figure, colorbar, title, show</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="im">from</span> matplotlib.colors <span class="im">import</span> LogNorm</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="im">from</span> pandas <span class="im">import</span> read_csv</span></code></pre>
</div>
</div>
<section><h2 class="section-heading" id="example">Example<a class="anchor" aria-label="anchor" href="#example"></a>
</h2>
<hr class="half-width">
<p>Import the patients data, scatter the data for Weight and Height and
get a summary statistics for both.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>df <span class="op">=</span> read_csv(<span class="st">"data/patients_data.csv"</span>)</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="co"># Weigth to kg and height to cm</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>pound_kg_conversion <span class="op">=</span> <span class="fl">0.45</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>inch_cm_conversion  <span class="op">=</span> <span class="fl">2.54</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>df[<span class="st">'Weight'</span>] <span class="op">=</span> pound_kg_conversion<span class="op">*</span>df[<span class="st">'Weight'</span>]</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>df[<span class="st">'Height'</span>] <span class="op">=</span> inch_cm_conversion <span class="op">*</span>df[<span class="st">'Height'</span>]</span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a>fig, ax <span class="op">=</span> subplots()</span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a>ax.scatter(df[<span class="st">'Weight'</span>], df[<span class="st">'Height'</span>])</span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Weight (kg)'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb2-16"><a href="#cb2-16" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Height (cm)'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb2-17"><a href="#cb2-17" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" tabindex="-1"></a>df[[<span class="st">'Weight'</span>, <span class="st">'Height'</span>]].describe()</span>
<span id="cb2-19"><a href="#cb2-19" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" tabindex="-1"></a>show()</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>           Weight      Height
count  100.000000  100.000000
mean    69.300000  170.357800
std     11.957139    7.204631
min     49.950000  152.400000
25%     58.837500  165.100000
50%     64.125000  170.180000
75%     81.112500  175.895000
max     90.900000  182.880000</code></pre>
</div>
<figure><img src="fig/01-clustering-intro-rendered-unnamed-chunk-2-1.png" width="672" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p style="text-align: justify;">
Looking at the data, one might expect that there are two distinct
groups, visually identified as two clouds separated e.g. by a vertical
line at Weight <span class="math inline">\(\approx\)</span> 70 (kg). A
consequence is that the mean value of 69.3 kg (which was calculated over
all samples) should better be replaced by two mean values, one for each
of the clouds. Visually, these can be estimated at around 60 and 80 kg.
</p>
<p>We can make this even clearer by looking at the two individual
distributions.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>fig, ax <span class="op">=</span> subplots(ncols<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>ax[<span class="dv">0</span>].hist(df[<span class="st">'Weight'</span>], bins<span class="op">=</span><span class="dv">20</span>)<span class="op">;</span></span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">'Weight'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="st">'Count'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>ax[<span class="dv">1</span>].hist(df[<span class="st">'Height'</span>], bins<span class="op">=</span><span class="dv">12</span>)<span class="op">;</span></span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">'Height'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)<span class="op">;</span></span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a>show()</span></code></pre>
</div>
<figure><img src="fig/01-clustering-intro-rendered-unnamed-chunk-3-3.png" width="672" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p style="text-align: justify;">
The Weight histogram shows two distributions (at the chosen number of
bins) which now also points to the two mean values as guessed above. The
Height histogram is at least not compatible with the assumption of a
normal distribution as would be expected for a typically noisy variable.
</p>
<p style="text-align: justify;">
Thus, visual inspection suggests to analyse the data in terms of more
than one underlying distribution. The automated assignment of data
points to distinct groups is called clustering.
</p>
<p style="text-align: justify;">
We now want to learn to use the Gaussian Mixture Model approach to find
those groups. As we will not provide any labels for the training, this
presents an example of unsupervised machine learning. Algorithms of this
type of machine learning are designed to learn to optimally assign
labels through training. As a result, we will be able to separate a
dataset into groups and be able to predict the labels of new, unlabelled
data.
</p>
</section><section><h2 class="section-heading" id="gaussian-mixture-models">Gaussian Mixture Models<a class="anchor" aria-label="anchor" href="#gaussian-mixture-models"></a>
</h2>
<hr class="half-width">
<p style="text-align: justify;">
A Gaussian Mixture Models (GMM) approach assumes that the data are
composed of two or more normal distributions that may overlap. In a
scatter plot that means that there is more than one centre in the
density distribution of the data (see scatter plot above). The task is
to find the centres and the spread of each distribution in the mixture.
The GMM algorithm thus belongs to the category of (probability) Density
Estimators. Another way of grouping is to find a curve that splits the
plane into two areas.
</p>
<p style="text-align: justify;">
The GMM assumes normally distributed data structure from at least two
sources. Other than that it does not make assumptions about the data.
</p>
<p style="text-align: justify;">
GMM is a parametric learning approach as it optimises the parameters of
a normal distributions, i.e. the mean and the covariance matrix of each
group. It is therefore an example of a model fitting method.
</p>
<p style="text-align: justify;">
As its name suggests, it assumes that the distribution of each group is
normal. If the groups are known to have a non-normal distribution, it
may not be the optimal approach.
</p>
<p style="text-align: justify;">
GMM is one example of clustering or <a href="https://en.wikipedia.org/wiki/Cluster_analysis" class="external-link">cluster
analysis</a>. Whenever we suspect that a data set contains contributions
of qualitatively different types, we can consider doing a cluster
analysis to separate those types. However, this is a vague notion and
clustering is therefore a complex field. We can only provide an
introduction to its basic components. The main point to keep in mind is
that the algorithms provided e.g. by Scikit-learn will always give some
result but that it is not easy to assess the quality of the results.
Scikit-learn has a good <a href="https://scikit-learn.org/stable/modules/clustering.html#c" class="external-link">overview
of clustering methods</a> showing advantages and disadvantages of each.
Here is a link to a readable introduction about the cautious application
and the <a href="https://stke.sciencemag.org/content/9/432/re6.full" class="external-link">pitfalls of
clustering</a>.
</p>
</section><section><h2 class="section-heading" id="work-through-example">Work Through Example<a class="anchor" aria-label="anchor" href="#work-through-example"></a>
</h2>
<hr class="half-width">
<div class="section level3">
<h3 id="creating-test-data">
<strong>Creating test data</strong><a class="anchor" aria-label="anchor" href="#creating-test-data"></a>
</h3>
<p style="text-align: justify;">
Let us create synthetic data for testing of the clustering algorithm. We
do this according to the assumptions of GMM: we create two Gaussian data
sets with different means and different standard distributions and add
them together. For illustration we only use two features.
</p>
<p>The example is adapted from a <a href="https://scikit-learn.org/stable/modules/mixture.html#mixture" class="external-link">Scikit-learn
example</a>. It uses the concept of <a href="https://datascienceplus.com/understanding-the-covariance-matrix/" class="external-link">covariance
matrix</a> which is the extension of variance (or standard deviation) to
multivariate datasets.</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>m_features <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a><span class="co"># Seed the random number generator</span></span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>RANDOM_NUMBER <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>seed(RANDOM_NUMBER)</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a><span class="co"># Data set 1, centered at (20, 20)</span></span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a>mean_11 <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a>mean_12 <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a>gaussian_1 <span class="op">=</span> randn(n_samples, m_features) <span class="op">+</span> array([mean_11, mean_12])</span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a></span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" tabindex="-1"></a><span class="co"># Data set 2, zero centered and stretched with covariance matrix C</span></span>
<span id="cb5-16"><a href="#cb5-16" tabindex="-1"></a>C <span class="op">=</span> array([[<span class="dv">1</span>, <span class="op">-</span><span class="fl">0.7</span>], [<span class="fl">3.5</span>, <span class="fl">.7</span>]])</span>
<span id="cb5-17"><a href="#cb5-17" tabindex="-1"></a></span>
<span id="cb5-18"><a href="#cb5-18" tabindex="-1"></a>gaussian_2 <span class="op">=</span> dot(randn(n_samples, m_features), C)</span>
<span id="cb5-19"><a href="#cb5-19" tabindex="-1"></a></span>
<span id="cb5-20"><a href="#cb5-20" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" tabindex="-1"></a><span class="co"># Concatenate the two Gaussians to obtain the training data set</span></span>
<span id="cb5-22"><a href="#cb5-22" tabindex="-1"></a>X_train <span class="op">=</span> vstack([gaussian_1, gaussian_2])</span>
<span id="cb5-23"><a href="#cb5-23" tabindex="-1"></a></span>
<span id="cb5-24"><a href="#cb5-24" tabindex="-1"></a><span class="bu">print</span>(X_train.shape)</span>
<span id="cb5-25"><a href="#cb5-25" tabindex="-1"></a></span>
<span id="cb5-26"><a href="#cb5-26" tabindex="-1"></a>fig, ax <span class="op">=</span> subplots()</span>
<span id="cb5-27"><a href="#cb5-27" tabindex="-1"></a></span>
<span id="cb5-28"><a href="#cb5-28" tabindex="-1"></a>ax.scatter(X_train[:, <span class="dv">0</span>], X_train[:, <span class="dv">1</span>])<span class="op">;</span></span>
<span id="cb5-29"><a href="#cb5-29" tabindex="-1"></a></span>
<span id="cb5-30"><a href="#cb5-30" tabindex="-1"></a>show()</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>(1000, 2)</code></pre>
</div>
<figure><img src="fig/01-clustering-intro-rendered-unnamed-chunk-4-5.png" width="672" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p style="text-align: justify;">
The scatter plot showes that this method allows the adjustment of the
centres of the distributions as well as the elliptic shape of the
distribution.
</p>
<p style="text-align: justify;">
Now we fit a GMM. Note that the GMM needs to be told how many components
one wants to fit. Modifications that estimate the optimal number of
components exist but we will restrict the demonstration to the method
that directly sets the number.
</p>
<p style="text-align: justify;">
Analogous to the classifier in supervised learning, we instantiate the
model from the imported class <code>GaussianMixture</code>. The
instantiation takes the number of independent data sets (clusters) as an
argument. By default, the classifier tries to fit the full covariance
matrix of each group. The fitting is done using the method
<code>fit</code>.
</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="im">from</span> sklearn.mixture <span class="im">import</span> GaussianMixture</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a><span class="co"># Fit a Gaussian Mixture Model with two components</span></span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>components <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a>clf <span class="op">=</span> GaussianMixture(n_components<span class="op">=</span>components)</span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a>clf.fit(X_train)</span></code></pre>
</div>
<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style>
<div id="sk-container-id-1" class="sk-top-container">
<div class="sk-text-repr-fallback">
<pre>GaussianMixture(n_components=2)</pre>
<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b>
</div>
<div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable">
<input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">GaussianMixture</label><div class="sk-toggleable__content"><pre>GaussianMixture(n_components=2)</pre></div>
</div></div></div>
</div>
<p style="text-align: justify;">
After the fitting of the model, we first create a meshgrid of the
(two-dimensional) state space. For each point in this state space, we
obtain the predicted scores using the method
<code>.score_samples</code>. These are the weighted logarithmic
probabilities which show the predicted distribution of points in the
state space.
</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>resolution <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>vec_a <span class="op">=</span> linspace(<span class="op">-</span><span class="fl">60.</span>, <span class="fl">80.</span>, resolution)</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>vec_b <span class="op">=</span> linspace(<span class="op">-</span><span class="fl">40.</span>, <span class="fl">50.</span>, resolution)</span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>grid_a, grid_b <span class="op">=</span> meshgrid(vec_a, vec_b)</span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a>XY_statespace <span class="op">=</span> c_[grid_a.ravel(), grid_b.ravel()]</span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a>Z_score <span class="op">=</span> clf.score_samples(XY_statespace)</span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a></span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a>Z_s <span class="op">=</span> Z_score.reshape(grid_a.shape)</span>
<span id="cb8-13"><a href="#cb8-13" tabindex="-1"></a></span>
<span id="cb8-14"><a href="#cb8-14" tabindex="-1"></a><span class="bu">print</span>(Z_s.shape)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>(100, 100)</code></pre>
</div>
<p style="text-align: justify;">
Now we can display the predicted scores as a contour plot. Typically,
the negative log-likelihood or <a href="https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_pdf.html?highlight=lognorm" class="external-link">density
estimation</a> is used for this. In this case, the highest probabilities
are shown as a landscape with two minima.
</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>fig, ax <span class="op">=</span> subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>cax <span class="op">=</span> ax.contour(grid_a, grid_b, <span class="op">-</span>Z_s,</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>           norm<span class="op">=</span>LogNorm(vmin<span class="op">=</span><span class="fl">1.0</span>, vmax<span class="op">=</span><span class="fl">1000.0</span>),</span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a>           levels<span class="op">=</span>logspace(<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">10</span>),</span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a>           cmap<span class="op">=</span><span class="st">'magma'</span></span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a>          )</span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a>fig.colorbar(cax)<span class="op">;</span></span>
<span id="cb10-10"><a href="#cb10-10" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" tabindex="-1"></a>ax.scatter(X_train[:, <span class="dv">0</span>], X_train[:, <span class="dv">1</span>], <span class="fl">.8</span>)</span>
<span id="cb10-12"><a href="#cb10-12" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" tabindex="-1"></a>title(<span class="st">'Negative log-likelihood of Prediction'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb10-14"><a href="#cb10-14" tabindex="-1"></a>axis(<span class="st">'tight'</span>)<span class="op">;</span></span>
<span id="cb10-15"><a href="#cb10-15" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" tabindex="-1"></a>show()</span></code></pre>
</div>
<figure><img src="fig/01-clustering-intro-rendered-unnamed-chunk-7-7.png" width="768" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>You can change the number of components to see the impact it has on
the result. E.g. picking 3 components:</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>clf_3 <span class="op">=</span> GaussianMixture(n_components<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>clf_3.fit(X_train)</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a>Z_score_3 <span class="op">=</span> clf_3.score_samples(XY_statespace)</span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a>Z_s_3 <span class="op">=</span> Z_score_3.reshape(grid_a.shape)</span>
<span id="cb11-8"><a href="#cb11-8" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" tabindex="-1"></a>fig, ax <span class="op">=</span> subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb11-10"><a href="#cb11-10" tabindex="-1"></a></span>
<span id="cb11-11"><a href="#cb11-11" tabindex="-1"></a>cax <span class="op">=</span> ax.contour(grid_a, grid_b, <span class="op">-</span>Z_s_3,</span>
<span id="cb11-12"><a href="#cb11-12" tabindex="-1"></a>           norm<span class="op">=</span>LogNorm(vmin<span class="op">=</span><span class="fl">1.0</span>, vmax<span class="op">=</span><span class="fl">1000.0</span>),</span>
<span id="cb11-13"><a href="#cb11-13" tabindex="-1"></a>           levels<span class="op">=</span>logspace(<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">10</span>),</span>
<span id="cb11-14"><a href="#cb11-14" tabindex="-1"></a>           cmap<span class="op">=</span><span class="st">'magma'</span></span>
<span id="cb11-15"><a href="#cb11-15" tabindex="-1"></a>          )</span>
<span id="cb11-16"><a href="#cb11-16" tabindex="-1"></a></span>
<span id="cb11-17"><a href="#cb11-17" tabindex="-1"></a>fig.colorbar(cax)<span class="op">;</span></span>
<span id="cb11-18"><a href="#cb11-18" tabindex="-1"></a></span>
<span id="cb11-19"><a href="#cb11-19" tabindex="-1"></a>title(<span class="st">'Negative log-likelihood (3 components)'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb11-20"><a href="#cb11-20" tabindex="-1"></a>axis(<span class="st">'tight'</span>)<span class="op">;</span></span>
<span id="cb11-21"><a href="#cb11-21" tabindex="-1"></a></span>
<span id="cb11-22"><a href="#cb11-22" tabindex="-1"></a>show()</span></code></pre>
</div>
<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style>
<div id="sk-container-id-2" class="sk-top-container">
<div class="sk-text-repr-fallback">
<pre>GaussianMixture(n_components=3)</pre>
<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b>
</div>
<div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable">
<input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox" checked><label for="sk-estimator-id-2" class="sk-toggleable__label sk-toggleable__label-arrow">GaussianMixture</label><div class="sk-toggleable__content"><pre>GaussianMixture(n_components=3)</pre></div>
</div></div></div>
</div>
<figure><img src="fig/01-clustering-intro-rendered-unnamed-chunk-8-9.png" width="768" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p style="text-align: justify;">
For the choice of 3 components it does not lead to a probability
distribution with 3 distinct maxima. This is because two of the maxima
coincide or at least nearly coincide.
</p>
<p style="text-align: justify;">
In our example, the choice of 2 components is very obvious because as
done above, we could visualise the complete state space and there was a
visually discernible structure in the data. In high-dimensional data the
task is difficult and while methods exist to automatically find the <a href="https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set" class="external-link">optimal
number of components for some clustering methods</a>, the success of
these depends very much on the problem.
</p>
</div>
<div class="section level3">
<h3 id="getting-optimal-model-parameters">
<strong>Getting optimal model parameters</strong><a class="anchor" aria-label="anchor" href="#getting-optimal-model-parameters"></a>
</h3>
<p style="text-align: justify;">
Now that the estimator is fitted, we can obtain the optimal parameters
for the fitted components. They are stored in the model attributes. We
can extract (i) the <code>.weights_</code>, the share of each of the
components (Gaussians) in the mixture; (ii) the <code>.means_</code>,
the coordinates of the mean values; and (iii)
<code>.covariances_</code>, the covariance matrix of each component.
</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>components <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a>clf <span class="op">=</span> GaussianMixture(n_components<span class="op">=</span>components)<span class="op">;</span></span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a>clf.fit(X_train)</span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Model Weights: '</span>)</span>
<span id="cb12-8"><a href="#cb12-8" tabindex="-1"></a><span class="bu">print</span>(clf.weights_)</span>
<span id="cb12-9"><a href="#cb12-9" tabindex="-1"></a><span class="bu">print</span>(<span class="st">''</span>)</span>
<span id="cb12-10"><a href="#cb12-10" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Mean coordinates: '</span>)</span>
<span id="cb12-12"><a href="#cb12-12" tabindex="-1"></a><span class="bu">print</span>(clf.means_)</span>
<span id="cb12-13"><a href="#cb12-13" tabindex="-1"></a><span class="bu">print</span>(<span class="st">''</span>)</span>
<span id="cb12-14"><a href="#cb12-14" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Covariance Matrices: '</span>)</span>
<span id="cb12-15"><a href="#cb12-15" tabindex="-1"></a><span class="bu">print</span>(clf.covariances_)</span></code></pre>
</div>
<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style>
<div id="sk-container-id-3" class="sk-top-container">
<div class="sk-text-repr-fallback">
<pre>GaussianMixture(n_components=2)</pre>
<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b>
</div>
<div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable">
<input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-3" type="checkbox" checked><label for="sk-estimator-id-3" class="sk-toggleable__label sk-toggleable__label-arrow">GaussianMixture</label><div class="sk-toggleable__content"><pre>GaussianMixture(n_components=2)</pre></div>
</div></div></div>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Model Weights:
[0.5 0.5]

Mean coordinates:
[[ 2.00467649e+01  2.00308601e+01]
 [ 1.10681138e-01 -6.87868023e-03]]

Covariance Matrices:
[[[ 0.95442218 -0.06641459]
  [-0.06641459  0.97019156]]

 [[13.78557368  1.81677876]
  [ 1.81677876  1.04931994]]]</code></pre>
</div>
<p style="text-align: justify;">
The fit returns a model where the two components have equal weight. The
means and covariance matrices can be compared directly to the values
chosen to create the data. They are not identical but good estimates are
obtained from a fit to 500 data points in each group.
</p>
</div>
<div class="section level3">
<h3 id="create-data-from-optimal-model">
<strong>Create data from optimal model</strong><a class="anchor" aria-label="anchor" href="#create-data-from-optimal-model"></a>
</h3>
<p style="text-align: justify;">
The result of the fitting are the parameters for two Gaussian
distributions with two features each. These parameters can be used to
create further model data with the same characteristics. In our
demonstration we know the original sources but if the parameters are
obtained from experimental or clinical data, it is useful to visualise
the predicted distributions using as many samples as necessary.
</p>
<p>If we know the mean and the covariance matrix of a Gaussian, the
function <code>multivariate_normal</code> can be used to create data
from that Gaussian.</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>model1_mean, model2_mean <span class="op">=</span> clf.means_[<span class="dv">0</span>], clf.means_[<span class="dv">1</span>]</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a>model1_cov, model2_cov <span class="op">=</span>  clf.covariances_[<span class="dv">0</span>], clf.covariances_[<span class="dv">1</span>]</span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a>samples <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a>model1_data <span class="op">=</span> multivariate_normal(model1_mean, model1_cov, samples)</span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a>model2_data <span class="op">=</span> multivariate_normal(model2_mean, model2_cov, samples)</span>
<span id="cb14-8"><a href="#cb14-8" tabindex="-1"></a></span>
<span id="cb14-9"><a href="#cb14-9" tabindex="-1"></a>fig, ax <span class="op">=</span> subplots()</span>
<span id="cb14-10"><a href="#cb14-10" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" tabindex="-1"></a>ax.scatter(model1_data[:, <span class="dv">0</span>], model1_data[:, <span class="dv">1</span>], c<span class="op">=</span><span class="st">'b'</span>)<span class="op">;</span></span>
<span id="cb14-12"><a href="#cb14-12" tabindex="-1"></a>ax.scatter(model2_data[:, <span class="dv">0</span>], model2_data[:, <span class="dv">1</span>], c<span class="op">=</span><span class="st">'r'</span>)<span class="op">;</span></span>
<span id="cb14-13"><a href="#cb14-13" tabindex="-1"></a></span>
<span id="cb14-14"><a href="#cb14-14" tabindex="-1"></a>show()</span></code></pre>
</div>
<figure><img src="fig/01-clustering-intro-rendered-unnamed-chunk-10-11.png" width="672" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure>
</div>
<div class="section level3">
<h3 id="predicting-labels">
<strong>Predicting Labels</strong><a class="anchor" aria-label="anchor" href="#predicting-labels"></a>
</h3>
Now we can apply what we have discussed in supervised machine learning
and use the trained model to predict.
<p style="text-align: justify;">
We can get the predictions of the group for new data. Here, for
simplicity, we create test data from the same distribution as the train
data. The label is obtained from the method <code>.predict</code>.
</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>m_features <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a><span class="co"># Seed the random number generator</span></span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a>RANDOM_NUMBER <span class="op">=</span> <span class="dv">111</span></span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a>seed(RANDOM_NUMBER)</span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" tabindex="-1"></a><span class="co"># Data set 1, centered at (20, 20)</span></span>
<span id="cb15-9"><a href="#cb15-9" tabindex="-1"></a></span>
<span id="cb15-10"><a href="#cb15-10" tabindex="-1"></a>mean_11 <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb15-11"><a href="#cb15-11" tabindex="-1"></a>mean_12 <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb15-12"><a href="#cb15-12" tabindex="-1"></a></span>
<span id="cb15-13"><a href="#cb15-13" tabindex="-1"></a>gaussian_1 <span class="op">=</span> randn(n_samples, m_features) <span class="op">+</span> array([mean_11, mean_12])</span>
<span id="cb15-14"><a href="#cb15-14" tabindex="-1"></a></span>
<span id="cb15-15"><a href="#cb15-15" tabindex="-1"></a></span>
<span id="cb15-16"><a href="#cb15-16" tabindex="-1"></a><span class="co"># Data set 2, zero centered and stretched with covariance matrix C</span></span>
<span id="cb15-17"><a href="#cb15-17" tabindex="-1"></a></span>
<span id="cb15-18"><a href="#cb15-18" tabindex="-1"></a>C <span class="op">=</span> array([[<span class="dv">1</span>, <span class="op">-</span><span class="fl">0.7</span>], [<span class="fl">3.5</span>, <span class="fl">.7</span>]])</span>
<span id="cb15-19"><a href="#cb15-19" tabindex="-1"></a></span>
<span id="cb15-20"><a href="#cb15-20" tabindex="-1"></a>gaussian_2 <span class="op">=</span> dot(randn(n_samples, m_features), C)</span>
<span id="cb15-21"><a href="#cb15-21" tabindex="-1"></a></span>
<span id="cb15-22"><a href="#cb15-22" tabindex="-1"></a></span>
<span id="cb15-23"><a href="#cb15-23" tabindex="-1"></a><span class="co"># Concatenate the two Gaussians to obtain the training data set</span></span>
<span id="cb15-24"><a href="#cb15-24" tabindex="-1"></a>X_test <span class="op">=</span> vstack([gaussian_1, gaussian_2])</span>
<span id="cb15-25"><a href="#cb15-25" tabindex="-1"></a></span>
<span id="cb15-26"><a href="#cb15-26" tabindex="-1"></a></span>
<span id="cb15-27"><a href="#cb15-27" tabindex="-1"></a><span class="co"># Predict group</span></span>
<span id="cb15-28"><a href="#cb15-28" tabindex="-1"></a>y_test <span class="op">=</span> clf.predict(X_test)</span>
<span id="cb15-29"><a href="#cb15-29" tabindex="-1"></a></span>
<span id="cb15-30"><a href="#cb15-30" tabindex="-1"></a><span class="bu">print</span>(y_test)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1]</code></pre>
</div>
<p style="text-align: justify;">
For simplicity, fit and predict can be combined with the
<code>.fit_predict</code> method to directly get the labels for each
sample. Here is an example where we fit the model to the test data and
directly extract their predicted labels.
</p>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a>components <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a>clf_2 <span class="op">=</span> GaussianMixture(n_components<span class="op">=</span>components, covariance_type<span class="op">=</span><span class="st">'full'</span>)</span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a>labels <span class="op">=</span> clf_2.fit_predict(X_test)</span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a><span class="bu">print</span>(labels)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]</code></pre>
</div>
<p style="text-align: justify;">
The probabilities of the predictions are obtained from the method
<code>.predict_proba</code>. In this case, all probabilities are 0 and 1
respectively. The model is sure about their group signature.
</p>
<div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a>y_proba <span class="op">=</span> clf.predict_proba(X_test)</span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a>fig, ax <span class="op">=</span> subplots()</span>
<span id="cb19-4"><a href="#cb19-4" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" tabindex="-1"></a>ax.hist(y_proba, bins<span class="op">=</span><span class="dv">10</span>)<span class="op">;</span></span>
<span id="cb19-6"><a href="#cb19-6" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" tabindex="-1"></a>show()</span></code></pre>
</div>
<figure><img src="fig/01-clustering-intro-rendered-unnamed-chunk-13-13.png" width="672" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p style="text-align: justify;">
The <code>.sample_</code> method produces individual samples from the
trained model. It takes the number of required samples as an input
argument and yields the sample values as well as the group for each
sample. Samples for each group are given with probability according to
the group weights.
</p>
<div class="codewrapper sourceCode" id="cb20">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a>samples <span class="op">=</span> clf.sample(<span class="dv">5</span>)</span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" tabindex="-1"></a><span class="bu">print</span>(samples[<span class="dv">0</span>])</span>
<span id="cb20-4"><a href="#cb20-4" tabindex="-1"></a><span class="bu">print</span>(<span class="st">''</span>)</span>
<span id="cb20-5"><a href="#cb20-5" tabindex="-1"></a><span class="bu">print</span>(samples[<span class="dv">1</span>])</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[[20.06076245 20.69983729]
 [21.67317947 21.86459529]
 [ 4.90658144 -0.99981225]
 [ 4.74232451  0.2655483 ]
 [ 0.92148265  2.32782549]]

[0 0 1 1 1]</code></pre>
</div>
<p>We can now redo the example with two distributions that lie closer
together, i.e. making the clustering task harder.</p>
<div class="codewrapper sourceCode" id="cb22">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb22-2"><a href="#cb22-2" tabindex="-1"></a>m_features <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb22-3"><a href="#cb22-3" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" tabindex="-1"></a><span class="co"># Seed the random number generator</span></span>
<span id="cb22-5"><a href="#cb22-5" tabindex="-1"></a>RANDOM_NUMBER <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb22-6"><a href="#cb22-6" tabindex="-1"></a>seed(RANDOM_NUMBER)</span>
<span id="cb22-7"><a href="#cb22-7" tabindex="-1"></a></span>
<span id="cb22-8"><a href="#cb22-8" tabindex="-1"></a><span class="co"># Data set 1, centered at (20, 20)</span></span>
<span id="cb22-9"><a href="#cb22-9" tabindex="-1"></a></span>
<span id="cb22-10"><a href="#cb22-10" tabindex="-1"></a>mean_11 <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb22-11"><a href="#cb22-11" tabindex="-1"></a>mean_12 <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb22-12"><a href="#cb22-12" tabindex="-1"></a></span>
<span id="cb22-13"><a href="#cb22-13" tabindex="-1"></a>gaussian_1 <span class="op">=</span> randn(n_samples, m_features) <span class="op">+</span> array([mean_11, mean_12])</span>
<span id="cb22-14"><a href="#cb22-14" tabindex="-1"></a></span>
<span id="cb22-15"><a href="#cb22-15" tabindex="-1"></a></span>
<span id="cb22-16"><a href="#cb22-16" tabindex="-1"></a><span class="co"># Data set 2, zero centered and stretched with covariance matrix C</span></span>
<span id="cb22-17"><a href="#cb22-17" tabindex="-1"></a></span>
<span id="cb22-18"><a href="#cb22-18" tabindex="-1"></a>C <span class="op">=</span> array([[<span class="dv">1</span>, <span class="op">-</span><span class="fl">0.7</span>], [<span class="fl">3.5</span>, <span class="fl">.7</span>]])</span>
<span id="cb22-19"><a href="#cb22-19" tabindex="-1"></a></span>
<span id="cb22-20"><a href="#cb22-20" tabindex="-1"></a>gaussian_2 <span class="op">=</span> dot(randn(n_samples, m_features), C)</span>
<span id="cb22-21"><a href="#cb22-21" tabindex="-1"></a></span>
<span id="cb22-22"><a href="#cb22-22" tabindex="-1"></a></span>
<span id="cb22-23"><a href="#cb22-23" tabindex="-1"></a><span class="co"># Concatenate the two Gaussians to obtain the training data set</span></span>
<span id="cb22-24"><a href="#cb22-24" tabindex="-1"></a>X_train <span class="op">=</span> vstack([gaussian_1, gaussian_2])</span>
<span id="cb22-25"><a href="#cb22-25" tabindex="-1"></a></span>
<span id="cb22-26"><a href="#cb22-26" tabindex="-1"></a><span class="bu">print</span>(X_train.shape)</span>
<span id="cb22-27"><a href="#cb22-27" tabindex="-1"></a></span>
<span id="cb22-28"><a href="#cb22-28" tabindex="-1"></a>fig, ax <span class="op">=</span> subplots()</span>
<span id="cb22-29"><a href="#cb22-29" tabindex="-1"></a></span>
<span id="cb22-30"><a href="#cb22-30" tabindex="-1"></a>ax.scatter(X_train[:, <span class="dv">0</span>], X_train[:, <span class="dv">1</span>])<span class="op">;</span></span>
<span id="cb22-31"><a href="#cb22-31" tabindex="-1"></a></span>
<span id="cb22-32"><a href="#cb22-32" tabindex="-1"></a>show()</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>(1000, 2)</code></pre>
</div>
<figure><img src="fig/01-clustering-intro-rendered-unnamed-chunk-15-15.png" width="672" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><div class="codewrapper sourceCode" id="cb24">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a>components <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb24-2"><a href="#cb24-2" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" tabindex="-1"></a>clf2 <span class="op">=</span> GaussianMixture(n_components<span class="op">=</span>components)</span>
<span id="cb24-4"><a href="#cb24-4" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" tabindex="-1"></a>clf2.fit(X_train)</span>
<span id="cb24-6"><a href="#cb24-6" tabindex="-1"></a></span>
<span id="cb24-7"><a href="#cb24-7" tabindex="-1"></a>resolution <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb24-8"><a href="#cb24-8" tabindex="-1"></a></span>
<span id="cb24-9"><a href="#cb24-9" tabindex="-1"></a>vec_a <span class="op">=</span> linspace(<span class="op">-</span><span class="fl">40.</span>, <span class="fl">60.</span>, resolution)</span>
<span id="cb24-10"><a href="#cb24-10" tabindex="-1"></a>vec_b <span class="op">=</span> linspace(<span class="op">-</span><span class="fl">20.</span>, <span class="fl">30.</span>, resolution)</span>
<span id="cb24-11"><a href="#cb24-11" tabindex="-1"></a></span>
<span id="cb24-12"><a href="#cb24-12" tabindex="-1"></a>grid_a, grid_b <span class="op">=</span> meshgrid(vec_a, vec_b)</span>
<span id="cb24-13"><a href="#cb24-13" tabindex="-1"></a></span>
<span id="cb24-14"><a href="#cb24-14" tabindex="-1"></a>XY_statespace <span class="op">=</span> c_[grid_a.ravel(), grid_b.ravel()]</span>
<span id="cb24-15"><a href="#cb24-15" tabindex="-1"></a></span>
<span id="cb24-16"><a href="#cb24-16" tabindex="-1"></a>Z_score <span class="op">=</span> clf2.score_samples(XY_statespace)</span>
<span id="cb24-17"><a href="#cb24-17" tabindex="-1"></a></span>
<span id="cb24-18"><a href="#cb24-18" tabindex="-1"></a>Z_s <span class="op">=</span> Z_score.reshape(grid_a.shape)</span>
<span id="cb24-19"><a href="#cb24-19" tabindex="-1"></a></span>
<span id="cb24-20"><a href="#cb24-20" tabindex="-1"></a>fig, ax <span class="op">=</span> subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb24-21"><a href="#cb24-21" tabindex="-1"></a></span>
<span id="cb24-22"><a href="#cb24-22" tabindex="-1"></a>cax <span class="op">=</span> ax.contour(grid_a, grid_b, <span class="op">-</span>Z_s,</span>
<span id="cb24-23"><a href="#cb24-23" tabindex="-1"></a>           norm<span class="op">=</span>LogNorm(vmin<span class="op">=</span><span class="fl">1.0</span>, vmax<span class="op">=</span><span class="fl">1000.0</span>),</span>
<span id="cb24-24"><a href="#cb24-24" tabindex="-1"></a>           levels<span class="op">=</span>logspace(<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">10</span>),</span>
<span id="cb24-25"><a href="#cb24-25" tabindex="-1"></a>           cmap<span class="op">=</span><span class="st">'magma'</span></span>
<span id="cb24-26"><a href="#cb24-26" tabindex="-1"></a>          )</span>
<span id="cb24-27"><a href="#cb24-27" tabindex="-1"></a></span>
<span id="cb24-28"><a href="#cb24-28" tabindex="-1"></a>fig.colorbar(cax)<span class="op">;</span></span>
<span id="cb24-29"><a href="#cb24-29" tabindex="-1"></a></span>
<span id="cb24-30"><a href="#cb24-30" tabindex="-1"></a>ax.scatter(X_train[:, <span class="dv">0</span>], X_train[:, <span class="dv">1</span>], <span class="fl">.8</span>)</span>
<span id="cb24-31"><a href="#cb24-31" tabindex="-1"></a></span>
<span id="cb24-32"><a href="#cb24-32" tabindex="-1"></a>title(<span class="st">'Negative log-likelihood of Prediction'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb24-33"><a href="#cb24-33" tabindex="-1"></a>axis(<span class="st">'tight'</span>)<span class="op">;</span></span>
<span id="cb24-34"><a href="#cb24-34" tabindex="-1"></a></span>
<span id="cb24-35"><a href="#cb24-35" tabindex="-1"></a>show()</span></code></pre>
</div>
<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style>
<div id="sk-container-id-4" class="sk-top-container">
<div class="sk-text-repr-fallback">
<pre>GaussianMixture(n_components=2)</pre>
<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b>
</div>
<div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable">
<input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-4" type="checkbox" checked><label for="sk-estimator-id-4" class="sk-toggleable__label sk-toggleable__label-arrow">GaussianMixture</label><div class="sk-toggleable__content"><pre>GaussianMixture(n_components=2)</pre></div>
</div></div></div>
</div>
<figure><img src="fig/01-clustering-intro-rendered-unnamed-chunk-16-17.png" width="768" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><div class="codewrapper sourceCode" id="cb25">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Model Weights: '</span>)</span>
<span id="cb25-2"><a href="#cb25-2" tabindex="-1"></a><span class="bu">print</span>(clf2.weights_)</span>
<span id="cb25-3"><a href="#cb25-3" tabindex="-1"></a><span class="bu">print</span>(<span class="st">''</span>)</span>
<span id="cb25-4"><a href="#cb25-4" tabindex="-1"></a></span>
<span id="cb25-5"><a href="#cb25-5" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Mean coordinates: '</span>)</span>
<span id="cb25-6"><a href="#cb25-6" tabindex="-1"></a><span class="bu">print</span>(clf2.means_)</span>
<span id="cb25-7"><a href="#cb25-7" tabindex="-1"></a><span class="bu">print</span>(<span class="st">''</span>)</span>
<span id="cb25-8"><a href="#cb25-8" tabindex="-1"></a></span>
<span id="cb25-9"><a href="#cb25-9" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Covariance Matrices: '</span>)</span>
<span id="cb25-10"><a href="#cb25-10" tabindex="-1"></a><span class="bu">print</span>(clf2.covariances_)</span>
<span id="cb25-11"><a href="#cb25-11" tabindex="-1"></a><span class="bu">print</span>(<span class="st">''</span>)</span>
<span id="cb25-12"><a href="#cb25-12" tabindex="-1"></a></span>
<span id="cb25-13"><a href="#cb25-13" tabindex="-1"></a>y_predict <span class="op">=</span> clf2.predict(X_train)</span>
<span id="cb25-14"><a href="#cb25-14" tabindex="-1"></a></span>
<span id="cb25-15"><a href="#cb25-15" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Predicted Labels'</span>)</span>
<span id="cb25-16"><a href="#cb25-16" tabindex="-1"></a><span class="bu">print</span>(y_predict)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Model Weights:
[0.51626883 0.48373117]

Mean coordinates:
[[ 2.02822009  2.0218213 ]
 [ 0.06535904 -0.06576508]]

Covariance Matrices:
[[[ 0.95990893 -0.03850552]
  [-0.03850552  0.94445254]]

 [[14.15940548  1.77380246]
  [ 1.77380246  0.97555916]]]

Predicted Labels
[0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0
 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0
 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0
 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0
 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1
 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0
 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1
 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0
 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0
 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1
 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0
 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1
 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1
 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1
 1]</code></pre>
</div>
</div>
<div class="section level3">
<h3 id="scoring-of-predictions">
<strong>Scoring of Predictions</strong><a class="anchor" aria-label="anchor" href="#scoring-of-predictions"></a>
</h3>
<p style="text-align: justify;">
Knowing the origin of the data we can now compare the predicted labels
with the true labels (the ground truth) and obtain a scoring. A function
provided by Scikit-learn is the (adjusted or unadjusted) <strong>Rand
index</strong>. It measures the similarity of the predicted and the true
assignments. However, random assignment of labels will (by chance) lead
to a number of correct predictions. To adjust for this fact and ensure
that randomly assigned labels get a scoring close to zero, the function
to use is <code>adjusted_rand_score</code>:
</p>
<div class="codewrapper sourceCode" id="cb27">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.cluster <span class="im">import</span> adjusted_rand_score</span>
<span id="cb27-2"><a href="#cb27-2" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" tabindex="-1"></a>y_true <span class="op">=</span> zeros(<span class="dv">2</span><span class="op">*</span>n_samples)</span>
<span id="cb27-4"><a href="#cb27-4" tabindex="-1"></a>y_true[n_samples:] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb27-5"><a href="#cb27-5" tabindex="-1"></a></span>
<span id="cb27-6"><a href="#cb27-6" tabindex="-1"></a>scoring <span class="op">=</span> adjusted_rand_score(y_true, y_predict)</span>
<span id="cb27-7"><a href="#cb27-7" tabindex="-1"></a></span>
<span id="cb27-8"><a href="#cb27-8" tabindex="-1"></a><span class="bu">print</span>(scoring)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span><span class="fl">0.6174145036659798</span></span></code></pre>
</div>
<p style="text-align: justify;">
The result shows that even though the two distributions are strongly
overlapping, there is still a reasonable score based on the known ground
truth.
</p>
<p style="text-align: justify;">
It is important to remember that the ground truth is typically not
known. There are therefore also measures to score the outcome based on
within-data criteria. See <a href="https://en.wikipedia.org/wiki/Cluster_analysis" class="external-link">internal
evaluation of the wikipedia article</a> for some techniques.
</p>
<p style="text-align: justify;">
In general, the outcome of clustering is not easy to assess with
confidence and specific measures need to be developed based on
additional knowledge about the source of the data.
</p>
</div>
</section><section><h2 class="section-heading" id="application-to-example-data">Application to Example Data<a class="anchor" aria-label="anchor" href="#application-to-example-data"></a>
</h2>
<hr class="half-width">
<p>Let us now apply the GMM approach to the example at the beginning of
the lesson.</p>
<div class="codewrapper sourceCode" id="cb29">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" tabindex="-1"></a><span class="im">from</span> pandas <span class="im">import</span> read_csv</span>
<span id="cb29-2"><a href="#cb29-2" tabindex="-1"></a></span>
<span id="cb29-3"><a href="#cb29-3" tabindex="-1"></a>df <span class="op">=</span> read_csv(<span class="st">"data/patients_data.csv"</span>)</span>
<span id="cb29-4"><a href="#cb29-4" tabindex="-1"></a></span>
<span id="cb29-5"><a href="#cb29-5" tabindex="-1"></a>df[<span class="st">'Weight'</span>] <span class="op">=</span> <span class="fl">0.45</span><span class="op">*</span>df[<span class="st">'Weight'</span>]</span>
<span id="cb29-6"><a href="#cb29-6" tabindex="-1"></a>df[<span class="st">'Height'</span>] <span class="op">=</span> <span class="fl">2.54</span><span class="op">*</span>df[<span class="st">'Height'</span>]</span>
<span id="cb29-7"><a href="#cb29-7" tabindex="-1"></a></span>
<span id="cb29-8"><a href="#cb29-8" tabindex="-1"></a>X_train <span class="op">=</span> df[[<span class="st">'Weight'</span>, <span class="st">'Height'</span>]]</span>
<span id="cb29-9"><a href="#cb29-9" tabindex="-1"></a>X_train <span class="op">=</span> X_train.to_numpy()</span>
<span id="cb29-10"><a href="#cb29-10" tabindex="-1"></a></span>
<span id="cb29-11"><a href="#cb29-11" tabindex="-1"></a><span class="bu">print</span>(X_train.shape)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>(100, 2)</code></pre>
</div>
<p>Now we can fit the GMM classifier using the suspected number of two
components.</p>
<div class="codewrapper sourceCode" id="cb31">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" tabindex="-1"></a>clf <span class="op">=</span> GaussianMixture(n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb31-2"><a href="#cb31-2" tabindex="-1"></a></span>
<span id="cb31-3"><a href="#cb31-3" tabindex="-1"></a>clf.fit(X_train)</span>
<span id="cb31-4"><a href="#cb31-4" tabindex="-1"></a></span>
<span id="cb31-5"><a href="#cb31-5" tabindex="-1"></a>resolution <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb31-6"><a href="#cb31-6" tabindex="-1"></a></span>
<span id="cb31-7"><a href="#cb31-7" tabindex="-1"></a>vec_a <span class="op">=</span> linspace(<span class="fl">0.8</span><span class="op">*</span><span class="bu">min</span>(X_train[:,<span class="dv">0</span>]), <span class="fl">1.2</span><span class="op">*</span><span class="bu">max</span>(X_train[:,<span class="dv">0</span>]), resolution)</span>
<span id="cb31-8"><a href="#cb31-8" tabindex="-1"></a>vec_b <span class="op">=</span> linspace(<span class="fl">0.8</span><span class="op">*</span><span class="bu">min</span>(X_train[:,<span class="dv">1</span>]), <span class="fl">1.2</span><span class="op">*</span><span class="bu">max</span>(X_train[:,<span class="dv">1</span>]), resolution)</span>
<span id="cb31-9"><a href="#cb31-9" tabindex="-1"></a></span>
<span id="cb31-10"><a href="#cb31-10" tabindex="-1"></a>grid_a, grid_b <span class="op">=</span> meshgrid(vec_a, vec_b)</span>
<span id="cb31-11"><a href="#cb31-11" tabindex="-1"></a></span>
<span id="cb31-12"><a href="#cb31-12" tabindex="-1"></a>XY_statespace <span class="op">=</span> c_[grid_a.ravel(), grid_b.ravel()]</span>
<span id="cb31-13"><a href="#cb31-13" tabindex="-1"></a></span>
<span id="cb31-14"><a href="#cb31-14" tabindex="-1"></a>Z_score <span class="op">=</span> clf.score_samples(XY_statespace)</span>
<span id="cb31-15"><a href="#cb31-15" tabindex="-1"></a></span>
<span id="cb31-16"><a href="#cb31-16" tabindex="-1"></a>Z_s <span class="op">=</span> Z_score.reshape(grid_a.shape)</span>
<span id="cb31-17"><a href="#cb31-17" tabindex="-1"></a></span>
<span id="cb31-18"><a href="#cb31-18" tabindex="-1"></a></span>
<span id="cb31-19"><a href="#cb31-19" tabindex="-1"></a>fig, ax <span class="op">=</span> subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb31-20"><a href="#cb31-20" tabindex="-1"></a></span>
<span id="cb31-21"><a href="#cb31-21" tabindex="-1"></a>cax <span class="op">=</span> ax.contour(grid_a, grid_b, <span class="op">-</span>Z_s,</span>
<span id="cb31-22"><a href="#cb31-22" tabindex="-1"></a>           norm<span class="op">=</span>LogNorm(vmin<span class="op">=</span><span class="fl">1.0</span>, vmax<span class="op">=</span><span class="fl">1000.0</span>),</span>
<span id="cb31-23"><a href="#cb31-23" tabindex="-1"></a>           levels<span class="op">=</span>logspace(<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">10</span>),</span>
<span id="cb31-24"><a href="#cb31-24" tabindex="-1"></a>           cmap<span class="op">=</span><span class="st">'magma'</span></span>
<span id="cb31-25"><a href="#cb31-25" tabindex="-1"></a>          )</span>
<span id="cb31-26"><a href="#cb31-26" tabindex="-1"></a></span>
<span id="cb31-27"><a href="#cb31-27" tabindex="-1"></a>fig.colorbar(cax)<span class="op">;</span></span>
<span id="cb31-28"><a href="#cb31-28" tabindex="-1"></a></span>
<span id="cb31-29"><a href="#cb31-29" tabindex="-1"></a>ax.scatter(X_train[:, <span class="dv">0</span>], X_train[:, <span class="dv">1</span>], <span class="fl">.8</span>)</span>
<span id="cb31-30"><a href="#cb31-30" tabindex="-1"></a></span>
<span id="cb31-31"><a href="#cb31-31" tabindex="-1"></a>title(<span class="st">'Negative log-likelihood of Prediction'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb31-32"><a href="#cb31-32" tabindex="-1"></a>axis(<span class="st">'tight'</span>)<span class="op">;</span></span>
<span id="cb31-33"><a href="#cb31-33" tabindex="-1"></a></span>
<span id="cb31-34"><a href="#cb31-34" tabindex="-1"></a>show()</span></code></pre>
</div>
<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style>
<div id="sk-container-id-5" class="sk-top-container">
<div class="sk-text-repr-fallback">
<pre>GaussianMixture(n_components=2)</pre>
<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b>
</div>
<div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable">
<input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-5" type="checkbox" checked><label for="sk-estimator-id-5" class="sk-toggleable__label sk-toggleable__label-arrow">GaussianMixture</label><div class="sk-toggleable__content"><pre>GaussianMixture(n_components=2)</pre></div>
</div></div></div>
</div>
<figure><img src="fig/01-clustering-intro-rendered-unnamed-chunk-20-19.png" width="768" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p style="text-align: justify;">
These predictions can now be compared with labels in the data, for
example the Gender. To check the outcome of the fitted model versus the
gender, we obtain the predicted labels from the model. We can compare
this with the Gender labels in the data:
</p>
<div class="codewrapper sourceCode" id="cb32">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" tabindex="-1"></a>y_predict <span class="op">=</span> clf.predict(X_train)</span>
<span id="cb32-2"><a href="#cb32-2" tabindex="-1"></a></span>
<span id="cb32-3"><a href="#cb32-3" tabindex="-1"></a>gender_boolean <span class="op">=</span> df[<span class="st">'Gender'</span>] <span class="op">==</span> <span class="st">'Female'</span></span>
<span id="cb32-4"><a href="#cb32-4" tabindex="-1"></a></span>
<span id="cb32-5"><a href="#cb32-5" tabindex="-1"></a>y_gender <span class="op">=</span> gender_boolean.to_numpy()</span>
<span id="cb32-6"><a href="#cb32-6" tabindex="-1"></a></span>
<span id="cb32-7"><a href="#cb32-7" tabindex="-1"></a>scoring <span class="op">=</span> adjusted_rand_score(y_gender, y_predict)</span>
<span id="cb32-8"><a href="#cb32-8" tabindex="-1"></a></span>
<span id="cb32-9"><a href="#cb32-9" tabindex="-1"></a><span class="bu">print</span>(scoring)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span><span class="fl">1.0</span></span></code></pre>
</div>
<p>In this case, the predictions from the GMM coincide 100 % with the
gender label in the data. The outcome is therefore perfect in both
cases.</p>
<p>We can also compare the predictions with the smoker labels:</p>
<div class="codewrapper sourceCode" id="cb34">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" tabindex="-1"></a>y_smoker <span class="op">=</span> df[<span class="st">'Smoker'</span>]</span>
<span id="cb34-2"><a href="#cb34-2" tabindex="-1"></a></span>
<span id="cb34-3"><a href="#cb34-3" tabindex="-1"></a>scoring <span class="op">=</span> adjusted_rand_score(y_smoker, y_predict)</span>
<span id="cb34-4"><a href="#cb34-4" tabindex="-1"></a></span>
<span id="cb34-5"><a href="#cb34-5" tabindex="-1"></a><span class="bu">print</span>(scoring)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span><span class="fl">0.039367492745118096</span></span></code></pre>
</div>
<p>This result shows that the GMM labelling is arbitrary when compared
the smoker labels in the data.</p>
<p>From the trained model we create the individual predicted
distributions for each group.</p>
<div class="codewrapper sourceCode" id="cb36">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" tabindex="-1"></a>group1_mean <span class="op">=</span> clf.means_[<span class="dv">0</span>]</span>
<span id="cb36-2"><a href="#cb36-2" tabindex="-1"></a>group1_cov  <span class="op">=</span> clf.covariances_[<span class="dv">0</span>]</span>
<span id="cb36-3"><a href="#cb36-3" tabindex="-1"></a></span>
<span id="cb36-4"><a href="#cb36-4" tabindex="-1"></a>group2_mean <span class="op">=</span> clf.means_[<span class="dv">1</span>]</span>
<span id="cb36-5"><a href="#cb36-5" tabindex="-1"></a>group2_cov  <span class="op">=</span> clf.covariances_[<span class="dv">1</span>]</span>
<span id="cb36-6"><a href="#cb36-6" tabindex="-1"></a></span>
<span id="cb36-7"><a href="#cb36-7" tabindex="-1"></a>samples <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb36-8"><a href="#cb36-8" tabindex="-1"></a></span>
<span id="cb36-9"><a href="#cb36-9" tabindex="-1"></a></span>
<span id="cb36-10"><a href="#cb36-10" tabindex="-1"></a>group1_data <span class="op">=</span> multivariate_normal(group1_mean, group1_cov, samples)</span>
<span id="cb36-11"><a href="#cb36-11" tabindex="-1"></a>group2_data <span class="op">=</span> multivariate_normal(group2_mean, group2_cov, samples)</span>
<span id="cb36-12"><a href="#cb36-12" tabindex="-1"></a></span>
<span id="cb36-13"><a href="#cb36-13" tabindex="-1"></a>fig, ax <span class="op">=</span> subplots(ncols<span class="op">=</span><span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb36-14"><a href="#cb36-14" tabindex="-1"></a></span>
<span id="cb36-15"><a href="#cb36-15" tabindex="-1"></a>ax[<span class="dv">1</span>].scatter(group1_data[:, <span class="dv">0</span>], group1_data[:, <span class="dv">1</span>], c<span class="op">=</span><span class="st">'r'</span>)<span class="op">;</span></span>
<span id="cb36-16"><a href="#cb36-16" tabindex="-1"></a>ax[<span class="dv">1</span>].scatter(group2_data[:, <span class="dv">0</span>], group2_data[:, <span class="dv">1</span>], c<span class="op">=</span><span class="st">'b'</span>)<span class="op">;</span></span>
<span id="cb36-17"><a href="#cb36-17" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">'Height'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb36-18"><a href="#cb36-18" tabindex="-1"></a></span>
<span id="cb36-19"><a href="#cb36-19" tabindex="-1"></a>ax[<span class="dv">0</span>].scatter(df[<span class="st">'Weight'</span>], df[<span class="st">'Height'</span>])<span class="op">;</span></span>
<span id="cb36-20"><a href="#cb36-20" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">'Height'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb36-21"><a href="#cb36-21" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="st">'Weight'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb36-22"><a href="#cb36-22" tabindex="-1"></a></span>
<span id="cb36-23"><a href="#cb36-23" tabindex="-1"></a>fig.suptitle(<span class="st">'Scatter plot from Data (left) and Model (right)'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)<span class="op">;</span></span>
<span id="cb36-24"><a href="#cb36-24" tabindex="-1"></a></span>
<span id="cb36-25"><a href="#cb36-25" tabindex="-1"></a>show()</span></code></pre>
</div>
<figure><img src="fig/01-clustering-intro-rendered-unnamed-chunk-23-21.png" width="1152" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure></section><section><h2 class="section-heading" id="exercises">Exercises<a class="anchor" aria-label="anchor" href="#exercises"></a>
</h2>
<hr class="half-width">
<div id="end-of-chapter-exercises" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="end-of-chapter-exercises" class="callout-inner">
<h3 class="callout-title">End of chapter Exercises</h3>
<div class="callout-content">
<p>Create the training and prediction workflow as above for a data set
with two other features, namely: Diastole and Systole values from the
‘patients_data.csv’ file.</p>
<ol style="list-style-type: decimal">
<li><p>Extract the Diastole and Systole columns.</p></li>
<li><p>Use the data to fit a Gaussian model with 2 components and create
a state space contour plot of the negative log likelihood with scattered
data superimposed.</p></li>
<li><p>Extract the model weights, the means of the two Gaussians and
their corresponding covariance matrices.</p></li>
<li><p>Calculate the adjusted random score for the labels ‘gender’ and
‘smoker’ in the data to estimate whether these have som overlap with the
model fit.</p></li>
<li><p>Compare the original scatter plot versus the model generated
scatter plot. Use a total of 100 samples for the model generated data
and distribute them according to the model weights.</p></li>
<li><p>Repeat the plot multiple times to see how the degree of overlap
in the model output changes with each choice of samples from the fitted
distribution.</p></li>
<li><p>Create corresponding histograms of the Diastolic and Systolic
blood pressure values from data and model. Try to guess where the
differences in appearance come from.</p></li>
</ol>
<p style="text-align: justify;">
The data show systematic gaps in the histogram meaning that some values
do not occur (integer values only). In contrast, the model data from the
random number generator can take any value. Therefore the counts per bin
are generally lower for the model.
</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">
  Solutions are provided after assignments are marked.
  </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">

</div>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>The automated assignment of data points to distinct groups is called
<code>clustering</code>.</li>
<li>Gaussian Mixture Models (GMM) is one example of cluster
analysis.</li>
<li>
<code>.fit</code>, <code>..score_samples</code> and
<code>.predict</code> are some of the key methods in GMM
clustering.</li>
<li>
<code>adjusted_rand_score</code> method randomly assigns labels for
prediction scoring.</li>
</ul>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</section></section><section id="aio-02-clustering-image"><p>Content from <a href="02-clustering-image.html">Clustering Images</a></p>
<hr>
<p>Last updated on 2025-01-27 |

        <a href="https://github.com/carpentries/workbench-template-md/edit/main/episodes/02-clustering-image.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<p><a href="https://drive.usercontent.google.com/u/1/uc?id=1fR6j_lbrHp2Ojwi24jy9Og-PnHGGT16a&amp;export=download" class="external-link"><strong>Download
Chapter notebook (ipynb)</strong></a></p>
<p><a href="https://drive.usercontent.google.com/u/1/uc?id=1UvmmoebKwkDX8Wk3iUHMJklp1fTyzzb1&amp;export=download" class="external-link"><strong>Download
Chapter PDF</strong></a></p>
<p><a href="https://docs.google.com/forms/d/e/1FAIpQLSdr0capF7jloJhPH3Pki1B3LZoKOG16poOpuVJ7SL2LkwLHQA/viewform?pli=1" class="external-link"><span style="color: rgb(255, 0, 0);"><strong>Mandatory Lesson Feedback
Survey</strong></span></a></p>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What makes image data unique for machine learning?</li>
<li>How can MR images be clustered and segmented?</li>
<li>How can segmentation be improved?</li>
<li>How do we visualise clustered image data?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Learning to work with a domain-specific Python package.</li>
<li>Clustering with Gaussian Mixture Models (GMM) to segment a medical
image.</li>
<li>Combining information from different imaging modalities for improved
segmentation.</li>
<li>Understanding strategies to visualise clustered output.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<br><p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/IuvEj39rM7k" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</p>
<br><p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/6LaIhEdco8g" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</p>
<p><br></p>
<div id="prereq1" class="callout prereq">
<div class="callout-square">
<i class="callout-icon" data-feather="check"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Prerequisite</h3>
<div class="callout-content">
<ul>
<li><a href="https://learntodiscover.github.io/Data_Handling/03-image_handling.html" class="external-link">Data
Handling Images</a></li>
</ul>
</div>
</div>
</div>
<section><h2 class="section-heading" id="concept">Concept<a class="anchor" aria-label="anchor" href="#concept"></a>
</h2>
<hr class="half-width">
<div class="section level3">
<h3 id="image-segmentation-with-clustering">
<strong>Image Segmentation with Clustering</strong><a class="anchor" aria-label="anchor" href="#image-segmentation-with-clustering"></a>
</h3>
<p style="text-align: justify;">
Medical imaging techniques are a valuable tool for probing diseases and
disorders non-invasively. Processing medical images often consists of an
expert, such as a radiologist, looking at the images and identifying,
labelling and characterising potential lesions. This process can be
time-consuming and reliant on a well-trained expert’s eye. To make
medical imaging techniques feasible in circumstances where there may be
insufficient time or resources for expert labelling, there is current
research into using artificial intelligence to label images. There are
many supervised learning techniques that utilise previously labelled
data from experts to train a computer algorithm to recognise certain
features of an image. However, this may require large amounts of data
that were previously labelled, which again is not always available. An
alternative approach is to use unsupervised learning strategies, such as
clustering, to group images into different regions. The interpretation
of these regions may be ambiguous, but with some previous knowledge, we
may still use it to infer information from an image.
</p>
</div>
<div class="section level3">
<h3 id="medical-image-example">
<strong>Medical Image Example</strong><a class="anchor" aria-label="anchor" href="#medical-image-example"></a>
</h3>
<p style="text-align: justify;">
The example used in this lesson is part of the National Cancer
Institute’s Clinical Proteomic Tumor Analysis Consortium Glioblastoma
Multiforme (CPTAC-GBM) cohort and is available at The Cancer Imaging
Archive: <a href="https://wiki.cancerimagingarchive.net/display/Public/CPTAC-GBM" class="external-link uri">https://wiki.cancerimagingarchive.net/display/Public/CPTAC-GBM</a>.
For each subject in this study, several different brain MRI scans were
performed, each of which gives different contrast in the brain. Each
subject has been diagnosed with glioblastoma, and a tumour is visible in
the MRI scans. To analyse the images and to, for example, estimate the
size of the tumour, we may wish to segment the brain into healthy tissue
and tumour tissue. The figure below shows four images in the different
modalities.
</p>
<figure><img src="fig/brain_images.png" alt="Figure 1: Example image, different MRI modalities imaging glioblastoma" class="figure mx-auto d-block"><div class="figcaption">Figure 1: Example image, different MRI
modalities imaging glioblastoma</div>
</figure>
</div>
</section><section><h2 class="section-heading" id="work-through-example">Work Through Example<a class="anchor" aria-label="anchor" href="#work-through-example"></a>
</h2>
<hr class="half-width">
<div class="section level3">
<h3 id="code-preparation">
<strong>Code Preparation</strong><a class="anchor" aria-label="anchor" href="#code-preparation"></a>
</h3>
<p style="text-align: justify;">
We first import the modules needed for this lesson. We use Numpy to
store and process images and we use <strong>nibabel</strong> to read the
MRI images, which have a file type called ‘nifti’. Nibabel is freely
available for download here: <a href="https://nipy.org/nibabel/" class="external-link uri">https://nipy.org/nibabel/</a>
</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> zeros, <span class="bu">sum</span>, stack</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> nibabel <span class="im">as</span> nib</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">from</span> matplotlib.pyplot <span class="im">import</span> subplots, tight_layout, show</span></code></pre>
</div>
<div id="note" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="note" class="callout-inner">
<h3 class="callout-title">Note</h3>
<div class="callout-content">
<p>Note how we import the nibabel package as ‘nib’. You can use any
abbreviation to access the package’s functions from within your
programme.</p>
</div>
</div>
</div>
<p>To familiarise yourself with the nibabel package, try the <a href="https://nipy.org/nibabel/gettingstarted.html" class="external-link">Getting started
tutorial</a> using an example image file.</p>
</div>
<div class="section level3">
<h3 id="reading-images-into-numpy-arrays">
<strong>Reading Images into Numpy Arrays</strong><a class="anchor" aria-label="anchor" href="#reading-images-into-numpy-arrays"></a>
</h3>
<p style="text-align: justify;">
Next, we want to use the nibabel package to read the MRI images into
Numpy arrays. In this example, we use four different images that were
acquired with different MRI protocols.
</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>img_3d <span class="op">=</span> nib.load(<span class="st">'fig/t1.nii'</span>)</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>img1 <span class="op">=</span> img_3d.get_fdata()</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>img_3d <span class="op">=</span> nib.load(<span class="st">'fig/t1_contrast.nii'</span>)</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>img2 <span class="op">=</span> img_3d.get_fdata()</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>img_3d <span class="op">=</span> nib.load(<span class="st">'fig/flair.nii'</span>)</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>img3 <span class="op">=</span> img_3d.get_fdata()</span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>img_3d <span class="op">=</span> nib.load(<span class="st">'fig/adc.nii'</span>)</span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a>img4 <span class="op">=</span> img_3d.get_fdata()</span></code></pre>
</div>
<p>Let’s have a look at the data shape:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="bu">print</span>(img1.shape)</span></code></pre>
</div>
(256, 256, 32)
<p style="text-align: justify;">
For plotting, we select a slice from the images. In this example we will
view axial slices, i.e. slices from the last dimension. Thus, we choose
a slice number between 0 and 31, here we go with slice 20 and plot it.
</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>img_slice <span class="op">=</span> <span class="dv">20</span></span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>fig, ax <span class="op">=</span> subplots(nrows<span class="op">=</span><span class="dv">1</span>, ncols<span class="op">=</span><span class="dv">4</span>, figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">10</span>))</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>ax[<span class="dv">0</span>].imshow(img1[:, :, img_slice], cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">"T1"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>ax[<span class="dv">1</span>].imshow(img2[:, :, img_slice], cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">"T1 with contrast agent"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>ax[<span class="dv">2</span>].imshow(img3[:, :, img_slice], cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a>ax[<span class="dv">2</span>].set_title(<span class="st">"FLAIR"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a>ax[<span class="dv">3</span>].imshow(img4[:, :, img_slice], cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a>ax[<span class="dv">3</span>].set_title(<span class="st">"Apparent diffusion coefficient"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)<span class="op">;</span></span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a>show()</span></code></pre>
</div>
<figure><img src="fig/02-clustering-image-rendered-unnamed-chunk-5-1.png" width="1920" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure>
</div>
<div class="section level3">
<h3 id="data-pre-processing">
<strong>Data Pre-processing</strong><a class="anchor" aria-label="anchor" href="#data-pre-processing"></a>
</h3>
<p>To analyse the images, we need to do a bit of pre-processing. First
of all, let us plot the histogram of the voxel (volume pixel)
intensities.</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>fig, ax <span class="op">=</span> subplots(nrows<span class="op">=</span><span class="dv">1</span>, ncols<span class="op">=</span><span class="dv">4</span>, figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">4</span>))</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>ax[<span class="dv">0</span>].hist(img1.flatten(), bins<span class="op">=</span><span class="dv">50</span>)<span class="op">;</span></span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">"T1"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">"Intensity"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>ax[<span class="dv">1</span>].hist(img2.flatten(), bins<span class="op">=</span><span class="dv">50</span>)<span class="op">;</span></span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">"T1 with contrast agent"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">"Intensity"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a>ax[<span class="dv">2</span>].hist(img3.flatten(), bins<span class="op">=</span><span class="dv">50</span>)<span class="op">;</span></span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a>ax[<span class="dv">2</span>].set_title(<span class="st">"FLAIR"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb6-13"><a href="#cb6-13" tabindex="-1"></a>ax[<span class="dv">2</span>].set_xlabel(<span class="st">"Intensity"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb6-14"><a href="#cb6-14" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" tabindex="-1"></a>ax[<span class="dv">3</span>].hist(img4.flatten(), bins<span class="op">=</span><span class="dv">50</span>)<span class="op">;</span></span>
<span id="cb6-16"><a href="#cb6-16" tabindex="-1"></a>ax[<span class="dv">3</span>].set_title(<span class="st">"Apparent diffusion coefficient"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb6-17"><a href="#cb6-17" tabindex="-1"></a>ax[<span class="dv">3</span>].set_xlabel(<span class="st">"Intensity"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb6-18"><a href="#cb6-18" tabindex="-1"></a></span>
<span id="cb6-19"><a href="#cb6-19" tabindex="-1"></a>tight_layout()</span>
<span id="cb6-20"><a href="#cb6-20" tabindex="-1"></a></span>
<span id="cb6-21"><a href="#cb6-21" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Number of voxels with intensity equal to 0 is: </span><span class="sc">%d</span><span class="st">'</span><span class="op">%</span><span class="bu">sum</span>(img1<span class="op">==</span><span class="dv">0</span>))</span>
<span id="cb6-22"><a href="#cb6-22" tabindex="-1"></a><span class="bu">print</span>(<span class="st">''</span>)</span>
<span id="cb6-23"><a href="#cb6-23" tabindex="-1"></a></span>
<span id="cb6-24"><a href="#cb6-24" tabindex="-1"></a>show()</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Number of voxels with intensity equal to 0 is: 1848804</code></pre>
</div>
<figure><img src="fig/02-clustering-image-rendered-unnamed-chunk-6-3.png" width="1920" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p style="text-align: justify;">
As we can see from these histograms, a large number of the values are
zero. This corresponds to the background voxels shown in black. We want
to remove these voxels, as they are not useful for our analysis. For
this, we create a binary mask and apply it to the images.
</p>
<div id="note-1" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="note-1" class="callout-inner">
<h3 class="callout-title">Note</h3>
<div class="callout-content">
<p>Note the use of <code>tight_layout</code> in the cell above. It is a
<a href="https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.tight_layout.html" class="external-link">Matplotlib
function</a> to pad between the figure edge and the edges of subplots.
This can be useful to avoid overlap of figures and labels. The keyword
parameter <code>pad</code> is set to 1.08 by default.</p>
</div>
</div>
</div>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>mask <span class="op">=</span> (img1<span class="op">&gt;</span><span class="dv">0</span>) <span class="op">&amp;</span> (img2<span class="op">&gt;</span><span class="dv">0</span>) <span class="op">&amp;</span> (img3<span class="op">&gt;</span><span class="dv">0</span>) <span class="op">&amp;</span> (img4<span class="op">&gt;</span><span class="dv">0</span>)</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>img1_nz <span class="op">=</span> img1[mask]</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>img2_nz <span class="op">=</span> img2[mask]</span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>img3_nz <span class="op">=</span> img3[mask]</span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>img4_nz <span class="op">=</span> img4[mask]</span></code></pre>
</div>
<p>With the mask applied, let us plot the histograms of the non-zero
voxels again:</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>fig, ax <span class="op">=</span> subplots(<span class="dv">1</span>, <span class="dv">4</span>, figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">4</span>))</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>ax[<span class="dv">0</span>].hist(img1_nz, bins<span class="op">=</span><span class="dv">50</span>)<span class="op">;</span></span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">"T1"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">"Intensity"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a>ax[<span class="dv">1</span>].hist(img2_nz, bins<span class="op">=</span><span class="dv">50</span>)<span class="op">;</span></span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">"T1 with contrast agent"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">"Intensity"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb9-10"><a href="#cb9-10" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" tabindex="-1"></a>ax[<span class="dv">2</span>].hist(img3_nz, bins<span class="op">=</span><span class="dv">50</span>)<span class="op">;</span></span>
<span id="cb9-12"><a href="#cb9-12" tabindex="-1"></a>ax[<span class="dv">2</span>].set_title(<span class="st">"FLAIR"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb9-13"><a href="#cb9-13" tabindex="-1"></a>ax[<span class="dv">2</span>].set_xlabel(<span class="st">"Intensity"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb9-14"><a href="#cb9-14" tabindex="-1"></a></span>
<span id="cb9-15"><a href="#cb9-15" tabindex="-1"></a>ax[<span class="dv">3</span>].hist(img4_nz, bins<span class="op">=</span><span class="dv">50</span>)<span class="op">;</span></span>
<span id="cb9-16"><a href="#cb9-16" tabindex="-1"></a>ax[<span class="dv">3</span>].set_title(<span class="st">"Apparent diffusion coefficient"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb9-17"><a href="#cb9-17" tabindex="-1"></a>ax[<span class="dv">3</span>].set_xlabel(<span class="st">"Intensity"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb9-18"><a href="#cb9-18" tabindex="-1"></a></span>
<span id="cb9-19"><a href="#cb9-19" tabindex="-1"></a>tight_layout()</span>
<span id="cb9-20"><a href="#cb9-20" tabindex="-1"></a></span>
<span id="cb9-21"><a href="#cb9-21" tabindex="-1"></a>show()</span></code></pre>
</div>
<figure><img src="fig/02-clustering-image-rendered-unnamed-chunk-8-5.png" width="1920" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>We can see that the data is no longer confounded by the zero-valued
background voxels. The distribution of relevant intensities now becomes
apparent.</p>
</div>
<div class="section level3">
<h3 id="image-scaling">
<strong>Image scaling</strong><a class="anchor" aria-label="anchor" href="#image-scaling"></a>
</h3>
<p style="text-align: justify;">
In many machine learning applications (both supervised and unsupervised)
an additional step of data preparation consists in normalising or
scaling, i.e. adjustment of the values under certain conditions. For
example, the numbers in a data file are all positive and very large but
the algorithms work best for numbers with mean zero and variance 1. In
Scikit-learn this can be done by using <code>fit_transform</code> for an
instance of the <code>StandardScaler</code>.
</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a>img1_scaled <span class="op">=</span> scaler.fit_transform(img1_nz.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a>img2_scaled <span class="op">=</span> scaler.fit_transform(img2_nz.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a>img3_scaled <span class="op">=</span> scaler.fit_transform(img3_nz.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a>img4_scaled <span class="op">=</span> scaler.fit_transform(img4_nz.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" tabindex="-1"></a>fig, ax <span class="op">=</span> subplots(<span class="dv">1</span>, <span class="dv">4</span>, figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">4</span>))</span>
<span id="cb10-11"><a href="#cb10-11" tabindex="-1"></a></span>
<span id="cb10-12"><a href="#cb10-12" tabindex="-1"></a>ax[<span class="dv">0</span>].hist(img1_scaled, bins<span class="op">=</span><span class="dv">50</span>)<span class="op">;</span></span>
<span id="cb10-13"><a href="#cb10-13" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">"T1"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb10-14"><a href="#cb10-14" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">"Intensity"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb10-15"><a href="#cb10-15" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" tabindex="-1"></a>ax[<span class="dv">1</span>].hist(img2_scaled, bins<span class="op">=</span><span class="dv">50</span>)<span class="op">;</span></span>
<span id="cb10-17"><a href="#cb10-17" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">"T1 with contrast agent"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb10-18"><a href="#cb10-18" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">"Intensity"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb10-19"><a href="#cb10-19" tabindex="-1"></a></span>
<span id="cb10-20"><a href="#cb10-20" tabindex="-1"></a>ax[<span class="dv">2</span>].hist(img3_scaled, bins<span class="op">=</span><span class="dv">50</span>)<span class="op">;</span></span>
<span id="cb10-21"><a href="#cb10-21" tabindex="-1"></a>ax[<span class="dv">2</span>].set_title(<span class="st">"FLAIR"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb10-22"><a href="#cb10-22" tabindex="-1"></a>ax[<span class="dv">2</span>].set_xlabel(<span class="st">"Intensity"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb10-23"><a href="#cb10-23" tabindex="-1"></a></span>
<span id="cb10-24"><a href="#cb10-24" tabindex="-1"></a>ax[<span class="dv">3</span>].hist(img4_scaled, bins<span class="op">=</span><span class="dv">50</span>)<span class="op">;</span></span>
<span id="cb10-25"><a href="#cb10-25" tabindex="-1"></a>ax[<span class="dv">3</span>].set_title(<span class="st">"Apparent diffusion coefficient"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb10-26"><a href="#cb10-26" tabindex="-1"></a>ax[<span class="dv">3</span>].set_xlabel(<span class="st">"Intensity"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb10-27"><a href="#cb10-27" tabindex="-1"></a></span>
<span id="cb10-28"><a href="#cb10-28" tabindex="-1"></a>tight_layout()</span>
<span id="cb10-29"><a href="#cb10-29" tabindex="-1"></a></span>
<span id="cb10-30"><a href="#cb10-30" tabindex="-1"></a>show()</span></code></pre>
</div>
<figure><img src="fig/02-clustering-image-rendered-unnamed-chunk-9-7.png" width="1920" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>If you compare the histograms, you can see that the values in the
data have changed (horizontal axis) but the shapes of the distributions
are the same.</p>
<p>We are not pursuing this further here but you are encouraged to re-do
the clustering below with the scaled images and check if there are any
differences.</p>
</div>
<div class="section level3">
<h3 id="image-segmentation-with-clustering-1">
<strong>Image Segmentation with Clustering</strong><a class="anchor" aria-label="anchor" href="#image-segmentation-with-clustering-1"></a>
</h3>
<p style="text-align: justify;">
After this data cleaning step, we can proceed with our analysis. We want
to segment the images into brain tissue and tumour tissue. It is not
obvious how to do this, as the intensity values in the above histogram
are continuous with only one major peak in intensity. We will
nonetheless attempt to cluster the images using a Gaussian Mixture Model
(GMM).
</p>
<p>First, we import the GMM class from Scikit-learn.</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="im">from</span> sklearn.mixture <span class="im">import</span> GaussianMixture</span></code></pre>
</div>
<p style="text-align: justify;">
We then fit the instantiated model with a few different numbers of
clusters (argument <code>n_components</code>, between n = 2-4)
individually for each image. We use the <code>fit_predict</code> method
to simultaneously fit and label the images. Note that we also add 1 to
each image label. This is because each data point gets labelled with a
number from 0 to <em>n-1</em> where <em>n</em> is the number of clusters
used to fit the model. At the plotting stage, we do not want any of our
labels to be equal to 0, as this corresponds to the background.
</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>RANDOM_SEED <span class="op">=</span> <span class="dv">123</span></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a>gmm_2 <span class="op">=</span> gmm_2 <span class="op">=</span> GaussianMixture(<span class="dv">2</span>, random_state<span class="op">=</span>RANDOM_SEED)</span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a>img1_n2_labels <span class="op">=</span> gmm_2.fit_predict(img1_nz.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a>img1_n2_labels <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a>gmm_3 <span class="op">=</span> GaussianMixture(<span class="dv">3</span>, random_state<span class="op">=</span>RANDOM_SEED)</span>
<span id="cb12-8"><a href="#cb12-8" tabindex="-1"></a>img1_n3_labels <span class="op">=</span> gmm_3.fit_predict(img1_nz.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb12-9"><a href="#cb12-9" tabindex="-1"></a>img1_n3_labels <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb12-10"><a href="#cb12-10" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" tabindex="-1"></a>gmm_4 <span class="op">=</span> GaussianMixture(<span class="dv">4</span>, random_state<span class="op">=</span>RANDOM_SEED)</span>
<span id="cb12-12"><a href="#cb12-12" tabindex="-1"></a>img1_n4_labels <span class="op">=</span> gmm_4.fit_predict(img1_nz.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb12-13"><a href="#cb12-13" tabindex="-1"></a>img1_n4_labels <span class="op">+=</span> <span class="dv">1</span></span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a>gmm_2 <span class="op">=</span> GaussianMixture(<span class="dv">2</span>, random_state<span class="op">=</span>RANDOM_SEED)</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>img2_n2_labels <span class="op">=</span> gmm_2.fit_predict(img2_nz.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>img2_n2_labels <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a>gmm_3 <span class="op">=</span> GaussianMixture(<span class="dv">3</span>, random_state<span class="op">=</span>RANDOM_SEED)</span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a>img2_n3_labels <span class="op">=</span> gmm_3.fit_predict(img2_nz.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb13-7"><a href="#cb13-7" tabindex="-1"></a>img2_n3_labels <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb13-8"><a href="#cb13-8" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" tabindex="-1"></a>gmm_4 <span class="op">=</span> GaussianMixture(<span class="dv">4</span>, random_state<span class="op">=</span>RANDOM_SEED)</span>
<span id="cb13-10"><a href="#cb13-10" tabindex="-1"></a>img2_n4_labels <span class="op">=</span> gmm_4.fit_predict(img2_nz.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb13-11"><a href="#cb13-11" tabindex="-1"></a>img2_n4_labels <span class="op">+=</span> <span class="dv">1</span></span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>gmm_2 <span class="op">=</span> GaussianMixture(<span class="dv">2</span>, random_state<span class="op">=</span>RANDOM_SEED)</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a>img3_n2_labels <span class="op">=</span> gmm_2.fit_predict(img3_nz.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a>img3_n2_labels <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a>gmm_3 <span class="op">=</span> GaussianMixture(<span class="dv">3</span>, random_state<span class="op">=</span>RANDOM_SEED)</span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a>img3_n3_labels <span class="op">=</span> gmm_3.fit_predict(img3_nz.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a>img3_n3_labels <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb14-8"><a href="#cb14-8" tabindex="-1"></a></span>
<span id="cb14-9"><a href="#cb14-9" tabindex="-1"></a>gmm_4 <span class="op">=</span> GaussianMixture(<span class="dv">4</span>, random_state<span class="op">=</span>RANDOM_SEED)</span>
<span id="cb14-10"><a href="#cb14-10" tabindex="-1"></a>img3_n4_labels <span class="op">=</span> gmm_4.fit_predict(img3_nz.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb14-11"><a href="#cb14-11" tabindex="-1"></a>img3_n4_labels <span class="op">+=</span> <span class="dv">1</span></span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>gmm_2 <span class="op">=</span> GaussianMixture(<span class="dv">2</span>, random_state<span class="op">=</span>RANDOM_SEED)</span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>img4_n2_labels <span class="op">=</span> gmm_2.fit_predict(img4_nz.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a>img4_n2_labels <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a>gmm_3 <span class="op">=</span> GaussianMixture(<span class="dv">3</span>, random_state<span class="op">=</span>RANDOM_SEED)</span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a>img4_n3_labels <span class="op">=</span> gmm_3.fit_predict(img4_nz.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a>img4_n3_labels <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb15-8"><a href="#cb15-8" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" tabindex="-1"></a>gmm_4 <span class="op">=</span> GaussianMixture(<span class="dv">4</span>, random_state<span class="op">=</span>RANDOM_SEED)</span>
<span id="cb15-10"><a href="#cb15-10" tabindex="-1"></a>img4_n4_labels <span class="op">=</span> gmm_4.fit_predict(img4_nz.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb15-11"><a href="#cb15-11" tabindex="-1"></a>img4_n4_labels <span class="op">+=</span> <span class="dv">1</span></span></code></pre>
</div>
<p>Once we have all our image labels, we map the labels back to the
two-dimensional image array and plot the result.</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a>img1_n2_labels_mapped <span class="op">=</span> zeros(img1.shape)</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a>img1_n2_labels_mapped[mask] <span class="op">=</span> img1_n2_labels</span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a>img1_n3_labels_mapped <span class="op">=</span> zeros(img1.shape)</span>
<span id="cb16-5"><a href="#cb16-5" tabindex="-1"></a>img1_n3_labels_mapped[mask] <span class="op">=</span> img1_n3_labels</span>
<span id="cb16-6"><a href="#cb16-6" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" tabindex="-1"></a>img1_n4_labels_mapped <span class="op">=</span> zeros(img1.shape)</span>
<span id="cb16-8"><a href="#cb16-8" tabindex="-1"></a>img1_n4_labels_mapped[mask] <span class="op">=</span> img1_n4_labels</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a>img2_n2_labels_mapped <span class="op">=</span> zeros(img2.shape)</span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a>img2_n2_labels_mapped[mask] <span class="op">=</span> img2_n2_labels</span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a>img2_n3_labels_mapped <span class="op">=</span> zeros(img2.shape)</span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a>img2_n3_labels_mapped[mask] <span class="op">=</span> img2_n3_labels</span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" tabindex="-1"></a>img2_n4_labels_mapped <span class="op">=</span> zeros(img2.shape)</span>
<span id="cb17-8"><a href="#cb17-8" tabindex="-1"></a>img2_n4_labels_mapped[mask] <span class="op">=</span> img2_n4_labels</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a>img3_n2_labels_mapped <span class="op">=</span> zeros(img3.shape)</span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a>img3_n2_labels_mapped[mask] <span class="op">=</span> img3_n2_labels</span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a></span>
<span id="cb18-4"><a href="#cb18-4" tabindex="-1"></a>img3_n3_labels_mapped <span class="op">=</span> zeros(img3.shape)</span>
<span id="cb18-5"><a href="#cb18-5" tabindex="-1"></a>img3_n3_labels_mapped[mask] <span class="op">=</span> img3_n3_labels</span>
<span id="cb18-6"><a href="#cb18-6" tabindex="-1"></a></span>
<span id="cb18-7"><a href="#cb18-7" tabindex="-1"></a>img3_n4_labels_mapped <span class="op">=</span> zeros(img3.shape)</span>
<span id="cb18-8"><a href="#cb18-8" tabindex="-1"></a>img3_n4_labels_mapped[mask] <span class="op">=</span> img3_n4_labels</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a>img4_n2_labels_mapped <span class="op">=</span> zeros(img4.shape)</span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a>img4_n2_labels_mapped[mask] <span class="op">=</span> img4_n2_labels</span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" tabindex="-1"></a>img4_n3_labels_mapped <span class="op">=</span> zeros(img4.shape)</span>
<span id="cb19-5"><a href="#cb19-5" tabindex="-1"></a>img4_n3_labels_mapped[mask] <span class="op">=</span> img4_n3_labels</span>
<span id="cb19-6"><a href="#cb19-6" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" tabindex="-1"></a>img4_n4_labels_mapped <span class="op">=</span> zeros(img4.shape)</span>
<span id="cb19-8"><a href="#cb19-8" tabindex="-1"></a>img4_n4_labels_mapped[mask] <span class="op">=</span> img4_n4_labels</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb20">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a>fig, ax <span class="op">=</span> subplots(<span class="dv">3</span>, <span class="dv">4</span>, figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">15</span>))</span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">0</span>].imshow(img1_n2_labels_mapped[:, :, img_slice], cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb20-4"><a href="#cb20-4" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">0</span>].imshow(img1_n3_labels_mapped[:, :, img_slice], cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb20-5"><a href="#cb20-5" tabindex="-1"></a>ax[<span class="dv">2</span>, <span class="dv">0</span>].imshow(img1_n4_labels_mapped[:, :, img_slice], cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb20-6"><a href="#cb20-6" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">1</span>].imshow(img2_n2_labels_mapped[:, :, img_slice], cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb20-7"><a href="#cb20-7" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">1</span>].imshow(img2_n3_labels_mapped[:, :, img_slice], cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb20-8"><a href="#cb20-8" tabindex="-1"></a>ax[<span class="dv">2</span>, <span class="dv">1</span>].imshow(img2_n4_labels_mapped[:, :, img_slice], cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb20-9"><a href="#cb20-9" tabindex="-1"></a></span>
<span id="cb20-10"><a href="#cb20-10" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">2</span>].imshow(img3_n2_labels_mapped[:, :, img_slice], cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb20-11"><a href="#cb20-11" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">2</span>].imshow(img3_n3_labels_mapped[:, :, img_slice], cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb20-12"><a href="#cb20-12" tabindex="-1"></a>ax[<span class="dv">2</span>, <span class="dv">2</span>].imshow(img3_n4_labels_mapped[:, :, img_slice], cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb20-13"><a href="#cb20-13" tabindex="-1"></a></span>
<span id="cb20-14"><a href="#cb20-14" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">3</span>].imshow(img4_n2_labels_mapped[:, :, img_slice], cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb20-15"><a href="#cb20-15" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">3</span>].imshow(img4_n3_labels_mapped[:, :, img_slice], cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb20-16"><a href="#cb20-16" tabindex="-1"></a>ax[<span class="dv">2</span>, <span class="dv">3</span>].imshow(img4_n4_labels_mapped[:, :, img_slice], cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb20-17"><a href="#cb20-17" tabindex="-1"></a></span>
<span id="cb20-18"><a href="#cb20-18" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">0</span>].set_ylabel(<span class="st">"2 clusters"</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb20-19"><a href="#cb20-19" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">0</span>].set_ylabel(<span class="st">"3 clusters"</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb20-20"><a href="#cb20-20" tabindex="-1"></a>ax[<span class="dv">2</span>, <span class="dv">0</span>].set_ylabel(<span class="st">"4 clusters"</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb20-21"><a href="#cb20-21" tabindex="-1"></a></span>
<span id="cb20-22"><a href="#cb20-22" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">0</span>].set_title(<span class="st">"Image 1"</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb20-23"><a href="#cb20-23" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">1</span>].set_title(<span class="st">"Image 2"</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb20-24"><a href="#cb20-24" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">2</span>].set_title(<span class="st">"Image 3"</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb20-25"><a href="#cb20-25" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">3</span>].set_title(<span class="st">"Image 4"</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb20-26"><a href="#cb20-26" tabindex="-1"></a></span>
<span id="cb20-27"><a href="#cb20-27" tabindex="-1"></a>tight_layout()</span>
<span id="cb20-28"><a href="#cb20-28" tabindex="-1"></a></span>
<span id="cb20-29"><a href="#cb20-29" tabindex="-1"></a>show()</span></code></pre>
</div>
<figure><img src="fig/02-clustering-image-rendered-unnamed-chunk-19-9.png" width="1920" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p style="text-align: justify;">
This figure shows the labels acquired from each of the images, using
different numbers of clusters. We see that using Image 3 (acquired with
FLAIR protocol), the lesion is segmented quite well from the rest of the
brain. The other images are less effective at clearly identifying the
lesion. However, some of these images, e.g. Image 4 (apparent diffusion
coefficient) performs better at segmenting brain tissue from surrounding
cerebrospinal fluid (CSF). CSF is not part of brain tissue and can
contaminate our results. Ideally, we want to segment three key areas:
brain, lesion and CSF.
</p>
</div>
<div class="section level3">
<h3 id="combining-contrast-from-different-images">
<strong>Combining Contrast from Different Images</strong><a class="anchor" aria-label="anchor" href="#combining-contrast-from-different-images"></a>
</h3>
<p style="text-align: justify;">
So far, we only used the intensities of each image individually,
i.e. using only one feature. We now try to combine the images into a
single Numpy array containing four columns, one for each image.
</p>
<div class="codewrapper sourceCode" id="cb21">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a>all_img <span class="op">=</span> stack([img1_nz, img2_nz, img3_nz, img4_nz], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a>all_img.shape</span></code></pre>
</div>
<p>(240391, 4)</p>
<div class="codewrapper sourceCode" id="cb22">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a>gmm_3 <span class="op">=</span> GaussianMixture(<span class="dv">3</span>, random_state<span class="op">=</span>RANDOM_SEED)</span>
<span id="cb22-2"><a href="#cb22-2" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" tabindex="-1"></a>all_img_n3_labels <span class="op">=</span> gmm_3.fit_predict(all_img)</span>
<span id="cb22-4"><a href="#cb22-4" tabindex="-1"></a>all_img_n3_labels <span class="op">+=</span> <span class="dv">1</span></span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb23">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a>all_img_n3_labels_mapped <span class="op">=</span> zeros(img1.shape)</span>
<span id="cb23-2"><a href="#cb23-2" tabindex="-1"></a>all_img_n3_labels_mapped[mask] <span class="op">=</span> all_img_n3_labels</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb24">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a>fig, ax <span class="op">=</span> subplots(<span class="dv">1</span>, <span class="dv">5</span>, figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">5</span>))</span>
<span id="cb24-2"><a href="#cb24-2" tabindex="-1"></a>ax[<span class="dv">0</span>].imshow(img1_n3_labels_mapped[:, :, img_slice], cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb24-3"><a href="#cb24-3" tabindex="-1"></a>ax[<span class="dv">1</span>].imshow(img2_n3_labels_mapped[:, :, img_slice], cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb24-4"><a href="#cb24-4" tabindex="-1"></a>ax[<span class="dv">2</span>].imshow(img3_n3_labels_mapped[:, :, img_slice], cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb24-5"><a href="#cb24-5" tabindex="-1"></a>ax[<span class="dv">3</span>].imshow(img4_n3_labels_mapped[:, :, img_slice], cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb24-6"><a href="#cb24-6" tabindex="-1"></a>ax[<span class="dv">4</span>].imshow(all_img_n3_labels_mapped[:, :, img_slice], cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb24-7"><a href="#cb24-7" tabindex="-1"></a></span>
<span id="cb24-8"><a href="#cb24-8" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="st">"3 clusters"</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb24-9"><a href="#cb24-9" tabindex="-1"></a></span>
<span id="cb24-10"><a href="#cb24-10" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">"Image 1"</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb24-11"><a href="#cb24-11" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">"Image 2"</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb24-12"><a href="#cb24-12" tabindex="-1"></a>ax[<span class="dv">2</span>].set_title(<span class="st">"Image 3"</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb24-13"><a href="#cb24-13" tabindex="-1"></a>ax[<span class="dv">3</span>].set_title(<span class="st">"Image 4"</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb24-14"><a href="#cb24-14" tabindex="-1"></a>ax[<span class="dv">4</span>].set_title(<span class="st">"All images"</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb24-15"><a href="#cb24-15" tabindex="-1"></a></span>
<span id="cb24-16"><a href="#cb24-16" tabindex="-1"></a>tight_layout()</span>
<span id="cb24-17"><a href="#cb24-17" tabindex="-1"></a></span>
<span id="cb24-18"><a href="#cb24-18" tabindex="-1"></a>show()</span></code></pre>
</div>
<figure><img src="fig/02-clustering-image-rendered-unnamed-chunk-23-11.png" width="1920" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p style="text-align: justify;">
Here, the last column shows the cluster results when all four images are
used in the Gaussian mixture model. These results seem to be better than
the individual images, and with three clusters, the lesion, CSF and
brain tissue seem clearly identified.
</p>
<p>Let’s plot some of the other image slices to check that the
segmentation performs well on the whole 3D image.</p>
<div class="codewrapper sourceCode" id="cb25">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" tabindex="-1"></a>fig, ax <span class="op">=</span> subplots(<span class="dv">5</span>, <span class="dv">5</span>, figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">20</span>))</span>
<span id="cb25-2"><a href="#cb25-2" tabindex="-1"></a></span>
<span id="cb25-3"><a href="#cb25-3" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">0</span>].imshow(img1[:, :, <span class="dv">16</span>], cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb25-4"><a href="#cb25-4" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">1</span>].imshow(img1[:, :, <span class="dv">18</span>], cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb25-5"><a href="#cb25-5" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">2</span>].imshow(img1[:, :, <span class="dv">20</span>], cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb25-6"><a href="#cb25-6" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">3</span>].imshow(img1[:, :, <span class="dv">22</span>], cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb25-7"><a href="#cb25-7" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">4</span>].imshow(img1[:, :, <span class="dv">24</span>], cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb25-8"><a href="#cb25-8" tabindex="-1"></a></span>
<span id="cb25-9"><a href="#cb25-9" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">0</span>].imshow(img2[:, :, <span class="dv">16</span>], cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb25-10"><a href="#cb25-10" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">1</span>].imshow(img2[:, :, <span class="dv">18</span>], cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb25-11"><a href="#cb25-11" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">2</span>].imshow(img2[:, :, <span class="dv">20</span>], cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb25-12"><a href="#cb25-12" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">3</span>].imshow(img2[:, :, <span class="dv">22</span>], cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb25-13"><a href="#cb25-13" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">4</span>].imshow(img2[:, :, <span class="dv">24</span>], cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb25-14"><a href="#cb25-14" tabindex="-1"></a></span>
<span id="cb25-15"><a href="#cb25-15" tabindex="-1"></a>ax[<span class="dv">2</span>, <span class="dv">0</span>].imshow(img3[:, :, <span class="dv">16</span>], cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb25-16"><a href="#cb25-16" tabindex="-1"></a>ax[<span class="dv">2</span>, <span class="dv">1</span>].imshow(img3[:, :, <span class="dv">18</span>], cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb25-17"><a href="#cb25-17" tabindex="-1"></a>ax[<span class="dv">2</span>, <span class="dv">2</span>].imshow(img3[:, :, <span class="dv">20</span>], cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb25-18"><a href="#cb25-18" tabindex="-1"></a>ax[<span class="dv">2</span>, <span class="dv">3</span>].imshow(img3[:, :, <span class="dv">22</span>], cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb25-19"><a href="#cb25-19" tabindex="-1"></a>ax[<span class="dv">2</span>, <span class="dv">4</span>].imshow(img3[:, :, <span class="dv">24</span>], cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb25-20"><a href="#cb25-20" tabindex="-1"></a></span>
<span id="cb25-21"><a href="#cb25-21" tabindex="-1"></a>ax[<span class="dv">3</span>, <span class="dv">0</span>].imshow(img4[:, :, <span class="dv">16</span>], cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb25-22"><a href="#cb25-22" tabindex="-1"></a>ax[<span class="dv">3</span>, <span class="dv">1</span>].imshow(img4[:, :, <span class="dv">18</span>], cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb25-23"><a href="#cb25-23" tabindex="-1"></a>ax[<span class="dv">3</span>, <span class="dv">2</span>].imshow(img4[:, :, <span class="dv">20</span>], cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb25-24"><a href="#cb25-24" tabindex="-1"></a>ax[<span class="dv">3</span>, <span class="dv">3</span>].imshow(img4[:, :, <span class="dv">22</span>], cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb25-25"><a href="#cb25-25" tabindex="-1"></a>ax[<span class="dv">3</span>, <span class="dv">4</span>].imshow(img4[:, :, <span class="dv">24</span>], cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb25-26"><a href="#cb25-26" tabindex="-1"></a></span>
<span id="cb25-27"><a href="#cb25-27" tabindex="-1"></a>ax[<span class="dv">4</span>, <span class="dv">0</span>].imshow(all_img_n3_labels_mapped[:, :, <span class="dv">16</span>], cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb25-28"><a href="#cb25-28" tabindex="-1"></a>ax[<span class="dv">4</span>, <span class="dv">1</span>].imshow(all_img_n3_labels_mapped[:, :, <span class="dv">18</span>], cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb25-29"><a href="#cb25-29" tabindex="-1"></a>ax[<span class="dv">4</span>, <span class="dv">2</span>].imshow(all_img_n3_labels_mapped[:, :, <span class="dv">20</span>], cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb25-30"><a href="#cb25-30" tabindex="-1"></a>ax[<span class="dv">4</span>, <span class="dv">3</span>].imshow(all_img_n3_labels_mapped[:, :, <span class="dv">22</span>], cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb25-31"><a href="#cb25-31" tabindex="-1"></a>ax[<span class="dv">4</span>, <span class="dv">4</span>].imshow(all_img_n3_labels_mapped[:, :, <span class="dv">24</span>], cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb25-32"><a href="#cb25-32" tabindex="-1"></a></span>
<span id="cb25-33"><a href="#cb25-33" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">0</span>].set_title(<span class="st">"Slice 16"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb25-34"><a href="#cb25-34" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">1</span>].set_title(<span class="st">"Slice 18"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb25-35"><a href="#cb25-35" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">2</span>].set_title(<span class="st">"Slice 20"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb25-36"><a href="#cb25-36" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">3</span>].set_title(<span class="st">"Slice 22"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb25-37"><a href="#cb25-37" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">4</span>].set_title(<span class="st">"Slice 24"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb25-38"><a href="#cb25-38" tabindex="-1"></a></span>
<span id="cb25-39"><a href="#cb25-39" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">0</span>].set_ylabel(<span class="st">"Image 1"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb25-40"><a href="#cb25-40" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">0</span>].set_ylabel(<span class="st">"Image 2"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb25-41"><a href="#cb25-41" tabindex="-1"></a>ax[<span class="dv">2</span>, <span class="dv">0</span>].set_ylabel(<span class="st">"Image 3"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb25-42"><a href="#cb25-42" tabindex="-1"></a>ax[<span class="dv">3</span>, <span class="dv">0</span>].set_ylabel(<span class="st">"Image 4"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb25-43"><a href="#cb25-43" tabindex="-1"></a>ax[<span class="dv">4</span>, <span class="dv">0</span>].set_ylabel(<span class="st">"Clustered labels"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)<span class="op">;</span></span>
<span id="cb25-44"><a href="#cb25-44" tabindex="-1"></a></span>
<span id="cb25-45"><a href="#cb25-45" tabindex="-1"></a>show()</span></code></pre>
</div>
<figure><img src="fig/02-clustering-image-rendered-unnamed-chunk-24-13.png" width="1920" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>Overall, the lesion, shown in yellow, seems to be segmented well
across the volume.</p>
</div>
<div class="section level3">
<h3 id="checking-the-gmm-labels">
<strong>Checking the GMM Labels</strong><a class="anchor" aria-label="anchor" href="#checking-the-gmm-labels"></a>
</h3>
<p style="text-align: justify;">
To investigate how the image intensities were clustered, we can look at
the scatter plots for each combination of images. The diagonal plots
show histograms of each image. This type of plot can be very useful in
exploratory data analysis.
</p>
<div id="note-2" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="note-2" class="callout-inner">
<h3 class="callout-title">Note</h3>
<div class="callout-content">
<p>Note that this plot might take a bit longer to run, as there are a
very large number of data points.</p>
</div>
</div>
</div>
<p>In a python plotting library called <a href="https://seaborn.pydata.org" class="external-link">seaborn</a>, such plots are called
<code>pairplots</code> and can be very easily plotted if your data is in
a pandas dataframe.</p>
<p>You can install it at the command prompt (Windows) or terminal
(MacOS, Linux) using:</p>
<pre><code>conda install seaborn</code></pre>
<p>To use it, import the required functions in your Python kernel,
e.g.:</p>
<div class="codewrapper sourceCode" id="cb27">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" tabindex="-1"></a><span class="im">from</span> seaborn <span class="im">import</span> pairplot</span></code></pre>
</div>
<p>ModuleNotFoundError: No module named ‘seaborn’</p>
<p>We don’t use this library here, but encourage you to look up further
information in <a href="https://seaborn.pydata.org/generated/seaborn.pairplot.html" class="external-link">the
seaborn documentation</a>.</p>
<div class="codewrapper sourceCode" id="cb28">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" tabindex="-1"></a>fig, ax <span class="op">=</span> subplots(<span class="dv">4</span>, <span class="dv">4</span>, figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">20</span>))</span>
<span id="cb28-2"><a href="#cb28-2" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">0</span>].hist(img1_nz, bins<span class="op">=</span><span class="dv">50</span>)<span class="op">;</span></span>
<span id="cb28-4"><a href="#cb28-4" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">0</span>].set_title(<span class="st">'Image 1'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb28-5"><a href="#cb28-5" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">1</span>].scatter(img1_nz, img2_nz, c<span class="op">=</span>all_img_n3_labels, cmap<span class="op">=</span><span class="st">'viridis'</span>, vmin<span class="op">=</span><span class="dv">0</span>)<span class="op">;</span></span>
<span id="cb28-6"><a href="#cb28-6" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">1</span>].set_xlabel(<span class="st">'Image 1'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb28-7"><a href="#cb28-7" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">1</span>].set_ylabel(<span class="st">'Image 2'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb28-8"><a href="#cb28-8" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">2</span>].scatter(img1_nz, img3_nz, c<span class="op">=</span>all_img_n3_labels, cmap<span class="op">=</span><span class="st">'viridis'</span>, vmin<span class="op">=</span><span class="dv">0</span>)<span class="op">;</span></span>
<span id="cb28-9"><a href="#cb28-9" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">2</span>].set_xlabel(<span class="st">'Image 1'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb28-10"><a href="#cb28-10" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">2</span>].set_ylabel(<span class="st">'Image 3'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb28-11"><a href="#cb28-11" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">3</span>].scatter(img1_nz, img4_nz, c<span class="op">=</span>all_img_n3_labels, cmap<span class="op">=</span><span class="st">'viridis'</span>, vmin<span class="op">=</span><span class="dv">0</span>)<span class="op">;</span></span>
<span id="cb28-12"><a href="#cb28-12" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">3</span>].set_xlabel(<span class="st">'Image 1'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb28-13"><a href="#cb28-13" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">3</span>].set_ylabel(<span class="st">'Image 4'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb28-14"><a href="#cb28-14" tabindex="-1"></a></span>
<span id="cb28-15"><a href="#cb28-15" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">0</span>].scatter(img2_nz, img1_nz, c<span class="op">=</span>all_img_n3_labels, cmap<span class="op">=</span><span class="st">'viridis'</span>, vmin<span class="op">=</span><span class="dv">0</span>)<span class="op">;</span></span>
<span id="cb28-16"><a href="#cb28-16" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">0</span>].set_xlabel(<span class="st">'Image 2'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb28-17"><a href="#cb28-17" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">0</span>].set_ylabel(<span class="st">'Image 1'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb28-18"><a href="#cb28-18" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">1</span>].hist(img2_nz, bins<span class="op">=</span><span class="dv">50</span>)<span class="op">;</span></span>
<span id="cb28-19"><a href="#cb28-19" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">1</span>].set_title(<span class="st">'Image 2'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb28-20"><a href="#cb28-20" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">2</span>].scatter(img2_nz, img3_nz, c<span class="op">=</span>all_img_n3_labels, cmap<span class="op">=</span><span class="st">'viridis'</span>, vmin<span class="op">=</span><span class="dv">0</span>)<span class="op">;</span></span>
<span id="cb28-21"><a href="#cb28-21" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">2</span>].set_xlabel(<span class="st">'Image 2'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb28-22"><a href="#cb28-22" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">2</span>].set_ylabel(<span class="st">'Image 3'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb28-23"><a href="#cb28-23" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">3</span>].scatter(img2_nz, img4_nz, c<span class="op">=</span>all_img_n3_labels, cmap<span class="op">=</span><span class="st">'viridis'</span>, vmin<span class="op">=</span><span class="dv">0</span>)<span class="op">;</span></span>
<span id="cb28-24"><a href="#cb28-24" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">3</span>].set_xlabel(<span class="st">'Image 2'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb28-25"><a href="#cb28-25" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">3</span>].set_ylabel(<span class="st">'Image 4'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb28-26"><a href="#cb28-26" tabindex="-1"></a></span>
<span id="cb28-27"><a href="#cb28-27" tabindex="-1"></a>ax[<span class="dv">2</span>, <span class="dv">0</span>].scatter(img3_nz, img1_nz, c<span class="op">=</span>all_img_n3_labels, cmap<span class="op">=</span><span class="st">'viridis'</span>, vmin<span class="op">=</span><span class="dv">0</span>)<span class="op">;</span></span>
<span id="cb28-28"><a href="#cb28-28" tabindex="-1"></a>ax[<span class="dv">2</span>, <span class="dv">0</span>].set_xlabel(<span class="st">'Image 3'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb28-29"><a href="#cb28-29" tabindex="-1"></a>ax[<span class="dv">2</span>, <span class="dv">0</span>].set_ylabel(<span class="st">'Image 1'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb28-30"><a href="#cb28-30" tabindex="-1"></a>ax[<span class="dv">2</span>, <span class="dv">1</span>].scatter(img3_nz, img2_nz, c<span class="op">=</span>all_img_n3_labels, cmap<span class="op">=</span><span class="st">'viridis'</span>, vmin<span class="op">=</span><span class="dv">0</span>)<span class="op">;</span></span>
<span id="cb28-31"><a href="#cb28-31" tabindex="-1"></a>ax[<span class="dv">2</span>, <span class="dv">1</span>].set_xlabel(<span class="st">'Image 3'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb28-32"><a href="#cb28-32" tabindex="-1"></a>ax[<span class="dv">2</span>, <span class="dv">1</span>].set_ylabel(<span class="st">'Image 2'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb28-33"><a href="#cb28-33" tabindex="-1"></a>ax[<span class="dv">2</span>, <span class="dv">2</span>].hist(img3_nz, bins<span class="op">=</span><span class="dv">50</span>)<span class="op">;</span></span>
<span id="cb28-34"><a href="#cb28-34" tabindex="-1"></a>ax[<span class="dv">2</span>, <span class="dv">2</span>].set_title(<span class="st">'Image 3'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb28-35"><a href="#cb28-35" tabindex="-1"></a>ax[<span class="dv">2</span>, <span class="dv">3</span>].scatter(img3_nz, img4_nz, c<span class="op">=</span>all_img_n3_labels, cmap<span class="op">=</span><span class="st">'viridis'</span>, vmin<span class="op">=</span><span class="dv">0</span>)<span class="op">;</span></span>
<span id="cb28-36"><a href="#cb28-36" tabindex="-1"></a>ax[<span class="dv">2</span>, <span class="dv">3</span>].set_xlabel(<span class="st">'Image 3'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb28-37"><a href="#cb28-37" tabindex="-1"></a>ax[<span class="dv">2</span>, <span class="dv">3</span>].set_ylabel(<span class="st">'Image 4'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb28-38"><a href="#cb28-38" tabindex="-1"></a></span>
<span id="cb28-39"><a href="#cb28-39" tabindex="-1"></a>ax[<span class="dv">3</span>, <span class="dv">0</span>].scatter(img4_nz, img1_nz, c<span class="op">=</span>all_img_n3_labels, cmap<span class="op">=</span><span class="st">'viridis'</span>, vmin<span class="op">=</span><span class="dv">0</span>)<span class="op">;</span></span>
<span id="cb28-40"><a href="#cb28-40" tabindex="-1"></a>ax[<span class="dv">3</span>, <span class="dv">0</span>].set_xlabel(<span class="st">'Image 4'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb28-41"><a href="#cb28-41" tabindex="-1"></a>ax[<span class="dv">3</span>, <span class="dv">0</span>].set_ylabel(<span class="st">'Image 1'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb28-42"><a href="#cb28-42" tabindex="-1"></a>ax[<span class="dv">3</span>, <span class="dv">1</span>].scatter(img4_nz, img2_nz, c<span class="op">=</span>all_img_n3_labels, cmap<span class="op">=</span><span class="st">'viridis'</span>, vmin<span class="op">=</span><span class="dv">0</span>)<span class="op">;</span></span>
<span id="cb28-43"><a href="#cb28-43" tabindex="-1"></a>ax[<span class="dv">3</span>, <span class="dv">1</span>].set_xlabel(<span class="st">'Image 4'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb28-44"><a href="#cb28-44" tabindex="-1"></a>ax[<span class="dv">3</span>, <span class="dv">1</span>].set_ylabel(<span class="st">'Image 2'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb28-45"><a href="#cb28-45" tabindex="-1"></a>ax[<span class="dv">3</span>, <span class="dv">2</span>].scatter(img4_nz, img3_nz, c<span class="op">=</span>all_img_n3_labels, cmap<span class="op">=</span><span class="st">'viridis'</span>, vmin<span class="op">=</span><span class="dv">0</span>)<span class="op">;</span></span>
<span id="cb28-46"><a href="#cb28-46" tabindex="-1"></a>ax[<span class="dv">3</span>, <span class="dv">2</span>].set_xlabel(<span class="st">'Image 4'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb28-47"><a href="#cb28-47" tabindex="-1"></a>ax[<span class="dv">3</span>, <span class="dv">2</span>].set_ylabel(<span class="st">'Image 3'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb28-48"><a href="#cb28-48" tabindex="-1"></a>ax[<span class="dv">3</span>, <span class="dv">3</span>].hist(img4_nz, bins<span class="op">=</span><span class="dv">50</span>)<span class="op">;</span></span>
<span id="cb28-49"><a href="#cb28-49" tabindex="-1"></a>ax[<span class="dv">3</span>, <span class="dv">3</span>].set_title(<span class="st">'Image 4'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb28-50"><a href="#cb28-50" tabindex="-1"></a></span>
<span id="cb28-51"><a href="#cb28-51" tabindex="-1"></a>fig.tight_layout()</span>
<span id="cb28-52"><a href="#cb28-52" tabindex="-1"></a>show()</span></code></pre>
</div>
<figure><img src="fig/02-clustering-image-rendered-unnamed-chunk-26-15.png" width="1920" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p style="text-align: justify;">
The colours in the scatter plots above correspond to the labels we
extracted using all four images and three clusters. I.e. green
corresponds to healthy brain tissue, yellow corresponds to CSF and blue
corresponds to the lesion. This figure shows reasonably well how CSF
(yellow) and lesion (blue) can be clustered. However, it is not as easy
to see how the healthy tissue was separated from CSF and lesion tissue.
To investigate further, we can plot the above slightly differently,
using a 2-dimensional histogram instead of a scatter plot.
</p>
<div id="section" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="section" class="callout-inner">
<h3 class="callout-title"></h3>
<div class="callout-content">
<p>A 2-dimensional histogram plots the counts of values in bins for two
variables. The results are displayed as a heatmap. An intuitive example
with code using the Matplotlib function <code>hist2d</code> <a href="https://matplotlib.org/3.1.1/gallery/scales/power_norm.html#sphx-glr-gallery-scales-power-norm-py" class="external-link">is
available here</a>.|</p>
</div>
</div>
</div>
<div class="codewrapper sourceCode" id="cb29">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" tabindex="-1"></a><span class="im">import</span> matplotlib.colors <span class="im">as</span> mcolors</span>
<span id="cb29-2"><a href="#cb29-2" tabindex="-1"></a></span>
<span id="cb29-3"><a href="#cb29-3" tabindex="-1"></a>fig, ax <span class="op">=</span> subplots(<span class="dv">4</span>, <span class="dv">4</span>, figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">20</span>))</span>
<span id="cb29-4"><a href="#cb29-4" tabindex="-1"></a></span>
<span id="cb29-5"><a href="#cb29-5" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">0</span>].hist(img1_nz, bins<span class="op">=</span><span class="dv">50</span>)<span class="op">;</span></span>
<span id="cb29-6"><a href="#cb29-6" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">1</span>].hist2d(img1_nz, img2_nz, bins<span class="op">=</span><span class="dv">100</span>, norm<span class="op">=</span>mcolors.PowerNorm(<span class="fl">0.2</span>))<span class="op">;</span></span>
<span id="cb29-7"><a href="#cb29-7" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">2</span>].hist2d(img1_nz, img3_nz, bins<span class="op">=</span><span class="dv">100</span>, norm<span class="op">=</span>mcolors.PowerNorm(<span class="fl">0.2</span>))<span class="op">;</span></span>
<span id="cb29-8"><a href="#cb29-8" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">3</span>].hist2d(img1_nz, img4_nz, bins<span class="op">=</span><span class="dv">100</span>, norm<span class="op">=</span>mcolors.PowerNorm(<span class="fl">0.2</span>))<span class="op">;</span></span>
<span id="cb29-9"><a href="#cb29-9" tabindex="-1"></a></span>
<span id="cb29-10"><a href="#cb29-10" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">0</span>].hist2d(img2_nz, img1_nz, bins<span class="op">=</span><span class="dv">100</span>, norm<span class="op">=</span>mcolors.PowerNorm(<span class="fl">0.2</span>))<span class="op">;</span></span>
<span id="cb29-11"><a href="#cb29-11" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">1</span>].hist(img2_nz, bins<span class="op">=</span><span class="dv">50</span>)<span class="op">;</span></span>
<span id="cb29-12"><a href="#cb29-12" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">2</span>].hist2d(img2_nz, img3_nz, bins<span class="op">=</span><span class="dv">100</span>, norm<span class="op">=</span>mcolors.PowerNorm(<span class="fl">0.2</span>))<span class="op">;</span></span>
<span id="cb29-13"><a href="#cb29-13" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">3</span>].hist2d(img2_nz, img4_nz, bins<span class="op">=</span><span class="dv">100</span>, norm<span class="op">=</span>mcolors.PowerNorm(<span class="fl">0.2</span>))<span class="op">;</span></span>
<span id="cb29-14"><a href="#cb29-14" tabindex="-1"></a></span>
<span id="cb29-15"><a href="#cb29-15" tabindex="-1"></a>ax[<span class="dv">2</span>, <span class="dv">0</span>].hist2d(img3_nz, img1_nz, bins<span class="op">=</span><span class="dv">100</span>, norm<span class="op">=</span>mcolors.PowerNorm(<span class="fl">0.2</span>))<span class="op">;</span></span>
<span id="cb29-16"><a href="#cb29-16" tabindex="-1"></a>ax[<span class="dv">2</span>, <span class="dv">1</span>].hist2d(img3_nz, img2_nz, bins<span class="op">=</span><span class="dv">100</span>, norm<span class="op">=</span>mcolors.PowerNorm(<span class="fl">0.2</span>))<span class="op">;</span></span>
<span id="cb29-17"><a href="#cb29-17" tabindex="-1"></a>ax[<span class="dv">2</span>, <span class="dv">2</span>].hist(img3_nz, bins<span class="op">=</span><span class="dv">50</span>)<span class="op">;</span></span>
<span id="cb29-18"><a href="#cb29-18" tabindex="-1"></a>ax[<span class="dv">2</span>, <span class="dv">3</span>].hist2d(img3_nz, img4_nz, bins<span class="op">=</span><span class="dv">100</span>, norm<span class="op">=</span>mcolors.PowerNorm(<span class="fl">0.2</span>))<span class="op">;</span></span>
<span id="cb29-19"><a href="#cb29-19" tabindex="-1"></a></span>
<span id="cb29-20"><a href="#cb29-20" tabindex="-1"></a>ax[<span class="dv">3</span>, <span class="dv">0</span>].hist2d(img4_nz, img1_nz, bins<span class="op">=</span><span class="dv">100</span>, norm<span class="op">=</span>mcolors.PowerNorm(<span class="fl">0.2</span>))<span class="op">;</span></span>
<span id="cb29-21"><a href="#cb29-21" tabindex="-1"></a>ax[<span class="dv">3</span>, <span class="dv">1</span>].hist2d(img4_nz, img2_nz, bins<span class="op">=</span><span class="dv">100</span>, norm<span class="op">=</span>mcolors.PowerNorm(<span class="fl">0.2</span>))<span class="op">;</span></span>
<span id="cb29-22"><a href="#cb29-22" tabindex="-1"></a>ax[<span class="dv">3</span>, <span class="dv">2</span>].hist2d(img4_nz, img3_nz, bins<span class="op">=</span><span class="dv">100</span>, norm<span class="op">=</span>mcolors.PowerNorm(<span class="fl">0.2</span>))<span class="op">;</span></span>
<span id="cb29-23"><a href="#cb29-23" tabindex="-1"></a>ax[<span class="dv">3</span>, <span class="dv">3</span>].hist(img4_nz, bins<span class="op">=</span><span class="dv">50</span>)<span class="op">;</span></span>
<span id="cb29-24"><a href="#cb29-24" tabindex="-1"></a></span>
<span id="cb29-25"><a href="#cb29-25" tabindex="-1"></a>show()</span></code></pre>
</div>
<figure><img src="fig/02-clustering-image-rendered-unnamed-chunk-27-17.png" width="1920" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p style="text-align: justify;">
Note in the plot, we used a PowerNorm normalisation on the image
intensities. This is just to aid with visualisation, and you are welcome
to change or completely remove the normalisation.
</p>
<p style="text-align: justify;">
The plots show that there is a bright, high-density region corresponding
to the clustered healthy tissue region. This gives us a better idea how
the GMM algorithm found the three regions. Healthy tissue has low signal
variance in all 4 images. Signal intensity in CSF and the lesion have
much higher variance making it possible to distinguish them from healthy
tissue. Furthermore, the relative intensities of CSF and lesion tissue
are different as shown in the scatter plots, making it possible for the
GMM to distinguish between the two.
</p>
</div>
</section><section><h2 class="section-heading" id="exercises">Exercises<a class="anchor" aria-label="anchor" href="#exercises"></a>
</h2>
<hr class="half-width">
<div id="end-of-chapter-exercises" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="end-of-chapter-exercises" class="callout-inner">
<h3 class="callout-title">End of chapter Exercises</h3>
<div class="callout-content">
<p style="text-align: justify;">
In this assignment, we ask you to use the same set of images as in the
work through example. However, instead of GMM, we want you to try a
different clustering method called <strong>KMeans</strong>. The
documentation for KMneans is available here: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html" class="external-link uri">https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html</a>.
Some examples of how kmeans clustering can go wrong are shown in <a href="https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html#sphx-glr-auto-examples-cluster-plot-kmeans-assumptions-py" class="external-link">this
example code</a>.
</p>
<p>Using <code>KMeans</code> from ‘sklearn.cluster’, do the following
tasks:</p>
<ol style="list-style-type: decimal">
<li><p>Investigate different numbers of clusters, similarly to what we
did in th work through example.</p></li>
<li><p>Use different combinations of the 4 images to see how the
clustering performs in different cases.</p></li>
<li><p>The labelled results using all four images may not look as clean
as the ones in the work-through example. Try scaling the images
e.g. using the sklearn standard scaler, and combining the scaled images.
Do the results change? If yes, explore and comment on why you think
scaling may be advantageous in this clustering example.</p></li>
<li><p>Compare the behaviour of <code>KMeans</code> to the outcome with
<code>GaussianMixture</code>.</p></li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">
  Solutions are provided after assignments are marked.
  </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">

</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="further-reading">Further Reading<a class="anchor" aria-label="anchor" href="#further-reading"></a>
</h2>
<hr class="half-width">
<p>If after this lesson you want to deepen your understanding of
clustering and, in particular, want to compare the performance of
different clustering methods when dealing with images, try the article
<a href="https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1174" class="external-link">Clustering
techniques for neuroimaging applications</a>. It is paywalled and you
will need an institutional access to download.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Image analysis almost always requires a bit of pre-processing.</li>
<li>Image scaling is performed by using <code>fit_transform</code>
method from module <code>StandardScaler</code> in
<code>Scikit-learn</code>.</li>
<li>GMM offers a good startig point in image clustering.</li>
<li>Diagonal plots are very useful in exploratory data analysis.</li>
</ul>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</section></section><section id="aio-03-dimensionality"><p>Content from <a href="03-dimensionality.html">Dimensionality Reduction</a></p>
<hr>
<p>Last updated on 2025-01-27 |

        <a href="https://github.com/carpentries/workbench-template-md/edit/main/episodes/03-dimensionality.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<p><a href="https://drive.usercontent.google.com/u/1/uc?id=1QpuoibEsIfbllix2dElYpfl5TBe700EP&amp;export=download" class="external-link"><strong>Download
Chapter notebook (ipynb)</strong></a></p>
<p><a href="https://drive.usercontent.google.com/u/1/uc?id=1vGeUNnFSlcAOVNkfB5OEFMCWL3BQY6Mq&amp;export=download" class="external-link"><strong>Download
Chapter PDF</strong></a></p>
<p><a href="https://docs.google.com/forms/d/e/1FAIpQLSdr0capF7jloJhPH3Pki1B3LZoKOG16poOpuVJ7SL2LkwLHQA/viewform?pli=1" class="external-link"><span style="color: rgb(255, 0, 0);"><strong>Mandatory Lesson Feedback
Survey</strong></span></a></p>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>Why is it important to perform dimensionality reduction?</li>
<li>How is dimensionality reduction performed?</li>
<li>How PCA is used to determine variance?</li>
<li>When does PCA fail?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Understanding high-dimensional datasets.</li>
<li>Applying dimensionality reduction to extract features.</li>
<li>Learning to use PCA for dimensionality reduction.</li>
<li>Estimating the optimal number of features.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<br><p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/ql80ZoyiWKY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</p>
<br><p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/yLjzGlZ19pM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</p>
<br><p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/E0YytUSJsQY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</p>
<p><br></p>
<div id="prereq1" class="callout prereq">
<div class="callout-square">
<i class="callout-icon" data-feather="check"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Prerequisite</h3>
<div class="callout-content">
<ul>
<li><a href="01-clustering-intro.html">Clustering Introduction</a></li>
<li><a href="02-clustering-image.html">Clustering Images</a></li>
</ul>
</div>
</div>
</div>
<div class="section level3">
<h3 id="code-preparation">
<strong>Code Preparation</strong><a class="anchor" aria-label="anchor" href="#code-preparation"></a>
</h3>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> reshape, append, mean, pi, linspace, var, array, cumsum, arange</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">from</span> numpy.random <span class="im">import</span> seed, multivariate_normal, binomial</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">from</span> numpy.linalg <span class="im">import</span> norm</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="im">from</span> math <span class="im">import</span> cos, sin, atan2</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="im">from</span> matplotlib.pyplot <span class="im">import</span> subplots, xlabel, ylabel, scatter, title</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="im">from</span> matplotlib.pyplot <span class="im">import</span> figure, plot, xticks, xlabel, ylabel, suptitle, show</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="im">import</span> glob</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a><span class="im">from</span> pandas <span class="im">import</span> read_csv, DataFrame</span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a><span class="im">import</span> csv</span></code></pre>
</div>
</div>
<section><h2 class="section-heading" id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<hr class="half-width">
<p style="text-align: justify;">
Clinical and scientific datasets often contain measurements of many
physiological variables, expression of many genes or metabolic markers.
These datasets, where each individual observation/sample is described by
many variables are referred to as <strong>high-dimensional</strong>.
However, some of these features can be highly correlated or redundant,
and we sometimes want a more condensed description of the data. This
process of describing the data by a reduced number of new combined
features that reduces redundancy, is called <strong>dimensionality
reduction</strong>.
</p>
<figure><img src="fig/goal.png" alt="Goal of decomposition" class="figure mx-auto d-block"><div class="figcaption">Goal of decomposition</div>
</figure><p>There are several reasons for performing dimensionality reduction,
both for data exploration and for pre-processing:</p>
<ul>
<li><p>Data compression, to optimize memory usage/loading speed and
visualisation of “important features”.</p></li>
<li><p>To find the individual variable or combinations of variables that
give a condensed but accurate description of the data (sometimes also
called feature extraction).</p></li>
<li><p>It can also be used for “denoising” - removing features that only
account for small variability.</p></li>
<li><p>It is also useful to perform this step before other supervised
(regression, classification) or unsupervised learning procedures (like
clustering), by reducing data to “main features” to improve robustness
of learning, especially with a limited number of independent
observations.</p></li>
</ul></section><section><h2 class="section-heading" id="example-predicting-outcome-from-large-scale-genetic-or-molecular-studies">Example: Predicting outcome from large-scale genetic or molecular
studies<a class="anchor" aria-label="anchor" href="#example-predicting-outcome-from-large-scale-genetic-or-molecular-studies"></a>
</h2>
<hr class="half-width">
<p style="text-align: justify;">
Clinical research can involve doing an exhaustive measurement of all
tentative diagnostic markers, for example levels of various cytokines
and hormones, or searching for genetic markers of a disease. There is
good reason to expect that these share regulatory/signalling mechanisms,
and that their interaction determines disease outcome. Building a
regression model including all measurements is problematic because of
multicollinearity and the demand for a much larger number of
observations.
</p>
<p style="text-align: justify;">
By performing dimensionality reduction, we first find features that
capture the major patterns of covariation of these factors in the sample
population. Then we will use these compact features, rather than
individual measurements, to train our classifier or regression model, to
study outcomes. With fewer features, we train models with less data.
</p>
<div class="section level3">
<h3 id="loading-example-data-childhood-onset-rheumatic-disease-gene-expression-profile">
<strong>Loading example data: Childhood Onset Rheumatic Disease gene
expression profile</strong><a class="anchor" aria-label="anchor" href="#loading-example-data-childhood-onset-rheumatic-disease-gene-expression-profile"></a>
</h3>
<p style="text-align: justify;">
<a href="https://www.ebi.ac.uk/arrayexpress/experiments/E-GEOD-11083/" class="external-link">Gene
expression data</a> (for selected markers) is available for 83
individuals, including some patients with varying forms of early-onset
rheumatic disease (available at EMBL-Bioinformatics database, also
included in folder). This is a typical example of high dimensional
datasets, with transcript levels for &gt;50,000 genes measured!
</p>
<p>Here, the data are read from individual files in the ‘Dataset’ folder
and combined into a Pandas dataframe called ‘geneData’.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="co"># Load all data</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>nGenes <span class="op">=</span> <span class="dv">1000</span> <span class="co"># keep only these genes for now</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>filectr<span class="op">=</span><span class="dv">0</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>datapath <span class="op">=</span> Path(<span class="st">"data"</span>)    <span class="co">#/relative/path/to/folder/with/datsets</span></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a><span class="cf">for</span> txt <span class="kw">in</span> datapath.glob(<span class="st">"*.txt"</span>):</span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a>    b<span class="op">=</span>read_csv(txt, sep<span class="op">=</span><span class="st">'</span><span class="ch">\t</span><span class="st">'</span>, usecols<span class="op">=</span>[<span class="dv">1</span>])</span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>    b<span class="op">=</span>reshape(b.values, (<span class="dv">1</span>,<span class="dv">54675</span>)) <span class="co"># each file is 1 observation sample i.e. 1 row.</span></span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a>    <span class="cf">if</span> filectr<span class="op">==</span><span class="dv">0</span>:</span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a>        coln <span class="op">=</span>  read_csv(txt, sep<span class="op">=</span><span class="st">'</span><span class="ch">\t</span><span class="st">'</span>, usecols<span class="op">=</span>[<span class="dv">0</span>])</span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a>        a <span class="op">=</span> b</span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb2-15"><a href="#cb2-15" tabindex="-1"></a>        a<span class="op">=</span>append( a, b, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb2-16"><a href="#cb2-16" tabindex="-1"></a>    filectr<span class="op">+=</span><span class="dv">1</span></span>
<span id="cb2-17"><a href="#cb2-17" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" tabindex="-1"></a><span class="co"># convert 2D array to dataframe</span></span>
<span id="cb2-19"><a href="#cb2-19" tabindex="-1"></a>geneData <span class="op">=</span> DataFrame( data<span class="op">=</span>a[:,<span class="bu">range</span>(nGenes)]<span class="op">-</span> mean(a[:,<span class="bu">range</span>(nGenes)], axis<span class="op">=</span><span class="dv">0</span>), columns <span class="op">=</span> coln.values[<span class="bu">range</span>(nGenes)])</span></code></pre>
</div>
The dataframe has 80 samples and 1000 features.
<p style="text-align: justify;">
Apart from the large number of variables (compared to number of
samples), we also have strongly correlated expression of various
markers. We can see that by plotting the correlation matrix of the
dataframe.
</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>fig, ax <span class="op">=</span> subplots(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">8</span>))</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>im <span class="op">=</span> ax.matshow(geneData.corr())</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>cbar <span class="op">=</span> fig.colorbar(im, shrink<span class="op">=</span><span class="fl">0.82</span>)</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>xlabel(<span class="st">'Locus #'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>ylabel(<span class="st">'Locus #'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span></span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>show()</span></code></pre>
</div>
<figure><img src="fig/03-dimensionality-rendered-unnamed-chunk-3-1.png" width="768" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p style="text-align: justify;">
The little yellow blocks indicate that various features are strongly
correlation (values around 1). Further analyses (e.g. checking if we
find clusters mapping to healthy individuals and patients, respectively,
or whether patients with different patterns of marker expression can be
identified for future diagnostic purposes) can be thus be improved if we
perform a dimensionality reduction, i.e. decrease the number of features
while maintaining relevant information.
</p>
</div>
</section><section><h2 class="section-heading" id="principal-component-analysis">Principal Component Analysis<a class="anchor" aria-label="anchor" href="#principal-component-analysis"></a>
</h2>
<hr class="half-width">
<p style="text-align: justify;">
Principal component analysis (PCA) is one of the most commonly used
dimensionality reduction methods. Instead of describing the data using
the original measurements, it finds a new set of ranked features (the
“basis”). The new “features” are linear combinations of the original
variables, such that they are all <em>orthogonal</em> to each other, and
the “importance” of a feature (called the <strong>principal
component</strong>) is generally given by how much of the variability in
the dataset it captures (the so-called <strong>explained
variance</strong>). Thus, we first transform the data from the original
variables into a new set of features or principal components (PCs).
</p>
<p>To get a lower dimensional description of the data, we retain only
the top features (PCs) such that the largest variations or ‘patterns’ in
the dataset are preserved.</p>
<p style="text-align: justify;">
To apply <strong>PCA</strong> from <code>Scikit-learn</code>, we need to
specify the number of components (<code>n_components</code>) we want to
reduce the data to. <code>n_components</code> must be less than the
<em>rank</em> of the dataset, i.e. less than the smaller of the number
of samples and number of variables measured.
</p>
<div class="section level3">
<h3 id="simple-example-with-2d-generated-dataset">
<strong>Simple example with 2D generated dataset</strong><a class="anchor" aria-label="anchor" href="#simple-example-with-2d-generated-dataset"></a>
</h3>
<p>To first explore what dimensionality reduction with PCA looks like,
we will work with a simple 2-dimensional (2D) dataset, where we want to
find a 1D description instead.</p>
<p>We start with generating some observations from a two-dimensional
(multivariate) Gaussian distribution, with mean and covariance
specified.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>RND_SEED <span class="op">=</span> <span class="dv">7890</span></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>seed(RND_SEED)</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a><span class="co"># Set up 2D multivariate Gaussians</span></span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>means <span class="op">=</span> [<span class="dv">20</span>, <span class="dv">20</span>]</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>cov_mat <span class="op">=</span> [[<span class="dv">1</span>, <span class="fl">.85</span>], [<span class="fl">.85</span>, <span class="fl">1.5</span>]] <span class="co"># 2x2 covariance matrix must be symmetric and positive-semidefinite(&gt;=0)</span></span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a><span class="co"># Generate data</span></span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a>nSamples <span class="op">=</span> <span class="dv">300</span></span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a>data <span class="op">=</span> multivariate_normal(means, cov_mat, nSamples)</span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a><span class="bu">print</span>(data.shape)</span></code></pre>
</div>
<p>(300, 2)</p>
<p>We can check the distribution of the two features in histograms, and
we can see how the two data dimensions relate to each other in the
scatter plot.</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="co"># Center data</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>zdata <span class="op">=</span> data <span class="op">-</span> mean(data, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>fig, ax <span class="op">=</span> subplots(ncols<span class="op">=</span><span class="dv">3</span>, nrows<span class="op">=</span><span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">4</span>))</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>ax[<span class="dv">0</span>].hist(zdata[:, <span class="dv">0</span>])<span class="op">;</span></span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">'Dim 1'</span>, fontsize<span class="op">=</span><span class="dv">16</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)<span class="op">;</span></span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">'Values'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span></span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="st">'Frequency'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span></span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a>ax[<span class="dv">1</span>].hist(zdata[:, <span class="dv">1</span>])<span class="op">;</span></span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">'Dim 2'</span>, fontsize<span class="op">=</span><span class="dv">16</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)<span class="op">;</span></span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">'Values'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span></span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" tabindex="-1"></a>ax[<span class="dv">2</span>].scatter(zdata[:, <span class="dv">0</span>], zdata[:, <span class="dv">1</span>])<span class="op">;</span></span>
<span id="cb5-17"><a href="#cb5-17" tabindex="-1"></a>ax[<span class="dv">2</span>].set_title(<span class="st">'Sample Data'</span>, fontsize<span class="op">=</span><span class="dv">16</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb5-18"><a href="#cb5-18" tabindex="-1"></a>ax[<span class="dv">2</span>].set_xlabel(<span class="st">'Dim 1'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span></span>
<span id="cb5-19"><a href="#cb5-19" tabindex="-1"></a>ax[<span class="dv">2</span>].set_ylabel(<span class="st">'Dim 2'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span></span>
<span id="cb5-20"><a href="#cb5-20" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" tabindex="-1"></a>show()</span></code></pre>
</div>
<figure><img src="fig/03-dimensionality-rendered-unnamed-chunk-5-3.png" width="1440" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><div id="note" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="note" class="callout-inner">
<h3 class="callout-title">Note</h3>
<div class="callout-content">
<p><em>The last plot, with the samples plotted as scatter in the space
of measured variables is called a “state space plot”. The main axes
correspond to individual variables.</em></p>
</div>
</div>
</div>
<p style="text-align: justify;">
Now if we want to represent data by a single feature, we can see that
the individual variables are not ideal. If we only keep Dim 1, we lose
substantial variability along Dim 2. We see that the data points are
stretched along the main diagonal. This implies that the two variables
co-vary to some extent. Thus, a linear combination of the two might
capture this relevant pattern.
</p>
A linear combination corresponds to a vector (or direction) in this 2D
state space.
<p style="text-align: justify;">
For example, the x-axis vector (1, 0) = [1 x Dim1 + 0 x Dim2], i.e. it
contains Dim 1 only. More generally, (a, b) represents the linear
combination [a x Dim1 + b x Dim2], i.e. a mixture of the two variables.
Typically, we ensure that the length of these vectors is 1, to avoid
artefactual rescaling of data.
</p>
</div>
<div class="section level3">
<h3 id="pca-as-projection-or-rotation-of-basis">
<strong>PCA as projection or rotation of basis</strong><a class="anchor" aria-label="anchor" href="#pca-as-projection-or-rotation-of-basis"></a>
</h3>
<p style="text-align: justify;">
Instead of representing data by their values along Dim 1 and Dim 2, we
now try to describe them along different directions. We do so by
<em>projecting</em> 2D data onto that vector. To quantify the importance
of each direction, we measure the variance of the projected samples.
</p>
<p style="text-align: justify;">
Let us examine different vectors that point at various angles compared
to the x-axis. Remember that the vector denoted by the angle = <em>(
cos(angle), sin(angle) )</em> denotes the following linear combination
of the 2D sample d=[d1, d2]:
</p>
<p>f(d1,d2) = cos(angle) * d1 + sin(angle) * d2</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="co">## Centre data at origin by subtracting mean</span></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>zdata <span class="op">=</span> data <span class="op">-</span> mean(data, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a><span class="co"># Define a feature as a direction in the 2D space using the angle</span></span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>angle <span class="op">=</span> pi<span class="op">/</span><span class="dv">4</span></span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a><span class="co"># Obtain the vector</span></span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>vec <span class="op">=</span> [cos(angle), sin(angle)] <span class="co"># has norm=1 by definition, otherwise:  vec = vec/norm(vec)</span></span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a><span class="co"># Project 2D data onto this single dimension</span></span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a>dim1_data <span class="op">=</span> zdata.dot(vec)</span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" tabindex="-1"></a>fig, ax <span class="op">=</span> subplots(ncols<span class="op">=</span><span class="dv">2</span>, nrows<span class="op">=</span><span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">6</span>))</span>
<span id="cb6-15"><a href="#cb6-15" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" tabindex="-1"></a><span class="co"># Plot data and the feature direction</span></span>
<span id="cb6-17"><a href="#cb6-17" tabindex="-1"></a>ax[<span class="dv">0</span>].scatter(zdata[:, <span class="dv">0</span>], zdata[:, <span class="dv">1</span>])<span class="op">;</span></span>
<span id="cb6-18"><a href="#cb6-18" tabindex="-1"></a>ax[<span class="dv">0</span>].quiver([<span class="dv">0</span>], [<span class="dv">0</span>], vec[<span class="dv">0</span>], vec[<span class="dv">1</span>], scale<span class="op">=</span><span class="dv">3</span>)<span class="op">;</span></span>
<span id="cb6-19"><a href="#cb6-19" tabindex="-1"></a></span>
<span id="cb6-20"><a href="#cb6-20" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">'Sample Data'</span>, fontsize<span class="op">=</span><span class="dv">16</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb6-21"><a href="#cb6-21" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">'Dim 1'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span> ax[<span class="dv">0</span>].set_xlim([<span class="op">-</span><span class="dv">4</span>,<span class="dv">4</span>])<span class="op">;</span></span>
<span id="cb6-22"><a href="#cb6-22" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="st">'Dim 2'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span> ax[<span class="dv">0</span>].set_ylim([<span class="op">-</span><span class="dv">4</span>,<span class="dv">4</span>])<span class="op">;</span></span>
<span id="cb6-23"><a href="#cb6-23" tabindex="-1"></a></span>
<span id="cb6-24"><a href="#cb6-24" tabindex="-1"></a><span class="co"># Plot histogram of new projected data</span></span>
<span id="cb6-25"><a href="#cb6-25" tabindex="-1"></a>ax[<span class="dv">1</span>].hist(dim1_data, bins <span class="op">=</span> linspace(<span class="op">-</span><span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">9</span>))<span class="op">;</span></span>
<span id="cb6-26"><a href="#cb6-26" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">'Along vec'</span>, fontsize<span class="op">=</span><span class="dv">16</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)<span class="op">;</span></span>
<span id="cb6-27"><a href="#cb6-27" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">'Values'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span></span>
<span id="cb6-28"><a href="#cb6-28" tabindex="-1"></a>ax[<span class="dv">1</span>].set_ylabel(<span class="st">'Frequency'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span></span>
<span id="cb6-29"><a href="#cb6-29" tabindex="-1"></a></span>
<span id="cb6-30"><a href="#cb6-30" tabindex="-1"></a>show()</span></code></pre>
</div>
<figure><img src="fig/03-dimensionality-rendered-unnamed-chunk-6-5.png" width="1440" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><div id="do-it-yourself" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="do-it-yourself" class="callout-inner">
<h3 class="callout-title">Do it Yourself</h3>
<div class="callout-content">
<p>Change the angle in the code above and see how distribution of
projected data changes. Use e.g. 0, <span class="math inline">\(\pi\)</span> /2, 2/3*<span class="math inline">\(\pi\)</span>, and <span class="math inline">\(\pi\)</span>. Try to estimate how much the
variance changes.</p>
</div>
</div>
</div>
<p style="text-align: justify;">
Now let’s formally calculate the variance by projecting along a set of
different directions. We use angles between 0 and <span class="math inline">\(\pi\)</span> (i.e. between 0 and 180 degrees). For
each direction (projection) we calculate the variance and plot it
against the angle.
</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>allAngles <span class="op">=</span> linspace( start<span class="op">=</span><span class="dv">0</span>, stop<span class="op">=</span>pi, num<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>allVar <span class="op">=</span> []</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a><span class="cf">for</span> angle <span class="kw">in</span> allAngles:</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>    vec <span class="op">=</span> array([cos(angle), sin(angle)]) <span class="co">#Unit length vector on which data will be projected</span></span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>    dim1_data <span class="op">=</span> data.dot(vec)</span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a>    allVar.append( var(dim1_data))</span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a><span class="co"># Plot variance captured by different features</span></span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a>scatter( allAngles, allVar )<span class="op">;</span></span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a>title(<span class="st">'Variance along different directions'</span>, fontsize<span class="op">=</span><span class="dv">16</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a>xlabel(<span class="st">'Direction (angle in radians)'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span></span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a>ylabel(<span class="st">'Variance of projected data'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span></span></code></pre>
</div>
<figure><img src="fig/03-dimensionality-rendered-unnamed-chunk-7-7.png" width="672" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p style="text-align: justify;">
It can be seen there is a particular direction that retains maximal
variance. This would be the optimal feature to give a 1D description of
the 2D data while retaining maximum variability. PCA is the method that
directly find this optimal direction. This makes it especially powerful
for high dimensional datasets.
</p>
</div>
<div class="section level3">
<h3 id="find-1-d-description-using-pca">
<strong>Find 1-D description using PCA</strong><a class="anchor" aria-label="anchor" href="#find-1-d-description-using-pca"></a>
</h3>
<p style="text-align: justify;">
Let us now use dimensionality reduction <code>PCA</code> to directly
find the direction that captures maximal variance. For an n-dimensional
dataset, PCA finds a set of <em>n</em> new features, ranked by the
variance along each feature. See the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" class="external-link">Scikit-learn
documentation</a> for information.
</p>
<p>To get the best 1D representation of data, we instantiate the class
with one component (<em>n_components</em> = 1) to be returned, and
transform the data to the feature space of that component.</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a><span class="co"># Initialize the PCA model</span></span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>pcaResults <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">1</span>, whiten<span class="op">=</span><span class="va">False</span>) <span class="co"># specify no. of components, and whether to standardize</span></span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a><span class="co"># Fit to data</span></span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a>pcaResults <span class="op">=</span> pcaResults.fit(data)</span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a><span class="co"># Transform the data, using our learned PCA representation</span></span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a>dim1_data <span class="op">=</span> pcaResults.transform(data)</span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a></span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a><span class="bu">print</span>( <span class="st">"Size of new dataset: "</span>, dim1_data.shape )</span></code></pre>
</div>
<p>Size of new dataset: (300, 1)</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="ex">Size</span> of new dataset:  <span class="er">(</span><span class="ex">300,</span> 1<span class="kw">)</span></span></code></pre>
</div>
<p>From the fitted model ‘pcaResults’ we can now get:</p>
<ul>
<li><p>The fractional variance using
<code>explained_variance_ratio_</code>. It tells us what fraction of the
total variance is retained by th reduced 1D description of the
data.</p></li>
<li><p>The direction (angle) of this first PC as calculate from the
arcus tangens <code>atan</code> of the vector. You can compare it
directly to the angle in the above plot.</p></li>
</ul>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="co"># How much variance was captured compared to original data?</span></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Fractional variance captured by first PC: </span><span class="sc">{:1.4f}</span><span class="st">."</span>.<span class="bu">format</span>(pcaResults.explained_variance_ratio_[<span class="dv">0</span>]))</span></code></pre>
</div>
<p>Fractional variance captured by first PC: 0.8722.</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>vec1 <span class="op">=</span> pcaResults.components_[<span class="dv">0</span>,:]</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Direction of PC1 is at angle = </span><span class="sc">{:1.2f}</span><span class="st"> radians"</span>.<span class="bu">format</span>(atan2(vec1[<span class="dv">1</span>],vec1[<span class="dv">0</span>])))</span></code></pre>
</div>
<p>Direction of PC1 is at angle = 0.99 radians</p>
<div id="do-it-yourself-1" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="do-it-yourself-1" class="callout-inner">
<h3 class="callout-title">Do it Yourself</h3>
<div class="callout-content">
<p>Exercise (DIY): Change the covariance of the synthetic data, and
repeat the above steps. Try to infer the relationship between the shape
of the scatter, the strength of correlation between the two dimensions,
and the variance captured by the first PC.</p>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="find-2-d-description-using-pca">
<strong>Find 2-D description using PCA</strong><a class="anchor" aria-label="anchor" href="#find-2-d-description-using-pca"></a>
</h3>
<p style="text-align: justify;">
Our dataset has two features. We can thus obtain two principal
components. The two components must be orthogonal to each other (form a
right angle). Let us get the corresponding vectors and plot them on the
scatterplot.
</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="co"># Initialize the PCA model</span></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>pcaResults <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">2</span>, whiten<span class="op">=</span><span class="va">False</span>) <span class="co"># specify no. of components, and whether to standardize</span></span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a><span class="co"># Fit to data</span></span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a>pcaResults <span class="op">=</span> pcaResults.fit(data)</span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a><span class="co"># Transform the data, using our learned PCA representation</span></span>
<span id="cb12-8"><a href="#cb12-8" tabindex="-1"></a>dim1_data <span class="op">=</span> pcaResults.transform(data)</span>
<span id="cb12-9"><a href="#cb12-9" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" tabindex="-1"></a><span class="bu">print</span>( <span class="st">"Size of new dataset: "</span>, dim1_data.shape )</span></code></pre>
</div>
<p>Size of new dataset: (300, 2)</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="bu">print</span>(pcaResults.components_)</span></code></pre>
</div>
<p>[[ 0.54864625 0.8360546 ] [ 0.8360546 -0.54864625]]</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>vec1 <span class="op">=</span> pcaResults.components_[<span class="dv">0</span>,:]</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a>vec2 <span class="op">=</span> pcaResults.components_[<span class="dv">1</span>,:]</span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a>fig, ax <span class="op">=</span> subplots(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>))</span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a><span class="co"># Plot data and the feature direction</span></span>
<span id="cb14-8"><a href="#cb14-8" tabindex="-1"></a>ax.scatter(zdata[:, <span class="dv">0</span>], zdata[:, <span class="dv">1</span>])<span class="op">;</span></span>
<span id="cb14-9"><a href="#cb14-9" tabindex="-1"></a>ax.quiver([<span class="dv">0</span>], [<span class="dv">0</span>], vec1[<span class="dv">0</span>], vec1[<span class="dv">1</span>], scale<span class="op">=</span><span class="dv">3</span>, color<span class="op">=</span><span class="st">'r'</span>)<span class="op">;</span></span>
<span id="cb14-10"><a href="#cb14-10" tabindex="-1"></a>ax.quiver([<span class="dv">0</span>], [<span class="dv">0</span>], vec2[<span class="dv">0</span>], vec2[<span class="dv">1</span>], scale<span class="op">=</span><span class="dv">3</span>, color<span class="op">=</span><span class="st">'r'</span>)<span class="op">;</span></span>
<span id="cb14-11"><a href="#cb14-11" tabindex="-1"></a></span>
<span id="cb14-12"><a href="#cb14-12" tabindex="-1"></a>ax.set_title(<span class="st">'Sample Data'</span>, fontsize<span class="op">=</span><span class="dv">16</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb14-13"><a href="#cb14-13" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Dim 1'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span> ax.set_xlim([<span class="op">-</span><span class="dv">4</span>,<span class="dv">4</span>])<span class="op">;</span></span>
<span id="cb14-14"><a href="#cb14-14" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Dim 2'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span> ax.set_ylim([<span class="op">-</span><span class="dv">4</span>,<span class="dv">4</span>])<span class="op">;</span></span>
<span id="cb14-15"><a href="#cb14-15" tabindex="-1"></a></span>
<span id="cb14-16"><a href="#cb14-16" tabindex="-1"></a>show()</span></code></pre>
</div>
<figure><img src="fig/03-dimensionality-rendered-unnamed-chunk-10-9.png" width="576" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p style="text-align: justify;">
The red arrows show the new coordinate system, defined by the principal
components. It also shows that the more correlated the data are, the
less information we lose by reducing the data representation using PCA.
</p>
<div id="important" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="important" class="callout-inner">
<h3 class="callout-title">Important</h3>
<div class="callout-content">
<p>When reading about PCA you will come across the term <strong>Singular
Value Decomposition</strong> (SVD). As mentioned in the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" class="external-link">Scikit-learn
documentation for PCA</a>, when you apply <code>PCA</code> the algorithm
in the background uses SVD to return the results. The difference between
the two is technical but SVD is the more general method. Wikipedia has
as <a href="https://en.wikipedia.org/wiki/Principal_component_analysis#Singular_value_decomposition" class="external-link">mathematical
introduction to PCA</a> which also discusses <a href="https://en.wikipedia.org/wiki/Principal_component_analysis#Singular_value_decomposition" class="external-link">the
relationship to SVD</a>.</p>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="pca-of-the-gene-expression-data">PCA of the Gene Expression Data<a class="anchor" aria-label="anchor" href="#pca-of-the-gene-expression-data"></a>
</h2>
<hr class="half-width">
<p style="text-align: justify;">
Now, we will use PCA to find a few features that capture most of the
variability in the gene expression dataset. we arbitrarily start with 50
components assuming that this will include the important features.
</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>nComp <span class="op">=</span> <span class="dv">50</span> <span class="co"># Number of PCs to be returned</span></span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a><span class="co">#trainIndx = binomial(1,0.9,size=filectr)</span></span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a>GenePCA <span class="op">=</span> PCA(n_components<span class="op">=</span>nComp, whiten<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a>GenePCA <span class="op">=</span> GenePCA.fit(geneData) <span class="co">#.values[trainIndx==1,:]</span></span></code></pre>
</div>
<p style="text-align: justify;">
How many features should we retain? To investigate this question, we
plot the total (cumulative) variance for retaining the top <em>k</em>
modes. The more we use, the higher the cumulative variance.
</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a>cumExpVar <span class="op">=</span> cumsum(GenePCA.explained_variance_ratio_)</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a>fig <span class="op">=</span> figure(figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">5</span>))</span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a>im  <span class="op">=</span> plot( <span class="bu">range</span>(nComp), cumExpVar )</span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a>xticks(arange(<span class="dv">0</span>,nComp,<span class="dv">5</span>))<span class="op">;</span></span>
<span id="cb16-5"><a href="#cb16-5" tabindex="-1"></a>xlabel( <span class="st">'Number of PCs'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span></span>
<span id="cb16-6"><a href="#cb16-6" tabindex="-1"></a>ylabel(<span class="st">'Cumulative explained variance'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span></span>
<span id="cb16-7"><a href="#cb16-7" tabindex="-1"></a></span>
<span id="cb16-8"><a href="#cb16-8" tabindex="-1"></a>show()</span></code></pre>
</div>
<figure><img src="fig/03-dimensionality-rendered-unnamed-chunk-12-11.png" width="480" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p style="text-align: justify;">
A common heuristic for choosing the number of components is by defining
a set threshold for the total explained variance. Thresholds commonly
vary between 0.8-0.9, depending on the structure of the PCs,
e.g. depending on whether there are a few top PCs or many small PCs, but
also depending on the expected noise in data, and on the desirable
accuracy of the reduced data set. While a dimensionality reduction is
convenient it always loses some information.
</p>
<p>As an example, we can check how many PCs we need to retain 99 % of
the variance.</p>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a>threshold <span class="op">=</span> <span class="fl">0.99</span></span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a>keepPC <span class="op">=</span> [pc <span class="cf">for</span> pc <span class="kw">in</span> <span class="bu">range</span>(nComp) <span class="cf">if</span> cumExpVar[pc]<span class="op">&gt;=</span>threshold][<span class="dv">0</span>]</span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Number of features needed to explain </span><span class="sc">{:1.2f}</span><span class="st"> fraction of total variance is </span><span class="sc">{:2d}</span><span class="st">. '</span>.<span class="bu">format</span>(threshold, keepPC) )</span></code></pre>
</div>
<p>Number of features needed to explain 0.99 fraction of total variance
is 19.</p>
<p>There are many alternative methods for estimating the number of
features. These include:</p>
<ul>
<li><p>Plot explained variance of individual PCs and cut off when there
is a sharp drop/saturation in the values.</p></li>
<li><p>Cross-validation - Find number of PCs that maximize explained
variance on heldout data (using a bi-cross validation scheme).</p></li>
<li><p>Visual inspection or interpretable PCs</p></li>
</ul>
<div class="section level3">
<h3 id="visualization-of-reduced-dataset">
<strong>Visualization of reduced dataset</strong><a class="anchor" aria-label="anchor" href="#visualization-of-reduced-dataset"></a>
</h3>
<p>Now that we have selected the number of features to be used, we can
see what the data along those features looks like.</p>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a>newGeneData <span class="op">=</span> GenePCA.transform(geneData)[:,<span class="bu">range</span>(keepPC)]</span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a>fig, ax <span class="op">=</span> subplots(ncols<span class="op">=</span><span class="dv">3</span>, nrows<span class="op">=</span><span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">18</span>, <span class="dv">8</span>))</span>
<span id="cb18-4"><a href="#cb18-4" tabindex="-1"></a>suptitle(<span class="st">'Data in new Feature space'</span>, fontsize<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb18-5"><a href="#cb18-5" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">0</span>].scatter(newGeneData[:,<span class="dv">0</span>], newGeneData[:,<span class="dv">1</span>], c<span class="op">=</span><span class="bu">range</span>(filectr), cmap<span class="op">=</span><span class="st">'summer'</span>)<span class="op">;</span></span>
<span id="cb18-7"><a href="#cb18-7" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">0</span>].set_xlabel(<span class="st">'PC1'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span> ax[<span class="dv">0</span>, <span class="dv">0</span>].set_xlim([<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>])<span class="op">;</span></span>
<span id="cb18-8"><a href="#cb18-8" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">0</span>].set_ylabel(<span class="st">'PC2'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span> ax[<span class="dv">0</span>, <span class="dv">0</span>].set_ylim([<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>])<span class="op">;</span></span>
<span id="cb18-9"><a href="#cb18-9" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" tabindex="-1"></a></span>
<span id="cb18-11"><a href="#cb18-11" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">1</span>].scatter(newGeneData[:,<span class="dv">0</span>], newGeneData[:,<span class="dv">2</span>], c<span class="op">=</span><span class="bu">range</span>(filectr), cmap<span class="op">=</span><span class="st">'summer'</span>)<span class="op">;</span></span>
<span id="cb18-12"><a href="#cb18-12" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">1</span>].set_xlabel(<span class="st">'PC1'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span> ax[<span class="dv">0</span>, <span class="dv">1</span>].set_xlim([<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>])<span class="op">;</span></span>
<span id="cb18-13"><a href="#cb18-13" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">1</span>].set_ylabel(<span class="st">'PC3'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span> ax[<span class="dv">0</span>, <span class="dv">1</span>].set_ylim([<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>])<span class="op">;</span></span>
<span id="cb18-14"><a href="#cb18-14" tabindex="-1"></a></span>
<span id="cb18-15"><a href="#cb18-15" tabindex="-1"></a></span>
<span id="cb18-16"><a href="#cb18-16" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">2</span>].scatter(newGeneData[:,<span class="dv">0</span>], newGeneData[:,<span class="dv">3</span>], c<span class="op">=</span><span class="bu">range</span>(filectr), cmap<span class="op">=</span><span class="st">'summer'</span>)<span class="op">;</span></span>
<span id="cb18-17"><a href="#cb18-17" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">2</span>].set_xlabel(<span class="st">'PC1'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span> ax[<span class="dv">0</span>, <span class="dv">2</span>].set_xlim([<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>])<span class="op">;</span></span>
<span id="cb18-18"><a href="#cb18-18" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">2</span>].set_ylabel(<span class="st">'PC4'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span> ax[<span class="dv">0</span>, <span class="dv">2</span>].set_ylim([<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>])<span class="op">;</span></span>
<span id="cb18-19"><a href="#cb18-19" tabindex="-1"></a></span>
<span id="cb18-20"><a href="#cb18-20" tabindex="-1"></a></span>
<span id="cb18-21"><a href="#cb18-21" tabindex="-1"></a></span>
<span id="cb18-22"><a href="#cb18-22" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">0</span>].scatter(newGeneData[:,<span class="dv">1</span>], newGeneData[:,<span class="dv">2</span>], c<span class="op">=</span><span class="bu">range</span>(filectr), cmap<span class="op">=</span><span class="st">'summer'</span>)<span class="op">;</span></span>
<span id="cb18-23"><a href="#cb18-23" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">0</span>].set_xlabel(<span class="st">'PC2'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span> ax[<span class="dv">1</span>, <span class="dv">0</span>].set_xlim([<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>])<span class="op">;</span></span>
<span id="cb18-24"><a href="#cb18-24" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">0</span>].set_ylabel(<span class="st">'PC3'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span> ax[<span class="dv">1</span>, <span class="dv">0</span>].set_ylim([<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>])<span class="op">;</span></span>
<span id="cb18-25"><a href="#cb18-25" tabindex="-1"></a></span>
<span id="cb18-26"><a href="#cb18-26" tabindex="-1"></a></span>
<span id="cb18-27"><a href="#cb18-27" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">1</span>].scatter(newGeneData[:,<span class="dv">1</span>], newGeneData[:,<span class="dv">3</span>], c<span class="op">=</span><span class="bu">range</span>(filectr), cmap<span class="op">=</span><span class="st">'summer'</span>)<span class="op">;</span></span>
<span id="cb18-28"><a href="#cb18-28" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">1</span>].set_xlabel(<span class="st">'PC2'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span> ax[<span class="dv">1</span>, <span class="dv">1</span>].set_xlim([<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>])<span class="op">;</span></span>
<span id="cb18-29"><a href="#cb18-29" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">1</span>].set_ylabel(<span class="st">'PC4'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span> ax[<span class="dv">1</span>, <span class="dv">1</span>].set_ylim([<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>])<span class="op">;</span></span>
<span id="cb18-30"><a href="#cb18-30" tabindex="-1"></a></span>
<span id="cb18-31"><a href="#cb18-31" tabindex="-1"></a></span>
<span id="cb18-32"><a href="#cb18-32" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">2</span>].scatter(newGeneData[:,<span class="dv">3</span>], newGeneData[:,<span class="dv">2</span>], c<span class="op">=</span><span class="bu">range</span>(filectr), cmap<span class="op">=</span><span class="st">'summer'</span>)<span class="op">;</span></span>
<span id="cb18-33"><a href="#cb18-33" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">2</span>].set_xlabel(<span class="st">'PC4'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span> ax[<span class="dv">1</span>, <span class="dv">2</span>].set_xlim([<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>])<span class="op">;</span></span>
<span id="cb18-34"><a href="#cb18-34" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">2</span>].set_ylabel(<span class="st">'PC3'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span> ax[<span class="dv">1</span>, <span class="dv">2</span>].set_ylim([<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>])<span class="op">;</span></span>
<span id="cb18-35"><a href="#cb18-35" tabindex="-1"></a></span>
<span id="cb18-36"><a href="#cb18-36" tabindex="-1"></a>show()</span></code></pre>
</div>
<figure><img src="fig/03-dimensionality-rendered-unnamed-chunk-14-13.png" width="1728" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure>
</div>
<div class="section level3">
<h3 id="beyond-dimensionality-reduction">
<strong>Beyond dimensionality reduction</strong><a class="anchor" aria-label="anchor" href="#beyond-dimensionality-reduction"></a>
</h3>
<p>After reducing dataset from hundreds or thousands of genes to fewer
features (PCs), it can be used:</p>
<ul>
<li><p>To find groups of genes that seem to be co-regulated (using
clustering).</p></li>
<li><p>To build classifiers to predict the likelihood of juvenile or
late-onset rheumatoid disease. For example, one feature can be highly
variable in the sampled population and therefore has high explained
variance, but it may not predictive for the current objective of
predicting disease phenotype (e.g. eye pigmentation genes).</p></li>
<li><p>To find what combination of genes the top features correspond
to.</p></li>
<li><p>To suggest what individual genes are most variable.</p></li>
<li><p>To discover co-expression patterns of multiple genes.</p></li>
</ul>
<div id="optional-exercises" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="optional-exercises" class="callout-inner">
<h3 class="callout-title">Optional Exercises</h3>
<div class="callout-content">
<ol style="list-style-type: decimal">
<li><p>The dataset contains two types of samples, expression in
neutrophil and expression in peripheral blood mononuclear cells (PBMC).
Download the <a href="https://www.ebi.ac.uk/arrayexpress/experiments/E-GEOD-11083/samples/" class="external-link">sample-data
table</a> to get the category of each sample. Make a scatter plot of two
(original) features and set the colour of the scatter based on the
sample’s origin. Do a PCA, scatter the top scoring components and colour
the samples in the same way. Do the top PCs capture gene expression
differences in the two types of cells?</p></li>
<li><p>The dataset similarly contains samples from individuals with
differing phenotypes - healthy, as well as 2 different disease
characteristics. Similar to the previous exercise, colour the scatter
based on the disease phenotypes. Do the top PCs capture variability in
gene expression for different phenotypes?</p></li>
</ol>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="when-does-pca-fail">When does PCA fail?<a class="anchor" aria-label="anchor" href="#when-does-pca-fail"></a>
</h2>
<hr class="half-width">
<p>Although PCA is the first and simplest method of exploring
high-dimensional datasets, there are important caveats to keep in
mind:</p>
<ol style="list-style-type: decimal">
<li><em>PCA is a linear method</em></li>
</ol>
<p>Nonlinear patterns of co-expression will be seemingly broken up into
many features. Thus, the ability to find whether genes are co-regulated
may be reduced if the effects are nonlinear.</p>
<ol start="2" style="list-style-type: decimal">
<li><em>PCA depends on scaling of individual measurements.</em></li>
</ol>
<p>As PCA measures variance along different directions, changing the
scale of one variable (for example from cm to mm, or fold-change to
actual levels) may drastically change the dominant “features”
extracted.</p>
<p>The lesson on clustering of images with Gaussian mixture models
contained instructions to scale data. You can explore the original
dataset of this lesson, to investigate how scaling of variables impacts
the results.</p>
<ol start="3" style="list-style-type: decimal">
<li><em>Estimating number of features is heuristic, and depends on the
purpose.</em></li>
</ol>
<p>The definition of what is the ‘relevant’ number of features can
depend on:</p>
<ul>
<li>the signal-to-noise ratio in data. For example, noisier or smaller
datasets make it harder to accurately estimate smaller PCs. In such
cases, a more conservative threshold should be used.</li>
<li>the classifier or regression performance following the
dimensionality reduction.</li>
</ul></section><section><h2 class="section-heading" id="resources">Resources<a class="anchor" aria-label="anchor" href="#resources"></a>
</h2>
<hr class="half-width">
<ul>
<li><p><a href="http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html" class="external-link">Detailed
tutorial on PCA</a></p></li>
<li>
<p><a href="https://towardsdatascience.com/dimension-reduction-techniques-with-python-f36ca7009e5c" class="external-link">Other
methods for dimensionality reduction</a>:</p>
<ul>
<li><p><a href="https://scikit-learn.org/stable/modules/decomposition.html" class="external-link">Matrix
factorization methods</a></p></li>
<li><p><a href="https://scikit-learn.org/stable/modules/manifold.html" class="external-link">Manifold
learning or topological methods</a></p></li>
<li><p><a href="https://scikit-learn.org/stable/modules/unsupervised_reduction.html" class="external-link">Example
pipelines with Scikit-learn</a></p></li>
</ul>
</li>
</ul>
<p>Dataset from:</p>
<p>Frank MB, Wang S, Aggarwal A, et al. <strong>Disease-associated
pathophysiologic structures in pediatric rheumatic diseases show
characteristics of scale-free networks seen in physiologic systems:
implications for pathogenesis and treatment.</strong> <em>BMC Med
Genomics. 2009;2:9.</em> Published 2009 Feb 23. <a href="doi:10.1186/1755-8794-2-9" class="uri">doi:10.1186/1755-8794-2-9</a></p>
<p>Download data from: <a href="https://www.ebi.ac.uk/arrayexpress/experiments/E-GEOD-11083/" class="external-link uri">https://www.ebi.ac.uk/arrayexpress/experiments/E-GEOD-11083/</a></p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Reduced features in a dataset reduce redundancy and process is
called dimensionality reduction.</li>
<li>Principal component analysis (PCA) is one of the commonly used
dimensionality reduction methods.</li>
<li>
<code>n_components</code> is used specify the number of components
in <code>Scikit-learn</code>.</li>
<li>PCA can be helpful to find groups of genes that seem to be
co-regulated.</li>
</ul>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</section></section>
</div>
    </main>
</div>
<!-- END  : inst/pkgdown/templates/content-extra.html -->

      </div>
<!--/div.row-->
      		<footer class="row footer mx-md-3"><hr>
<div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/carpentries/workbench-template-md/edit/main/README.md" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/carpentries/workbench-template-md/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/carpentries/workbench-template-md/" class="external-link">Source</a></p>
				<p><a href="https://github.com/carpentries/workbench-template-md/blob/main/CITATION" class="external-link">Cite</a> | <a href="mailto:team@carpentries.org">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/69aaefee46a17928e2a1694825599e169b9793e3" class="external-link">sandpaper (0.16.10)</a>, <a href="https://github.com/carpentries/pegboard/tree/bad0be19a12f0c6545801b276ddf26c945f8bfd1" class="external-link">pegboard (0.7.7)</a>, and <a href="https://github.com/milanmlft/varnish/tree/milanmlft/sticky-sidebar" class="external-link">varnish (1.0.3.9000)</a></p>
			</div>
		</footer>
</div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "TrainingMaterial",
  "@id": "https://carpentries.github.io/workbench-template-md/aio.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries",
  "name": "All in One View",
  "creativeWorkStatus": "active",
  "url": "https://carpentries.github.io/workbench-template-md/aio.html",
  "identifier": "https://carpentries.github.io/workbench-template-md/aio.html",
  "dateCreated": "2022-07-05",
  "dateModified": "2025-01-27",
  "datePublished": "2025-01-27"
}

  </script><script>
		feather.replace();
	</script>
</body>
</html><!-- END:   inst/pkgdown/templates/layout.html-->

