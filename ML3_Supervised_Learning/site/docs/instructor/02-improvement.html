<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><title>Machine Learning - Supervised: Improvement</title><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="stylesheet" type="text/css" href="../assets/styles.css"><script src="../assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png"><link rel="manifest" href="../site.webmanifest"><link rel="mask-icon" href="../safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" content="#ffffff"></head><body>
    <header id="top" class="navbar navbar-expand-md navbar-light bg-white top-nav l2d"><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-6">
      <div class="large-logo">
        <img alt="l2d" src="../assets/images/l2d-logo.svg"></div>
    </div>
    <div class="selector-container">


      <div class="dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1"><li><button class="dropdown-item" type="button" onclick="window.location.href='../02-improvement.html';">Learner View</button></li>
        </ul></div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl navbar-light bg-white bottom-nav l2d" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="l2d" src="../assets/images/l2d-logo-sm.svg"></div>
    <div class="lesson-title-md">
      Machine Learning - Supervised
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0"><li class="nav-item">
          <span class="lesson-title">
            Machine Learning - Supervised
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/instructor-notes.html">Instructor Notes</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/images.html">Extract All Images</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown"><hr></ul></li>
      </ul></div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a class="btn btn-primary" href="../instructor/aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div><!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Machine Learning - Supervised
</div>

<aside class="col-md-12 lesson-progress"><div style="width: 33%" class="percentage">
    33%
  </div>
  <div class="progress l2d">
    <div class="progress-bar l2d" role="progressbar" style="width: 33%" aria-valuenow="33" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar" style="display: flex">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle sticky-top" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner sticky-top">
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="../02-improvement.html">Learner View</a>
                      </div>
                    </div>
                  </div><!--/div.accordion-item-->
                </div><!--/div.accordion-flush-->
              </div><!--div.sidenav-view-selector -->
            </div><!--/div.col -->

            <hr></div><!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Schedule</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="01-classification_intro.html">1. Classification</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlushcurrent">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-headingcurrent">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapsecurrent" aria-expanded="true" aria-controls="flush-collapsecurrent">
        <span class="visually-hidden">Current Chapter</span>
        <span class="current-chapter">
        2. Improvement
        </span>
      </button>
    </div><!--/div.accordion-header-->

    <div id="flush-collapsecurrent" class="accordion-collapse collapse show" aria-labelledby="flush-headingcurrent" data-bs-parent="#accordionFlushcurrent">
      <div class="accordion-body">
        <ul><li><a href="#training-a-variety-of-machine-learning-models">Training a variety of machine learning models</a></li>
<li><a href="#the-stratified-shuffle-split">The Stratified Shuffle Split</a></li>
<li><a href="#evaluation-roc-and-auc">Evaluation: ROC and AUC</a></li>
<li><a href="#exercises">Exercises</a></li>
        </ul></div><!--/div.accordion-body-->
    </div><!--/div.accordion-collapse-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="03-refinement.html">3. Refinement</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width"><div class="accordion accordion-flush resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul><li>
                        <a href="../instructor/key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="../instructor/instructor-notes.html">Instructor Notes</a>
                      </li>
                      <li>
                        <a href="../instructor/images.html">Extract All Images</a>
                      </li>
                      <hr></ul></div>
                </div>
              </div>
            </div>
            <hr class="half-width resources"><a href="../instructor/aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none"><div class="d-grid gap-1">

            </div>
          </div><!-- /div.accordion -->
        </div><!-- /div.sidebar-inner -->
      </nav></div><!-- /div.sidebar -->
  </div><!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-instructor.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <nav class="lesson-content mx-md-4" aria-label="Previous and Next Chapter"><!-- content for small screens --><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="../instructor/01-classification_intro.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="../instructor/03-refinement.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="../instructor/01-classification_intro.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Classification
        </a>
        <a class="chapter-link float-end" href="../instructor/03-refinement.html" rel="next">
          Next: Refinement...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
      <hr></nav><main id="main-content" class="main-content"><div class="container lesson-content">
        <h1>Improvement</h1>
        <p>Last updated on 2025-03-31 |

        <a href="https://github.com/carpentries/workbench-template-md/edit/main/episodes/02-improvement.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>



        <p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 12 minutes</p>

        <div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>

        

<p><a href="https://drive.usercontent.google.com/u/1/uc?id=1Gib2BeOLzIb6dgVMQQuPqw8nLA8Ti0SX&amp;export=download" class="external-link"><strong>Download
Chapter notebook (ipynb)</strong></a></p>
<p><a href="https://drive.usercontent.google.com/u/1/uc?id=16tDnxEFO77t0UusDStdhXdnm9duuMCOP&amp;export=download" class="external-link"><strong>Download
Chapter PDF</strong></a></p>
<p><a href="https://docs.google.com/forms/d/e/1FAIpQLSdr0capF7jloJhPH3Pki1B3LZoKOG16poOpuVJ7SL2LkwLHQA/viewform?pli=1" class="external-link"><span style="color: rgb(255, 0, 0);"><strong>Mandatory Lesson Feedback
Survey</strong></span></a></p>
<div id="discussion1" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Discussion</h3>
<div class="callout-content">
<ul><li>How to deal with complex classification problems?</li>
<li>Why is it important to use different classification algorithms?</li>
<li>What is the best way to find the optimal classifier?</li>
<li>How can we avoid over-fitting of data?</li>
<li>How do we evaluate the performance of classifiers?</li>
</ul></div>
</div>
</div>
<p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/LH3cUN7WXlg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</p>
<br><p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/GvUvwHmTXUs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</p>
<br><p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/xjpQRhtY1l0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</p>
<br><p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/nEyt1Ht8GOk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</p>
<p><br></p>
<div id="remarks" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="remarks" class="callout-inner">
<h3 class="callout-title">Remarks</h3>
<div class="callout-content">
<ol style="list-style-type: decimal"><li><p>From now on the code will become more complex. When copied, the
code should run without errors with the given data sets. (Please report
any errors thrown when running the code without modifications).</p></li>
<li><p>Make a copy of the notebook and start experimenting by modifying
part of the code and comparing the outcome. Modifying existing code is
one of the successful strategies when learning to programme as a
non-programmer.</p></li>
<li><p>The first resource to consult when facing bugs are the official
documentations, be it Python, Numpy, SciKit Learn or other.</p></li>
<li><p>If you formulate a problem adequately, often there may be good
answers on <a href="https://stackoverflow.com" class="external-link">Stack
Overflow</a>.</p></li>
<li><p>Sometimes, simply copying and pasting an error message into the
search engine can point you to the solution.</p></li>
</ol></div>
</div>
</div>
<div class="section level3">
<h3 id="import-functions"><strong>Import functions</strong><a class="anchor" aria-label="anchor" href="#import-functions"></a></h3>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> mgrid, linspace, c_, arange, mean, array</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">from</span> numpy.random <span class="im">import</span> uniform, seed</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="im">from</span> mpl_toolkits <span class="im">import</span> mplot3d</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="im">from</span> matplotlib.pyplot <span class="im">import</span> subplots, axes, scatter, xticks, show</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_circles</span></code></pre>
</div>
<div id="challenge" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="challenge" class="callout-inner">
<h3 class="callout-title">Challenge</h3>
<div class="callout-content">
<p style="text-align: justify;">
We would like to test several machine learning models’ ability to deal
with a complicated task. A complicated task is one where the topology of
the labelled data is not trivially separable into classes by
(hyper)planes, e.g. by a straight line in a scatter plot.
</p>
<p>Our example is one class of data organised in a doughnut shape and
the other class contained within the first doughnut forming a
doughnut-within-a-doughnut.</p>
<p>Here is the function code to create these data, followed by a
function call to produce a figure.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="kw">def</span> make_torus_3D(n_samples<span class="op">=</span><span class="dv">100</span>, shuffle<span class="op">=</span><span class="va">True</span>, noise<span class="op">=</span><span class="va">None</span>, random_state<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>                 factor<span class="op">=</span><span class="fl">.8</span>):</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>    <span class="co">"""Make a large torus containing a smaller torus in 3-D.</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a><span class="co">    A toy dataset to visualise clustering and classification</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a><span class="co">    algorithms.</span></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a><span class="co">    Read more in the User Guide.</span></span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a><span class="co">    Parameters</span></span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a><span class="co">    ----------</span></span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a><span class="co">    n_samples : int, optional (default=100)</span></span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a><span class="co">        The total number of points generated. If odd, the inner circle will</span></span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a><span class="co">        have one point more than the outer circle.</span></span>
<span id="cb2-15"><a href="#cb2-15" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" tabindex="-1"></a><span class="co">    shuffle : bool, optional (default=True)</span></span>
<span id="cb2-17"><a href="#cb2-17" tabindex="-1"></a><span class="co">        Whether to shuffle the samples.</span></span>
<span id="cb2-18"><a href="#cb2-18" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" tabindex="-1"></a><span class="co">    noise : double or None (default=None)</span></span>
<span id="cb2-20"><a href="#cb2-20" tabindex="-1"></a><span class="co">        Standard deviation of Gaussian noise added to the data.</span></span>
<span id="cb2-21"><a href="#cb2-21" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" tabindex="-1"></a><span class="co">    random_state : int, RandomState instance or None (default)</span></span>
<span id="cb2-23"><a href="#cb2-23" tabindex="-1"></a><span class="co">        Determines random number generation for dataset shuffling and noise.</span></span>
<span id="cb2-24"><a href="#cb2-24" tabindex="-1"></a><span class="co">        Pass an int for reproducible output across multiple function calls.</span></span>
<span id="cb2-25"><a href="#cb2-25" tabindex="-1"></a><span class="co">        See :term:`Glossary &lt;random_state&gt;`.</span></span>
<span id="cb2-26"><a href="#cb2-26" tabindex="-1"></a></span>
<span id="cb2-27"><a href="#cb2-27" tabindex="-1"></a><span class="co">    factor : 0 &lt; double &lt; 1 (default=.8)</span></span>
<span id="cb2-28"><a href="#cb2-28" tabindex="-1"></a><span class="co">        Scale factor between inner and outer circle.</span></span>
<span id="cb2-29"><a href="#cb2-29" tabindex="-1"></a></span>
<span id="cb2-30"><a href="#cb2-30" tabindex="-1"></a><span class="co">    Returns</span></span>
<span id="cb2-31"><a href="#cb2-31" tabindex="-1"></a><span class="co">    -------</span></span>
<span id="cb2-32"><a href="#cb2-32" tabindex="-1"></a><span class="co">    X : array of shape [n_samples, 2]</span></span>
<span id="cb2-33"><a href="#cb2-33" tabindex="-1"></a><span class="co">        The generated samples.</span></span>
<span id="cb2-34"><a href="#cb2-34" tabindex="-1"></a></span>
<span id="cb2-35"><a href="#cb2-35" tabindex="-1"></a><span class="co">    y : array of shape [n_samples]</span></span>
<span id="cb2-36"><a href="#cb2-36" tabindex="-1"></a><span class="co">        The integer labels (0 or 1) for class membership of each sample.</span></span>
<span id="cb2-37"><a href="#cb2-37" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-38"><a href="#cb2-38" tabindex="-1"></a>    <span class="im">from</span> numpy <span class="im">import</span> pi, linspace, cos, sin, append, ones, zeros, hstack, vstack, intp</span>
<span id="cb2-39"><a href="#cb2-39" tabindex="-1"></a>    <span class="im">from</span> sklearn.utils <span class="im">import</span> check_random_state, shuffle</span>
<span id="cb2-40"><a href="#cb2-40" tabindex="-1"></a></span>
<span id="cb2-41"><a href="#cb2-41" tabindex="-1"></a>    <span class="cf">if</span> factor <span class="op">&gt;=</span> <span class="dv">1</span> <span class="kw">or</span> factor <span class="op">&lt;</span> <span class="dv">0</span>:</span>
<span id="cb2-42"><a href="#cb2-42" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"'factor' has to be between 0 and 1."</span>)</span>
<span id="cb2-43"><a href="#cb2-43" tabindex="-1"></a>    </span>
<span id="cb2-44"><a href="#cb2-44" tabindex="-1"></a>    <span class="co"># Determine the number of samples for each torus.</span></span>
<span id="cb2-45"><a href="#cb2-45" tabindex="-1"></a>    n_samples_out <span class="op">=</span> n_samples <span class="op">//</span> <span class="dv">2</span></span>
<span id="cb2-46"><a href="#cb2-46" tabindex="-1"></a>    n_samples_in <span class="op">=</span> n_samples <span class="op">-</span> n_samples_out</span>
<span id="cb2-47"><a href="#cb2-47" tabindex="-1"></a>    </span>
<span id="cb2-48"><a href="#cb2-48" tabindex="-1"></a>    <span class="co"># Define the radii and thickness of the outer and inner tori.</span></span>
<span id="cb2-49"><a href="#cb2-49" tabindex="-1"></a>    co, ao, ci, ai <span class="op">=</span> <span class="dv">3</span>, <span class="dv">1</span>, <span class="fl">3.6</span>, <span class="fl">0.2</span></span>
<span id="cb2-50"><a href="#cb2-50" tabindex="-1"></a>    </span>
<span id="cb2-51"><a href="#cb2-51" tabindex="-1"></a>    <span class="co"># Initialize the random number generator.</span></span>
<span id="cb2-52"><a href="#cb2-52" tabindex="-1"></a>    generator <span class="op">=</span> check_random_state(random_state)</span>
<span id="cb2-53"><a href="#cb2-53" tabindex="-1"></a>    </span>
<span id="cb2-54"><a href="#cb2-54" tabindex="-1"></a>    <span class="co"># to not have the first point = last point, we set endpoint=False.</span></span>
<span id="cb2-55"><a href="#cb2-55" tabindex="-1"></a>    linspace_out <span class="op">=</span> linspace(<span class="dv">0</span>, <span class="dv">2</span> <span class="op">*</span> pi, n_samples_out, endpoint<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-56"><a href="#cb2-56" tabindex="-1"></a>    linspace_in  <span class="op">=</span> linspace(<span class="dv">0</span>, <span class="dv">2</span> <span class="op">*</span> pi, n_samples_in,  endpoint<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-57"><a href="#cb2-57" tabindex="-1"></a>    </span>
<span id="cb2-58"><a href="#cb2-58" tabindex="-1"></a>    <span class="co"># Compute 3D coordinates for the outer torus.</span></span>
<span id="cb2-59"><a href="#cb2-59" tabindex="-1"></a>    outer_circ_x <span class="op">=</span> (co<span class="op">+</span>ao<span class="op">*</span>cos(linspace_out)) <span class="op">*</span> cos(linspace_out<span class="op">*</span><span class="fl">61.1</span>)</span>
<span id="cb2-60"><a href="#cb2-60" tabindex="-1"></a>    outer_circ_y <span class="op">=</span> (co<span class="op">+</span>ao<span class="op">*</span>cos(linspace_out)) <span class="op">*</span> sin(linspace_out<span class="op">*</span><span class="fl">61.1</span>)</span>
<span id="cb2-61"><a href="#cb2-61" tabindex="-1"></a>    outer_circ_z <span class="op">=</span>    ao<span class="op">*</span>sin(linspace_out)</span>
<span id="cb2-62"><a href="#cb2-62" tabindex="-1"></a>    </span>
<span id="cb2-63"><a href="#cb2-63" tabindex="-1"></a>    <span class="co"># Compute 3D coordinates for the inner torus (scaled by `factor`).</span></span>
<span id="cb2-64"><a href="#cb2-64" tabindex="-1"></a>    inner_circ_x <span class="op">=</span> (ci<span class="op">+</span>ai<span class="op">*</span>cos(linspace_in)) <span class="op">*</span> cos(linspace_in<span class="op">*</span><span class="fl">61.1</span>)<span class="op">*</span> factor</span>
<span id="cb2-65"><a href="#cb2-65" tabindex="-1"></a>    inner_circ_y <span class="op">=</span> (ci<span class="op">+</span>ai<span class="op">*</span>cos(linspace_in)) <span class="op">*</span> sin(linspace_in<span class="op">*</span><span class="fl">61.1</span>) <span class="op">*</span> factor</span>
<span id="cb2-66"><a href="#cb2-66" tabindex="-1"></a>    inner_circ_z <span class="op">=</span>    ai<span class="op">*</span>sin(linspace_in) <span class="op">*</span> factor</span>
<span id="cb2-67"><a href="#cb2-67" tabindex="-1"></a></span>
<span id="cb2-68"><a href="#cb2-68" tabindex="-1"></a>    <span class="co"># Stack the coordinates into a single array (X: [n_samples, 3]).</span></span>
<span id="cb2-69"><a href="#cb2-69" tabindex="-1"></a>    X <span class="op">=</span> vstack([append(outer_circ_x, inner_circ_x),</span>
<span id="cb2-70"><a href="#cb2-70" tabindex="-1"></a>                append(outer_circ_y, inner_circ_y),</span>
<span id="cb2-71"><a href="#cb2-71" tabindex="-1"></a>                append(outer_circ_z, inner_circ_z)]).T</span>
<span id="cb2-72"><a href="#cb2-72" tabindex="-1"></a></span>
<span id="cb2-73"><a href="#cb2-73" tabindex="-1"></a>    <span class="co"># Generate class labels: 0 for outer torus, 1 for inner torus.</span></span>
<span id="cb2-74"><a href="#cb2-74" tabindex="-1"></a>    y <span class="op">=</span> hstack([zeros(n_samples_out, dtype<span class="op">=</span>intp),</span>
<span id="cb2-75"><a href="#cb2-75" tabindex="-1"></a>                   ones(n_samples_in, dtype<span class="op">=</span>intp)])</span>
<span id="cb2-76"><a href="#cb2-76" tabindex="-1"></a></span>
<span id="cb2-77"><a href="#cb2-77" tabindex="-1"></a>    <span class="co"># If specified, shuffle the dataset.</span></span>
<span id="cb2-78"><a href="#cb2-78" tabindex="-1"></a>    <span class="cf">if</span> shuffle:</span>
<span id="cb2-79"><a href="#cb2-79" tabindex="-1"></a>        X, y <span class="op">=</span> shuffle(X, y, random_state<span class="op">=</span>generator)</span>
<span id="cb2-80"><a href="#cb2-80" tabindex="-1"></a></span>
<span id="cb2-81"><a href="#cb2-81" tabindex="-1"></a>    <span class="co"># Add Gaussian noise if specified.</span></span>
<span id="cb2-82"><a href="#cb2-82" tabindex="-1"></a>    <span class="cf">if</span> noise <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb2-83"><a href="#cb2-83" tabindex="-1"></a>        X <span class="op">+=</span> generator.normal(scale<span class="op">=</span>noise, size<span class="op">=</span>X.shape)</span>
<span id="cb2-84"><a href="#cb2-84" tabindex="-1"></a></span>
<span id="cb2-85"><a href="#cb2-85" tabindex="-1"></a>    <span class="cf">return</span> X, y</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="co"># Set a fixed random seed for reproducibility</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>RANDOM_STATE  <span class="op">=</span> <span class="dv">12345</span></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>seed(RANDOM_STATE)</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a><span class="co"># Generate a 3D torus dataset with 2000 samples</span></span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>X, y <span class="op">=</span> make_torus_3D(n_samples<span class="op">=</span><span class="dv">2000</span>, factor<span class="op">=</span><span class="fl">.9</span>, noise<span class="op">=</span><span class="fl">.001</span>, random_state<span class="op">=</span>RANDOM_STATE)</span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a><span class="co"># Select feature indices for 3D visualization</span></span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a>feature_1, feature_2, feature_3 <span class="op">=</span> <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span></span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a><span class="co"># Get the minimum and maximum values of X for scaling (not used later)</span></span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a>ft_min, ft_max <span class="op">=</span> X.<span class="bu">min</span>(), X.<span class="bu">max</span>()</span>
<span id="cb3-13"><a href="#cb3-13" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" tabindex="-1"></a><span class="co"># Create a new figure and axis for 3D plotting</span></span>
<span id="cb3-15"><a href="#cb3-15" tabindex="-1"></a>fig, ax <span class="op">=</span> subplots(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">9</span>))</span>
<span id="cb3-16"><a href="#cb3-16" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" tabindex="-1"></a><span class="co"># Set up 3D axes for visualization</span></span>
<span id="cb3-18"><a href="#cb3-18" tabindex="-1"></a>ax <span class="op">=</span> axes(projection<span class="op">=</span><span class="st">"3d"</span>)</span>
<span id="cb3-19"><a href="#cb3-19" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" tabindex="-1"></a><span class="co"># Create a 3D scatter plot</span></span>
<span id="cb3-21"><a href="#cb3-21" tabindex="-1"></a>im <span class="op">=</span> ax.scatter3D(X[:, feature_1], X[:, feature_2], X[:, feature_3], marker<span class="op">=</span><span class="st">'o'</span>, s<span class="op">=</span><span class="dv">20</span>, c<span class="op">=</span>y, cmap<span class="op">=</span><span class="st">'bwr'</span>)<span class="op">;</span></span>
<span id="cb3-22"><a href="#cb3-22" tabindex="-1"></a></span>
<span id="cb3-23"><a href="#cb3-23" tabindex="-1"></a><span class="co"># Set axis labels</span></span>
<span id="cb3-24"><a href="#cb3-24" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Feature 1'</span>)</span>
<span id="cb3-25"><a href="#cb3-25" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Feature 2'</span>)</span>
<span id="cb3-26"><a href="#cb3-26" tabindex="-1"></a>ax.set_zlabel(<span class="st">'Feature 3'</span>)</span>
<span id="cb3-27"><a href="#cb3-27" tabindex="-1"></a></span>
<span id="cb3-28"><a href="#cb3-28" tabindex="-1"></a><span class="co"># Angles to pick the perspective</span></span>
<span id="cb3-29"><a href="#cb3-29" tabindex="-1"></a>ax.view_init(<span class="dv">30</span>, <span class="dv">50</span>)<span class="op">;</span></span>
<span id="cb3-30"><a href="#cb3-30" tabindex="-1"></a></span>
<span id="cb3-31"><a href="#cb3-31" tabindex="-1"></a>show()</span></code></pre>
</div>
<figure><img src="../fig/02-improvement-rendered-unnamed-chunk-3-1.png" width="1152" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p style="text-align: justify;">
The challenge here is that the only way to separate the data of the two
labels from each other is to find a separating border that lies between
the blue and the red doughnut (mathematically: torus) and itself is a
torus, i.e. a complex topology. Similarly, one can test to separate one
class of data that lie on the surface of a sphere and then have data on
another sphere embedded within it. Typically, it is unknown what type of
high-dimensional topologies is present in biological data. As such it is
not clear at the outset which classification strategy will work best.
Let us start with a simpler example.
</p>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="training-a-variety-of-machine-learning-models">Training a variety of machine learning models<a class="anchor" aria-label="anchor" href="#training-a-variety-of-machine-learning-models"></a></h2>
<hr class="half-width"><p><code>scikit-learn</code> provides the means to generate practice
datasets with specific qualities. In this section, we will use the
<code>make_circles</code> function - a function that generates a toy
dataset which consists of two concentric circles (see the function’s <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html" class="external-link">documentation.</a>)
for more information:</p>
<div class="section level3">
<h3 id="circular-test-data"><strong>Circular Test Data</strong><a class="anchor" aria-label="anchor" href="#circular-test-data"></a></h3>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="co"># Set random seed for reproducibility.</span></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>RANDOM_STATE  <span class="op">=</span> <span class="dv">1234</span></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>seed(RANDOM_STATE)</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a><span class="co"># Generates synthetic circular data: 'n_samples' sets the total number of datapoints to 500.</span></span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a><span class="co"># 'factor' controls the inner circle to be 30% of the radius of the outer circle.</span></span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a><span class="co"># 'noise' adds a very small amount of Gaussian noise.</span></span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a>X, y <span class="op">=</span> make_circles(n_samples<span class="op">=</span><span class="dv">500</span>, factor<span class="op">=</span><span class="fl">0.3</span>, noise<span class="op">=</span><span class="fl">.05</span>, random_state<span class="op">=</span>RANDOM_STATE)</span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a><span class="co"># This obtains the overall maximum and minimum feature values:</span></span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a>feature_1, feature_2 <span class="op">=</span> <span class="dv">0</span>, <span class="dv">1</span></span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a>ft_min, ft_max <span class="op">=</span> X.<span class="bu">min</span>(), X.<span class="bu">max</span>()</span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Shape of X:'</span>, X.shape)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Shape of X: (500, 2)</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="co"># Plotting:</span></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>fig, ax <span class="op">=</span> subplots(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>), nrows<span class="op">=</span><span class="dv">1</span>, ncols<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>ax[<span class="dv">0</span>].scatter(X[:, feature_1], X[:, feature_2], c<span class="op">=</span>y, s<span class="op">=</span><span class="dv">4</span>, cmap<span class="op">=</span><span class="st">'bwr'</span>)<span class="op">;</span></span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>ax[<span class="dv">1</span>].hist(X)<span class="op">;</span></span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>show()</span></code></pre>
</div>
<figure><img src="../fig/02-improvement-rendered-unnamed-chunk-4-3.png" width="960" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p style="text-align: justify;">
The function yields only two features. The reason is that with two
features we can visualise the complete state space in a two-dimensional
scatter plot. The data of both labels are organised along a ring. There
is a certain amount of randomness added to create data distributed
normally around the ring.
</p>
<p style="text-align: justify;">
The tricky thing about such a data distribution is that in a standard
view of the data, the histogram, the clear state space organisation is
not visible. There are e.g. no two distinct mean values of the
distributions. Also, while the two features are clearly dependent on
each other (as seen in the scatter plot), it is not possible to regress
one with the other by means of fits of the type y = f(x).
</p>
<p>We will now use different classes of machine learning models to fit
to these labelled data.</p>
</div>
<div class="section level3">
<h3 id="classification-algorithms"><strong>Classification Algorithms</strong><a class="anchor" aria-label="anchor" href="#classification-algorithms"></a></h3>
<p>Different classification algorithms approach problems differently.
Let us name the algorithms in <code>scikit-learn</code>.</p>
<p><code>scikit-learn</code> provides the following algorithms for
classification problems. Each listed classifier is hyperlinked to its
relevant scikit-learn Documentation page.</p>
<ul><li>Ensemble: Averaging:
<ul><li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html" class="external-link">Random
Forest</a></li>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html" class="external-link">Extra
Tree</a></li>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html" class="external-link">Isolation
Forest</a></li>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html" class="external-link">Bagging</a></li>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html" class="external-link">Voting</a></li>
</ul></li>
<li>Boosting:
<ul><li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html" class="external-link">Gradient
Boosting</a></li>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html" class="external-link">AdaBoost</a></li>
</ul></li>
<li>Decision Trees:
<ul><li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html" class="external-link">Decision
Tree</a></li>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.ExtraTreeClassifier.html" class="external-link">Extra
Tree</a></li>
</ul></li>
<li>Nearest Neighbour:
<ul><li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html" class="external-link">K
Nearest Neighbour</a></li>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsClassifier.html" class="external-link">Radius
Neighbours</a></li>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestCentroid.html" class="external-link">Nearest
Centroid</a></li>
</ul></li>
<li>Support Vector Machine:
<ul><li>with non-linear kernel:
<ul><li><a href="https://scikit-learn.org/stable/modules/svm.html#svm-kernels" class="external-link">Radial
Basis Function (RBF)</a></li>
<li><a href="https://scikit-learn.org/stable/modules/svm.html#svm-kernels" class="external-link">Polynomial</a></li>
<li><a href="https://scikit-learn.org/stable/modules/svm.html#svm-kernels" class="external-link">Sigmoid</a></li>
</ul></li>
<li>with linear kernel:
<ul><li><a href="https://scikit-learn.org/stable/modules/svm.html#svm-kernels" class="external-link">Linear
kernel</a></li>
</ul></li>
<li>parametrised with non-linear kernel:
<ul><li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html" class="external-link">Nu-Support
Vector Classification</a></li>
</ul></li>
</ul></li>
<li>Neural Networks:
<ul><li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html" class="external-link">Multi-layer
Perceptron</a></li>
<li>Gaussian:
<ul><li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessClassifier.html" class="external-link">Gaussian
Process</a></li>
</ul></li>
<li>Linear Models:
<ul><li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" class="external-link">Logistic
Regression</a></li>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.PassiveAggressiveClassifier.html" class="external-link">Passive
Aggressive</a></li>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html" class="external-link">Ridge</a></li>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html" class="external-link">Linear
classifiers with Stochastic Gradient Descent</a></li>
</ul></li>
</ul></li>
<li>Baysian:
<ul><li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html" class="external-link">Bernoulli</a></li>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html" class="external-link">Multinomial</a></li>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.ComplementNB.html" class="external-link">Complement</a></li>
</ul></li>
</ul><p>Some of these algorithms require a more in-depth understanding of how
they work. To that end, we only review the performance of those that are
easier to implement and adjust.</p>
<strong>AdaBoost</strong>
<p style="text-align: justify;">
The AdaBoost algorithm is special in that it does not work on its own;
instead, it complements another ensemble algorithm (e.g. Random Forest)
and <em>boosts</em> its performance by weighing the training data
through a boosting algorithm. Note that boosting the performance does
not necessarily translate into a better fit. This is because boosting
algorithms are generally robust against over-fitting, meaning that they
always try to produce generalisable models.
</p>
<strong>Seeding</strong>
<p style="text-align: justify;">
Most machine learning algorithms rely on random number generation to
produce results. Therefore, one simple, but important adjustment is to
<code>seed</code> the number generator, and thereby making our
comparisons more consistent; i.e. ensure that all models use the same
set of random numbers. Almost all scikit-learn models take an argument
called <code>random_state</code>, which takes an integer number to seed
the random number generator.
</p>
</div>
<div class="section level3">
<h3 id="training-and-testing"><strong>Training and Testing</strong><a class="anchor" aria-label="anchor" href="#training-and-testing"></a></h3>
<p>Here is code to import a number of classifiers from scikit-learn, fit
them to the training data and predict the (complete) state space. The
result is plotted below.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC, LinearSVC</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a><span class="im">from</span> sklearn.neural_network <span class="im">import</span> MLPClassifier</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsClassifier</span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a>classifiers <span class="op">=</span> {</span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a>    <span class="st">'Random Forest'</span>: RandomForestClassifier(random_state<span class="op">=</span>RANDOM_STATE),</span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a>    <span class="st">'AdaBoost (Random Forest)'</span>: AdaBoostClassifier(RandomForestClassifier(random_state<span class="op">=</span>RANDOM_STATE)),</span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a>    <span class="st">'Extra Trees'</span>: ExtraTreesClassifier(random_state<span class="op">=</span>RANDOM_STATE),</span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a>    <span class="st">'AdaBoost (Extra Tree)'</span>: AdaBoostClassifier(ExtraTreesClassifier(random_state<span class="op">=</span>RANDOM_STATE)),</span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a>    <span class="st">'Decision Tree'</span>: DecisionTreeClassifier(random_state<span class="op">=</span>RANDOM_STATE),</span>
<span id="cb7-13"><a href="#cb7-13" tabindex="-1"></a>    <span class="st">'SVC (RBF)'</span>: SVC(random_state<span class="op">=</span>RANDOM_STATE),</span>
<span id="cb7-14"><a href="#cb7-14" tabindex="-1"></a>    <span class="st">'SVC (Linear)'</span>: LinearSVC(random_state<span class="op">=</span>RANDOM_STATE),</span>
<span id="cb7-15"><a href="#cb7-15" tabindex="-1"></a>    <span class="st">'Multi-layer Perceptron'</span>: MLPClassifier(max_iter<span class="op">=</span><span class="dv">5000</span>, random_state<span class="op">=</span>RANDOM_STATE)</span>
<span id="cb7-16"><a href="#cb7-16" tabindex="-1"></a>}</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>ft_min, ft_max <span class="op">=</span> <span class="op">-</span><span class="fl">1.5</span>, <span class="fl">1.5</span></span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a><span class="co"># Constructing (2 grids x 300 rows x 300 cols):</span></span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>grid_1, grid_2 <span class="op">=</span> mgrid[ft_min:ft_max:<span class="fl">.01</span>, ft_min:ft_max:<span class="fl">.01</span>]</span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a><span class="co"># We need only the shape for one of the grids (i.e. 300 x  300):</span></span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a>grid_shape <span class="op">=</span> grid_1.shape</span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a><span class="co"># state space grid for testing</span></span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a>new_obs <span class="op">=</span> c_[grid_1.ravel(), grid_2.ravel()]</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>contour_levels <span class="op">=</span> linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">6</span>)</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>fig, all_axes <span class="op">=</span> subplots(figsize<span class="op">=</span>[<span class="dv">15</span>, <span class="dv">5</span>], ncols<span class="op">=</span><span class="dv">4</span>, nrows<span class="op">=</span><span class="dv">2</span>, sharey<span class="op">=</span><span class="va">True</span>, sharex<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a><span class="cf">for</span> ax, (name, clf) <span class="kw">in</span> <span class="bu">zip</span>(all_axes.ravel(), classifiers.items()):</span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a>    clf.fit(X, y)</span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a>    y_pred <span class="op">=</span> clf.predict(new_obs)</span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a>    y_pred_grid <span class="op">=</span> y_pred.reshape(grid_shape)</span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" tabindex="-1"></a>    ax.scatter(X[:, feature_1], X[:, feature_2], c<span class="op">=</span>y, s<span class="op">=</span><span class="dv">1</span>, cmap<span class="op">=</span><span class="st">'bwr_r'</span>)</span>
<span id="cb9-11"><a href="#cb9-11" tabindex="-1"></a>    ax.contourf(grid_1, grid_2, y_pred_grid, cmap<span class="op">=</span><span class="st">'gray_r'</span>, alpha<span class="op">=</span><span class="fl">.2</span>, levels<span class="op">=</span>contour_levels)</span>
<span id="cb9-12"><a href="#cb9-12" tabindex="-1"></a>    ax.set_ylim(ft_min, ft_max)</span>
<span id="cb9-13"><a href="#cb9-13" tabindex="-1"></a>    ax.set_xlim(ft_min, ft_max)</span>
<span id="cb9-14"><a href="#cb9-14" tabindex="-1"></a>    ax.set_yticks([<span class="op">-</span><span class="fl">1.5</span>, <span class="dv">0</span>, <span class="fl">1.5</span>])</span>
<span id="cb9-15"><a href="#cb9-15" tabindex="-1"></a>    ax.set_xticks([<span class="op">-</span><span class="fl">1.5</span>, <span class="dv">0</span>, <span class="fl">1.5</span>])</span>
<span id="cb9-16"><a href="#cb9-16" tabindex="-1"></a>    ax.set_title(name, fontsize<span class="op">=</span><span class="dv">10</span>)<span class="op">;</span></span>
<span id="cb9-17"><a href="#cb9-17" tabindex="-1"></a></span>
<span id="cb9-18"><a href="#cb9-18" tabindex="-1"></a>show()</span></code></pre>
</div>
<figure><img src="../fig/02-improvement-rendered-unnamed-chunk-7-5.png" width="1440" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p style="text-align: justify;">
Seven of the eight classifiers were able to separate the inner data set
from the outer data set successfully. The main difference is that some
algorithms ended up with a more rectangular shape of the boundary
whereas the others found a more circular form which reflects the
original data distribution more closely. One classifier simply fails:
the support vector classifier (SVC) with linear basis functions: it
tries to fit a straight line to separate the classes which in this case
is impossible.
</p>
</div>
<div class="section level3">
<h3 id="the-train-test-split"><strong>The Train-Test Split</strong><a class="anchor" aria-label="anchor" href="#the-train-test-split"></a></h3>
<p style="text-align: justify;">
We will now modify our workflow to avoid the need to create separate
testing data (the typical situation when dealing with recorded data).
For this we start with a data set of n labelled samples. Of these n
samples, a certain percentage is used for training (using the provided
labels) and the rest for testing (withholding the labels). The testing
data then do not need to be prepared separately.
</p>
<p style="text-align: justify;">
The function we use is <code>train_test_split</code> from SciKit Learn.
A nice feature of this function is that it tries to preserve the ratio
of labels in the split. E.g. if the data contain 70% of
<code>True</code> and 30 % of <code>False</code> labels, the algorithm
tries to preserve this ratio in the split as good as possible: around
70% of the training data and of the testing data will have the
<code>True</code> label.
</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>X, y <span class="op">=</span> make_circles(n_samples<span class="op">=</span><span class="dv">1000</span>, factor<span class="op">=</span><span class="fl">0.3</span>, noise<span class="op">=</span><span class="fl">.05</span>, random_state<span class="op">=</span>RANDOM_STATE)</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">.3</span>, random_state<span class="op">=</span>RANDOM_STATE, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a><span class="bu">print</span>(X_train.shape, X_test.shape)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>(700, 2) (300, 2)</code></pre>
</div>
<p style="text-align: justify;">
Here is an illustration of the two sets of data. The splitting into
testing and training data is done randomly. Picking test data randomly
is particularly important for real data as it helps to reduce potential
bias in the recording order.
</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>fig, ax <span class="op">=</span> subplots(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">6</span>), ncols<span class="op">=</span><span class="dv">2</span>, nrows<span class="op">=</span><span class="dv">2</span>, sharex<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">0</span>].scatter(X_train[:, feature_1], X_train[:, feature_2], c<span class="op">=</span>y_train, s<span class="op">=</span><span class="dv">4</span>, cmap<span class="op">=</span><span class="st">'bwr'</span>)</span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">1</span>].scatter(X_test[:, feature_1], X_test[:, feature_2], c<span class="op">=</span>y_test, s<span class="op">=</span><span class="dv">4</span>, cmap<span class="op">=</span><span class="st">'bwr'</span>)</span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">0</span>].hist(X_train)</span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">1</span>].hist(X_test)</span>
<span id="cb12-8"><a href="#cb12-8" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">0</span>].set_title(<span class="st">'Training data'</span>)</span>
<span id="cb12-10"><a href="#cb12-10" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">1</span>].set_title(<span class="st">'Test data'</span>)</span>
<span id="cb12-11"><a href="#cb12-11" tabindex="-1"></a></span>
<span id="cb12-12"><a href="#cb12-12" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">0</span>].set_ylim(ft_min, ft_max)</span>
<span id="cb12-13"><a href="#cb12-13" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">1</span>].set_ylim(ft_min, ft_max)</span>
<span id="cb12-14"><a href="#cb12-14" tabindex="-1"></a></span>
<span id="cb12-15"><a href="#cb12-15" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">0</span>].set_ylim(<span class="dv">0</span>, <span class="dv">100</span>)</span>
<span id="cb12-16"><a href="#cb12-16" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">1</span>].set_ylim(<span class="dv">0</span>, <span class="dv">100</span>)<span class="op">;</span></span>
<span id="cb12-17"><a href="#cb12-17" tabindex="-1"></a></span>
<span id="cb12-18"><a href="#cb12-18" tabindex="-1"></a>show()</span></code></pre>
</div>
<figure><img src="../fig/02-improvement-rendered-unnamed-chunk-9-7.png" width="672" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure>
Now we can repeat the training with this split dataset using eight types
of models as above.
<p style="text-align: justify;">
To compare the model performances, we use <strong>scoring</strong>: the
method <code>.score</code> takes as input arguments the testing samples
and their true labels. It then uses the model predictions to calculate
the fraction of labels in the testing data that were predicted
correctly.
</p>
<p style="text-align: justify;">
There are different techniques to evaluate the performance, but the
<code>.score</code> method provides a quick, simple, and handy way to
assess a model. As far as classification algorithms in scikit-learn are
concerned, the method usually produces the <strong>mean
accuracy</strong>, which is between 0 and 1; and the higher the score,
the better the fit.
</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a>fig, all_axes <span class="op">=</span> subplots(figsize<span class="op">=</span>[<span class="dv">15</span>, <span class="dv">5</span>], ncols<span class="op">=</span><span class="dv">4</span>, nrows<span class="op">=</span><span class="dv">2</span>, sharey<span class="op">=</span><span class="va">True</span>, sharex<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a><span class="cf">for</span> ax, (name, clf) <span class="kw">in</span> <span class="bu">zip</span>(all_axes.ravel(), classifiers.items()):</span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a>    <span class="co"># Training the model using training data:</span></span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a>    clf.fit(X_train, y_train)</span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" tabindex="-1"></a>    y_pred <span class="op">=</span> clf.predict(new_obs)</span>
<span id="cb13-8"><a href="#cb13-8" tabindex="-1"></a>    y_pred_grid <span class="op">=</span> y_pred.reshape(grid_shape)</span>
<span id="cb13-9"><a href="#cb13-9" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" tabindex="-1"></a>    <span class="co"># Evaluating the score using test data:</span></span>
<span id="cb13-11"><a href="#cb13-11" tabindex="-1"></a>    score <span class="op">=</span> clf.score(X_test, y_test)</span>
<span id="cb13-12"><a href="#cb13-12" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" tabindex="-1"></a>    <span class="co"># Scattering the test data only:</span></span>
<span id="cb13-14"><a href="#cb13-14" tabindex="-1"></a>    ax.scatter(X_test[:, feature_1], X_test[:, feature_2], c<span class="op">=</span>y_test, s<span class="op">=</span><span class="dv">4</span>, cmap<span class="op">=</span><span class="st">'bwr'</span>, marker<span class="op">=</span><span class="st">'.'</span>)</span>
<span id="cb13-15"><a href="#cb13-15" tabindex="-1"></a></span>
<span id="cb13-16"><a href="#cb13-16" tabindex="-1"></a>    ax.contourf(grid_1, grid_2, y_pred_grid, cmap<span class="op">=</span><span class="st">'gray_r'</span>, alpha<span class="op">=</span><span class="fl">.2</span>, levels<span class="op">=</span>contour_levels)</span>
<span id="cb13-17"><a href="#cb13-17" tabindex="-1"></a><span class="co">#    ax.contourf(grid[0], grid[1], y_pred_grid, cmap='gray_r', alpha=.2, levels=contour_levels)</span></span>
<span id="cb13-18"><a href="#cb13-18" tabindex="-1"></a></span>
<span id="cb13-19"><a href="#cb13-19" tabindex="-1"></a>    ax.set_ylim(ft_min, ft_max)</span>
<span id="cb13-20"><a href="#cb13-20" tabindex="-1"></a>    ax.set_xlim(ft_min, ft_max)</span>
<span id="cb13-21"><a href="#cb13-21" tabindex="-1"></a>    ax.set_yticks([<span class="op">-</span><span class="fl">1.5</span>, <span class="dv">0</span>, <span class="fl">1.5</span>])</span>
<span id="cb13-22"><a href="#cb13-22" tabindex="-1"></a>    ax.set_xticks([<span class="op">-</span><span class="fl">1.5</span>, <span class="dv">0</span>, <span class="fl">1.5</span>])</span>
<span id="cb13-23"><a href="#cb13-23" tabindex="-1"></a></span>
<span id="cb13-24"><a href="#cb13-24" tabindex="-1"></a>    label <span class="op">=</span> <span class="st">'</span><span class="sc">{}</span><span class="st"> - Score: </span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(name, score)</span>
<span id="cb13-25"><a href="#cb13-25" tabindex="-1"></a>    ax.set_title(label , fontsize<span class="op">=</span><span class="dv">10</span>)<span class="op">;</span></span>
<span id="cb13-26"><a href="#cb13-26" tabindex="-1"></a></span>
<span id="cb13-27"><a href="#cb13-27" tabindex="-1"></a>show()</span></code></pre>
</div>
<figure><img src="../fig/02-improvement-rendered-unnamed-chunk-10-9.png" width="1440" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p style="text-align: justify;">
Here, we only plotted the test data, those that were classified based on
the trained model. The gray area shows the result of the classification:
within the gray area the prediction is 1 (the red samples) and outside
it is 0 (the blue samples). The result is that testing data are
classified correctly in all but one of the classifiers, so their
performance is 1, or 100 %. This is excellent because it demonstrates
that most classifiers are able to deal with embedded topologies.
</p>
<p>Let us now repeat the procedure with a higher level of noise to make
the task more complicated.</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>X, y <span class="op">=</span> make_circles(n_samples<span class="op">=</span><span class="dv">1000</span>, factor<span class="op">=</span><span class="fl">.5</span>, noise<span class="op">=</span><span class="fl">.3</span>, random_state<span class="op">=</span>RANDOM_STATE)</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">.3</span>, random_state<span class="op">=</span>RANDOM_STATE, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a>fig, ax <span class="op">=</span> subplots(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">6</span>), ncols<span class="op">=</span><span class="dv">2</span>, nrows<span class="op">=</span><span class="dv">2</span>, sharex<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">0</span>].scatter(X_train[:, feature_1], X_train[:, feature_2], c<span class="op">=</span>y_train, s<span class="op">=</span><span class="dv">4</span>, cmap<span class="op">=</span><span class="st">'bwr'</span>)</span>
<span id="cb14-8"><a href="#cb14-8" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">1</span>].scatter(X_test[:, feature_1], X_test[:, feature_2], c<span class="op">=</span>y_test, s<span class="op">=</span><span class="dv">4</span>, cmap<span class="op">=</span><span class="st">'bwr'</span>)</span>
<span id="cb14-9"><a href="#cb14-9" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">0</span>].hist(X_train)</span>
<span id="cb14-12"><a href="#cb14-12" tabindex="-1"></a></span>
<span id="cb14-13"><a href="#cb14-13" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">1</span>].hist(X_test)</span>
<span id="cb14-14"><a href="#cb14-14" tabindex="-1"></a></span>
<span id="cb14-15"><a href="#cb14-15" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">0</span>].set_title(<span class="st">'Training data'</span>)</span>
<span id="cb14-16"><a href="#cb14-16" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">1</span>].set_title(<span class="st">'Test data'</span>)</span>
<span id="cb14-17"><a href="#cb14-17" tabindex="-1"></a></span>
<span id="cb14-18"><a href="#cb14-18" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">0</span>].set_ylim(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb14-19"><a href="#cb14-19" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">1</span>].set_ylim(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb14-20"><a href="#cb14-20" tabindex="-1"></a></span>
<span id="cb14-21"><a href="#cb14-21" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">0</span>].set_ylim(<span class="dv">0</span>, <span class="dv">200</span>)</span>
<span id="cb14-22"><a href="#cb14-22" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">1</span>].set_ylim(<span class="dv">0</span>, <span class="dv">200</span>)<span class="op">;</span></span>
<span id="cb14-23"><a href="#cb14-23" tabindex="-1"></a></span>
<span id="cb14-24"><a href="#cb14-24" tabindex="-1"></a>show()</span></code></pre>
</div>
<figure><img src="../fig/02-improvement-rendered-unnamed-chunk-11-11.png" width="672" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>fig, all_axes <span class="op">=</span> subplots(figsize<span class="op">=</span>[<span class="dv">15</span>, <span class="dv">5</span>], ncols<span class="op">=</span><span class="dv">4</span>, nrows<span class="op">=</span><span class="dv">2</span>, sharey<span class="op">=</span><span class="va">True</span>, sharex<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a><span class="cf">for</span> ax, (name, clf) <span class="kw">in</span> <span class="bu">zip</span>(all_axes.ravel(), classifiers.items()):</span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a>    <span class="co"># Training the model using training data:</span></span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a>    clf.fit(X_train, y_train)</span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a>    y_pred <span class="op">=</span> clf.predict(new_obs)</span>
<span id="cb15-8"><a href="#cb15-8" tabindex="-1"></a>    y_pred_grid <span class="op">=</span> y_pred.reshape(grid_shape)</span>
<span id="cb15-9"><a href="#cb15-9" tabindex="-1"></a></span>
<span id="cb15-10"><a href="#cb15-10" tabindex="-1"></a>    <span class="co"># Evaluating the score using test data:</span></span>
<span id="cb15-11"><a href="#cb15-11" tabindex="-1"></a>    score <span class="op">=</span> clf.score(X_test, y_test)</span>
<span id="cb15-12"><a href="#cb15-12" tabindex="-1"></a></span>
<span id="cb15-13"><a href="#cb15-13" tabindex="-1"></a>    <span class="co"># Scattering the test data only:</span></span>
<span id="cb15-14"><a href="#cb15-14" tabindex="-1"></a>    ax.scatter(X_test[:, feature_1], X_test[:, feature_2], c<span class="op">=</span>y_test, s<span class="op">=</span><span class="dv">4</span>, cmap<span class="op">=</span><span class="st">'bwr'</span>, marker<span class="op">=</span><span class="st">'.'</span>)</span>
<span id="cb15-15"><a href="#cb15-15" tabindex="-1"></a></span>
<span id="cb15-16"><a href="#cb15-16" tabindex="-1"></a>    ax.contourf(grid_1, grid_2, y_pred_grid, cmap<span class="op">=</span><span class="st">'gray_r'</span>, alpha<span class="op">=</span><span class="fl">.2</span>, levels<span class="op">=</span>contour_levels)</span>
<span id="cb15-17"><a href="#cb15-17" tabindex="-1"></a></span>
<span id="cb15-18"><a href="#cb15-18" tabindex="-1"></a>    ax.set_ylim(ft_min, ft_max)</span>
<span id="cb15-19"><a href="#cb15-19" tabindex="-1"></a>    ax.set_xlim(ft_min, ft_max)</span>
<span id="cb15-20"><a href="#cb15-20" tabindex="-1"></a>    ax.set_yticks([<span class="op">-</span><span class="fl">1.5</span>, <span class="dv">0</span>, <span class="fl">1.5</span>])</span>
<span id="cb15-21"><a href="#cb15-21" tabindex="-1"></a>    ax.set_xticks([<span class="op">-</span><span class="fl">1.5</span>, <span class="dv">0</span>, <span class="fl">1.5</span>])</span>
<span id="cb15-22"><a href="#cb15-22" tabindex="-1"></a></span>
<span id="cb15-23"><a href="#cb15-23" tabindex="-1"></a>    label <span class="op">=</span> <span class="st">'</span><span class="sc">{}</span><span class="st"> - Score: </span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(name, score)</span>
<span id="cb15-24"><a href="#cb15-24" tabindex="-1"></a>    ax.set_title(label , fontsize<span class="op">=</span><span class="dv">10</span>)<span class="op">;</span></span>
<span id="cb15-25"><a href="#cb15-25" tabindex="-1"></a></span>
<span id="cb15-26"><a href="#cb15-26" tabindex="-1"></a>show()</span></code></pre>
</div>
<figure><img src="../fig/02-improvement-rendered-unnamed-chunk-12-13.png" width="1440" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p style="text-align: justify;">
Now the data are mixed in the plane and there is no simple way to
separate the two classes. We can see in the plots how the algorithms try
to cope with their different strategies. One thing that is immediately
obvious is that the fitting patterns are different. Particularly, we can
see the fragmented outcome of the <em>decision tree</em> classifier and
the smooth elliptic area found by the <em>support vector classifier
(SVC)</em> with radial basis functions (RBF) and the neural network
(MLP). On a closer look, you may also notice that with ensemble methods
in the upper row, the patterns are somewhat disorganised. This is due to
the way ensemble methods work: they sample the data randomly and then
class them into different categories based on their labels.
</p>
<p style="text-align: justify;">
If the prediction was made by chance (throwing a dice), one would expect
a 50 % score. Thus, the example also shows that the performance depends
on the type of problem and that this testing helps to find an optimal
classifier.
</p>
<div id="never-expose-the-test-data" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="never-expose-the-test-data" class="callout-inner">
<h3 class="callout-title"><strong>Never expose the test
data</strong></h3>
<div class="callout-content">
<p style="text-align: justify;">
Testing a model on data that is used in training is a methodological
mistake. It is therefore vital that the test data is <strong>never,
ever</strong> used for training a model at any stage. This is one of the
most fundamental principles of machine learning, and its importance
cannot be exaggerated. There are numerous examples of people making this
mistake one way or another, especially where multiple classification
algorithms are used to address a problem.
</p>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="the-stratified-shuffle-split">The Stratified Shuffle Split<a class="anchor" aria-label="anchor" href="#the-stratified-shuffle-split"></a></h2>
<hr class="half-width"><p style="text-align: justify;">
One potential bias arises when we try to improve the performance of our
models through the change of the so-called
<strong>hyperparameters</strong> (instead of using the default
parameters as we did so far). We will always receive the optimal output
given <strong>the specific test data chosen</strong>. This may lead to
overfitting the model on the chosen training and testing data. This can
be avoided by choosing different splits into testing and training data
and repeating the fit procedure. Doing different splits while preserving
the fraction of labels of each class in the original data, the method is
called the <strong>stratified shuffle split</strong>.
</p>
<p style="text-align: justify;">
We first need to import and instantiate the splitter. We set key word
argument <code>n_splits</code> to determine the number of different
splits. <code>test_size</code> lets us determine what fraction of
samples is used for the testing data.
</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> StratifiedShuffleSplit</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a>sss <span class="op">=</span> StratifiedShuffleSplit(random_state<span class="op">=</span>RANDOM_STATE, n_splits<span class="op">=</span><span class="dv">10</span>, test_size<span class="op">=</span><span class="fl">0.3</span>)</span></code></pre>
</div>
<p>Let us look at the different splits obtained:</p>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a>fig, ax <span class="op">=</span> subplots(figsize<span class="op">=</span>[<span class="dv">10</span>, <span class="dv">5</span>])</span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a>n_splits <span class="op">=</span> sss.n_splits</span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a>split_data_indices <span class="op">=</span> sss.split(X<span class="op">=</span>X, y<span class="op">=</span>y)</span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a><span class="cf">for</span> index, (tr, tt) <span class="kw">in</span> <span class="bu">enumerate</span>(split_data_indices):</span>
<span id="cb17-7"><a href="#cb17-7" tabindex="-1"></a>    indices <span class="op">=</span> X[:, feature_1].copy()</span>
<span id="cb17-8"><a href="#cb17-8" tabindex="-1"></a>    indices[tt] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb17-9"><a href="#cb17-9" tabindex="-1"></a>    indices[tr] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb17-10"><a href="#cb17-10" tabindex="-1"></a></span>
<span id="cb17-11"><a href="#cb17-11" tabindex="-1"></a>    <span class="co"># Visualize the results</span></span>
<span id="cb17-12"><a href="#cb17-12" tabindex="-1"></a>    x_axis <span class="op">=</span> arange(indices.size)</span>
<span id="cb17-13"><a href="#cb17-13" tabindex="-1"></a>    y_axis <span class="op">=</span> [index <span class="op">+</span> <span class="fl">.5</span>] <span class="op">*</span> indices.size</span>
<span id="cb17-14"><a href="#cb17-14" tabindex="-1"></a>    ax.scatter(x_axis, y_axis, c<span class="op">=</span>indices, marker<span class="op">=</span><span class="st">'_'</span>, lw<span class="op">=</span><span class="dv">10</span>, cmap<span class="op">=</span><span class="st">'coolwarm'</span>, vmin<span class="op">=-</span><span class="fl">.2</span>, vmax<span class="op">=</span><span class="fl">1.2</span>)</span>
<span id="cb17-15"><a href="#cb17-15" tabindex="-1"></a></span>
<span id="cb17-16"><a href="#cb17-16" tabindex="-1"></a><span class="co"># Plot the data classes and groups at the end</span></span>
<span id="cb17-17"><a href="#cb17-17" tabindex="-1"></a>class_y <span class="op">=</span> [index <span class="op">+</span> <span class="fl">1.5</span>] <span class="op">*</span> indices.size</span>
<span id="cb17-18"><a href="#cb17-18" tabindex="-1"></a>ax.scatter(x_axis, class_y, c<span class="op">=</span>y, marker<span class="op">=</span><span class="st">'_'</span>, lw<span class="op">=</span><span class="dv">10</span>, cmap<span class="op">=</span><span class="st">'coolwarm'</span>)</span>
<span id="cb17-19"><a href="#cb17-19" tabindex="-1"></a></span>
<span id="cb17-20"><a href="#cb17-20" tabindex="-1"></a><span class="co"># Formatting</span></span>
<span id="cb17-21"><a href="#cb17-21" tabindex="-1"></a>ylabels <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(n_splits))</span>
<span id="cb17-22"><a href="#cb17-22" tabindex="-1"></a>ylabels.extend([<span class="st">'Data'</span>])</span>
<span id="cb17-23"><a href="#cb17-23" tabindex="-1"></a></span>
<span id="cb17-24"><a href="#cb17-24" tabindex="-1"></a>ax.set_yticks(arange(n_splits <span class="op">+</span> <span class="dv">1</span>) <span class="op">+</span> <span class="fl">.5</span>)</span>
<span id="cb17-25"><a href="#cb17-25" tabindex="-1"></a>ax.set_yticklabels(ylabels)</span>
<span id="cb17-26"><a href="#cb17-26" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Sample index'</span>)</span>
<span id="cb17-27"><a href="#cb17-27" tabindex="-1"></a>ax.set_ylabel(<span class="st">'SSS iteration'</span>)<span class="op">;</span></span>
<span id="cb17-28"><a href="#cb17-28" tabindex="-1"></a></span>
<span id="cb17-29"><a href="#cb17-29" tabindex="-1"></a>show()</span></code></pre>
</div>
<figure><img src="../fig/02-improvement-rendered-unnamed-chunk-14-15.png" width="960" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p style="text-align: justify;">
By choosing n_splits=10, we obtained ten different splits that have
similarly distributed train and test data subsets from the original
data. The fraction of the data set aside for testing is 30 %. The
different splits cover the whole data set evenly. As such, using them
for training and testing will lead to a fairly unbiased average
performance.
</p>
<p>Let us look at the data in state space to check that the
classification task is now a real challenge.</p>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a>fig, ax <span class="op">=</span> subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">8</span>))</span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a><span class="cf">for</span> train_index, test_index <span class="kw">in</span> sss.split(X, y):</span>
<span id="cb18-4"><a href="#cb18-4" tabindex="-1"></a>    ax.scatter(X[train_index, <span class="dv">0</span>], X[train_index, <span class="dv">1</span>], c<span class="op">=</span>y[train_index], cmap<span class="op">=</span><span class="st">'Set1'</span>, s<span class="op">=</span><span class="dv">30</span>, marker<span class="op">=</span><span class="st">'^'</span>, alpha<span class="op">=</span><span class="fl">.5</span>)</span>
<span id="cb18-5"><a href="#cb18-5" tabindex="-1"></a>    ax.scatter(X[test_index, <span class="dv">0</span>], X[test_index, <span class="dv">1</span>], c<span class="op">=</span>y[test_index], cmap<span class="op">=</span><span class="st">'cool'</span>, s<span class="op">=</span><span class="dv">30</span>, alpha<span class="op">=</span><span class="fl">.5</span>, marker<span class="op">=</span><span class="st">'*'</span>, label<span class="op">=</span><span class="st">'Test'</span>)<span class="op">;</span></span>
<span id="cb18-6"><a href="#cb18-6" tabindex="-1"></a></span>
<span id="cb18-7"><a href="#cb18-7" tabindex="-1"></a>show()</span></code></pre>
</div>
<figure><img src="../fig/02-improvement-rendered-unnamed-chunk-15-17.png" width="768" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>These are the scatter plots of the training (magenta) and testing
(blue) data. Here are their distributions:</p>
<div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a>fig, ax <span class="op">=</span> subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">8</span>))</span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a><span class="cf">for</span> train_index, test_index <span class="kw">in</span> sss.split(X, y):</span>
<span id="cb19-4"><a href="#cb19-4" tabindex="-1"></a>    ax.hist(X[train_index], color<span class="op">=</span>[<span class="st">'magenta'</span>, <span class="st">'red'</span>], alpha<span class="op">=</span><span class="fl">.5</span>, histtype<span class="op">=</span><span class="st">'step'</span>)</span>
<span id="cb19-5"><a href="#cb19-5" tabindex="-1"></a>    ax.hist(X[test_index], color<span class="op">=</span>[<span class="st">'cyan'</span>, <span class="st">'blue'</span>], alpha<span class="op">=</span><span class="fl">.4</span>, histtype<span class="op">=</span><span class="st">'step'</span>)<span class="op">;</span></span>
<span id="cb19-6"><a href="#cb19-6" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" tabindex="-1"></a>show()</span></code></pre>
</div>
<figure><img src="../fig/02-improvement-rendered-unnamed-chunk-16-19.png" width="768" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p style="text-align: justify;">
The distributions differ in height because less data are in the testing
test. Otherwise they are similarly centred and spread. Using a number of
realisations (instead of just one) we expect to obtain a more accurate
and robust result of the training.
</p>
<p style="text-align: justify;">
We now train our classifiers on these different splits and obtain the
respective scores. They will give a robust measure of the classifier’s
performance given the data and avoid potential bias due to the selection
of specific test data.
</p>
<div class="codewrapper sourceCode" id="cb20">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a>X, y <span class="op">=</span> make_circles(n_samples<span class="op">=</span><span class="dv">1000</span>, factor<span class="op">=</span><span class="fl">.3</span>, noise<span class="op">=</span><span class="fl">.4</span>, random_state<span class="op">=</span>RANDOM_STATE)</span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" tabindex="-1"></a>score <span class="op">=</span> <span class="bu">list</span>()</span>
<span id="cb20-4"><a href="#cb20-4" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" tabindex="-1"></a><span class="cf">for</span> train_index, test_index <span class="kw">in</span> sss.split(X, y):</span>
<span id="cb20-6"><a href="#cb20-6" tabindex="-1"></a>    X_s, y_s <span class="op">=</span> X[train_index, :], y[train_index]</span>
<span id="cb20-7"><a href="#cb20-7" tabindex="-1"></a>    new_obs_s, y_test_s <span class="op">=</span> X[test_index, :], y[test_index]</span>
<span id="cb20-8"><a href="#cb20-8" tabindex="-1"></a></span>
<span id="cb20-9"><a href="#cb20-9" tabindex="-1"></a>    score_clf <span class="op">=</span> <span class="bu">list</span>()</span>
<span id="cb20-10"><a href="#cb20-10" tabindex="-1"></a></span>
<span id="cb20-11"><a href="#cb20-11" tabindex="-1"></a>    <span class="cf">for</span> name, clf <span class="kw">in</span> classifiers.items():</span>
<span id="cb20-12"><a href="#cb20-12" tabindex="-1"></a></span>
<span id="cb20-13"><a href="#cb20-13" tabindex="-1"></a>        clf.fit(X_s, y_s)</span>
<span id="cb20-14"><a href="#cb20-14" tabindex="-1"></a>        y_pred <span class="op">=</span> clf.predict(new_obs_s)</span>
<span id="cb20-15"><a href="#cb20-15" tabindex="-1"></a>        score_clf.append(clf.score(new_obs_s, y_test_s))</span>
<span id="cb20-16"><a href="#cb20-16" tabindex="-1"></a></span>
<span id="cb20-17"><a href="#cb20-17" tabindex="-1"></a>    score.append(score_clf)</span>
<span id="cb20-18"><a href="#cb20-18" tabindex="-1"></a></span>
<span id="cb20-19"><a href="#cb20-19" tabindex="-1"></a>score_mean <span class="op">=</span> mean(score, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb20-20"><a href="#cb20-20" tabindex="-1"></a></span>
<span id="cb20-21"><a href="#cb20-21" tabindex="-1"></a>bins <span class="op">=</span> arange(<span class="bu">len</span>(score_mean))</span>
<span id="cb20-22"><a href="#cb20-22" tabindex="-1"></a></span>
<span id="cb20-23"><a href="#cb20-23" tabindex="-1"></a>fig, ax <span class="op">=</span> subplots()</span>
<span id="cb20-24"><a href="#cb20-24" tabindex="-1"></a></span>
<span id="cb20-25"><a href="#cb20-25" tabindex="-1"></a>ax.bar(bins, score_mean)<span class="op">;</span></span>
<span id="cb20-26"><a href="#cb20-26" tabindex="-1"></a>ax.set_xticks(arange(<span class="dv">0</span>,<span class="dv">8</span>)<span class="op">+</span><span class="fl">0.4</span>)</span>
<span id="cb20-27"><a href="#cb20-27" tabindex="-1"></a>ax.set_xticklabels(classifiers.keys(), rotation<span class="op">=-</span><span class="dv">70</span>)<span class="op">;</span></span>
<span id="cb20-28"><a href="#cb20-28" tabindex="-1"></a></span>
<span id="cb20-29"><a href="#cb20-29" tabindex="-1"></a>show()</span>
<span id="cb20-30"><a href="#cb20-30" tabindex="-1"></a></span>
<span id="cb20-31"><a href="#cb20-31" tabindex="-1"></a><span class="bu">print</span>(classifiers.keys())</span>
<span id="cb20-32"><a href="#cb20-32" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Average scores: '</span>)</span>
<span id="cb20-33"><a href="#cb20-33" tabindex="-1"></a><span class="bu">print</span>([<span class="st">"</span><span class="sc">{0:0.2f}</span><span class="st">"</span>.<span class="bu">format</span>(ind) <span class="cf">for</span> ind <span class="kw">in</span> score_mean])</span></code></pre>
</div>
<style>#sk-container-id-1 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: #000;
  --sklearn-color-text-muted: #666;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-1 {
  color: var(--sklearn-color-text);
}

#sk-container-id-1 pre {
  padding: 0;
}

#sk-container-id-1 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-1 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-1 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-1 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-1 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-1 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-1 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-1 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-1 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-1 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-1 label.sk-toggleable__label {
  cursor: pointer;
  display: flex;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
  align-items: start;
  justify-content: space-between;
  gap: 0.5em;
}

#sk-container-id-1 label.sk-toggleable__label .caption {
  font-size: 0.6rem;
  font-weight: lighter;
  color: var(--sklearn-color-text-muted);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-1 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-1 div.sk-label label.sk-toggleable__label,
#sk-container-id-1 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-1 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-1 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-1 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-1 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 0.5em;
  text-align: center;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-1 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-1 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-1 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-1 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-1" class="sk-top-container">
<div class="sk-text-repr-fallback">
<pre>MLPClassifier(max_iter=5000, random_state=1234)</pre>
<b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b>
</div>
<div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator fitted sk-toggleable">
<input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked><label for="sk-estimator-id-1" class="sk-toggleable__label fitted sk-toggleable__label-arrow"><div><div>MLPClassifier</div></div>
<div>
<a class="external-link sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.6/modules/generated/sklearn.neural_network.MLPClassifier.html">?<span>Documentation for MLPClassifier</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span>
</div></label><div class="sk-toggleable__content fitted"><pre>MLPClassifier(max_iter=5000, random_state=1234)</pre></div> </div></div></div>
</div>
<figure><img src="../fig/02-improvement-rendered-unnamed-chunk-17-21.png" width="672" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>dict_keys(['Random Forest', 'AdaBoost (Random Forest)', 'Extra Trees', 'AdaBoost (Extra Tree)', 'Decision Tree', 'SVC (RBF)', 'SVC (Linear)', 'Multi-layer Perceptron'])
Average scores:
['0.76', '0.76', '0.75', '0.75', '0.70', '0.79', '0.50', '0.78']</code></pre>
</div>
<p>The result is the average score for the ten splits performed. All
results for the noise-contaminated data are now in the seventies.</p>
<p style="text-align: justify;">
This is still good given the quality of the data. It appears that the
<em>decision tree</em> classifier gives the lowest result for this kind
of problem, <em>SVC (RBF)</em> scores highest. We have to keep in mind,
however, that we are using the classifiers with their default settings.
We will later use variation of the so-called hyperparameters to further
improve the classification score.
</p>
<p>Here we have used a for loop to train and test on each of the
different splits of the data. SciKit Learn also contains functions that
take the stratified shuffle split as an argument,
e.g. <code>permutation_test_score</code>. In that case, the splits do
not need to be done separately.</p>
<p>We have now reached a point where we can trust to have a robust and
unbiased outcome of the training. Let us now look at more refined ways
to quantify the result.</p>
</section><section><h2 class="section-heading" id="evaluation-roc-and-auc">Evaluation: ROC and AUC<a class="anchor" aria-label="anchor" href="#evaluation-roc-and-auc"></a></h2>
<hr class="half-width"><p style="text-align: justify;">
There are various measures that may be used to evaluate the performance
of a machine learning model. Such measures look at different
characteristics, including the goodness of fit and generalisability of a
model. Evaluation measures used with regards to classification models
include, but are not limited to:
</p>
<ul><li>Receiver Operation Characteristic (ROC) and Area Under the Curve
(AUC) - for binary classifiers.</li>
<li>Accuracy</li>
<li>Precision</li>
<li>Recall</li>
</ul><p style="text-align: justify;">
There are many other metrics that, depending on the problem, we may use
to evaluate a machine learning model. Please see <a href="https://scikit-learn.org/stable/modules/model_evaluation.html" class="external-link">the
official documentation</a> for additional information on these measures
and their implementation in scikit-learn.
</p>
<p>The quantities we are going to look at are the <strong>Receiver
Operation Characteristic (ROC)</strong> and the <strong>Area Under the
Curve (AUC)</strong>.</p>
<p style="text-align: justify;">
A receiver operation characteristic, often referred to as the
<strong>ROC curve</strong>, is a visualisation of the discrimination
threshold in a binary classification model. It illustrates the rate of
true positives (TPR) against the rate of false positives (FPR) at
different thresholds. The aforementioned rates are essentially defined
as:
</p>
<ul><li>True Positive Rate (TPR): the sensitivity of the model</li>
<li>False Positive Rate (FPR): one minus the specificity of the
model</li>
</ul><p>This makes ROC a measure of sensitivity versus specificity.</p>
<p style="text-align: justify;">
The area under the ROC curve, often referred to as AUC, reduces the
information contained within a ROC curve down to a value between 0 and
1, with 1 being a perfect fit. An AUC value of 0.5 represents any random
guess, and values below demonstrate a performance that’s even worse than
a lucky guess!
</p>
<div id="discussion3" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Discussion</h3>
<div class="callout-content">
<p style="text-align: justify;">
<code>scikit-learn</code> includes specialist functions called
<code>roc_curve</code> and <code>roc_auc_score</code> to obtain ROC (FPR
and TPR values for visualisation) and AUC respectively. Both functions
receive as input arguments the test labels (i.e. <code>y_test</code>)
and the score (probability) associated with each prediction. We obtain
the latter measure using one of the following two techniques:
</p>
<ul><li>Decision function: where classification models have a
<code>.decision_function</code> method that provides us with score
associated with each label.</li>
<li>Probability: where classification models have a
<code>.predict_proba</code> method that provides us with the probability
associated with each prediction (we used it in the Classification
Introduction lesson). In this case, however, the results are provided in
the form of a two-dimensional array where columns represent different
labels (as defined in property). Given that we will plot ROC curves for
binary problems (two labels), we only pick one of these columns.
Usually, the second column (the feature representing <code>True</code>
or <strong>1</strong>) is the one to choose. However, if you notice that
the results are unexpectedly bad, you may try the other column just be
sure.</li>
</ul></div>
</div>
</div>
<p style="text-align: justify;">
We can see that our classifiers now reach different degrees of
prediction. The degree can be quantified by the <strong>Area Under the
Curve (AUC)</strong>. It refers to the area between the blue ROC curve
and the orange diagonal. The area under the ROC curve, often referred to
as AUC, reduces the information contained within a ROC curve down to a
value between and 0 and 1, with 1 being a perfect fit. An AUC value of
0.5 represents a random guess, and values below the diagonal demonstrate
a performance that’s even worse than a guess!
</p>
<p style="text-align: justify;">
scikit-learn includes specialist functions called <code>roc_curve</code>
and <code>roc_auc_score</code> to obtain ROC (FPR and TPR values for
visualisation) and AUC respectively. Both function receive as input
arguments the test labels (i.e. y_score) and the score (probability)
associated with each prediction. We obtain the latter measure using one
of the following two techniques:
</p>
<ul><li>Decision function: where classification models have a
<code>.decision_function</code> method that provides us with a score
associated with each label.</li>
<li>Probability: where classification models have a
<code>predict_proba_</code> method that provides us with the probability
associated with each prediction. In this case, however, the results are
provided in the form of a two-dimensional array where columns represents
different labels (as defined in <code>.classes</code> property). Given
that we only plot ROC curves for binary problems, we should only use one
of these columns. Usually, the second column (the feature representing
<code>True</code> or <strong>1</strong>) is the one to choose. However,
if you notice that the results are unexpectedly bad, you may try the
other column just be sure.</li>
</ul><div class="codewrapper sourceCode" id="cb22">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> roc_curve, roc_auc_score</span>
<span id="cb22-2"><a href="#cb22-2" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" tabindex="-1"></a>fig, all_axes <span class="op">=</span> subplots(figsize<span class="op">=</span>[<span class="dv">15</span>, <span class="dv">10</span>], ncols<span class="op">=</span><span class="dv">4</span>, nrows<span class="op">=</span><span class="dv">2</span>, sharey<span class="op">=</span><span class="va">True</span>, sharex<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb22-4"><a href="#cb22-4" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" tabindex="-1"></a><span class="cf">for</span> ax, (name, clf) <span class="kw">in</span> <span class="bu">zip</span>(all_axes.ravel(), classifiers.items()):</span>
<span id="cb22-6"><a href="#cb22-6" tabindex="-1"></a>    clf.fit(X_train, y_train)</span>
<span id="cb22-7"><a href="#cb22-7" tabindex="-1"></a></span>
<span id="cb22-8"><a href="#cb22-8" tabindex="-1"></a>    <span class="co"># Checking whether or not the object has `decision_function`:</span></span>
<span id="cb22-9"><a href="#cb22-9" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">hasattr</span>(clf, <span class="st">'decision_function'</span>):</span>
<span id="cb22-10"><a href="#cb22-10" tabindex="-1"></a>        <span class="co"># If it does:</span></span>
<span id="cb22-11"><a href="#cb22-11" tabindex="-1"></a>        y_score <span class="op">=</span> clf.decision_function(X_test)</span>
<span id="cb22-12"><a href="#cb22-12" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb22-13"><a href="#cb22-13" tabindex="-1"></a>        <span class="co"># Otherwise:</span></span>
<span id="cb22-14"><a href="#cb22-14" tabindex="-1"></a>        y_score <span class="op">=</span> clf.predict_proba(X_test)[:, feature_2]  <span class="co"># We only need one column.</span></span>
<span id="cb22-15"><a href="#cb22-15" tabindex="-1"></a></span>
<span id="cb22-16"><a href="#cb22-16" tabindex="-1"></a>    <span class="co"># Obtaining the x- and y-axis values for the ROC curve:</span></span>
<span id="cb22-17"><a href="#cb22-17" tabindex="-1"></a>    fpr, tpr, thresh <span class="op">=</span> roc_curve(y_test, y_score)</span>
<span id="cb22-18"><a href="#cb22-18" tabindex="-1"></a></span>
<span id="cb22-19"><a href="#cb22-19" tabindex="-1"></a>    <span class="co"># Obtaining the AUC value:</span></span>
<span id="cb22-20"><a href="#cb22-20" tabindex="-1"></a>    roc_auc <span class="op">=</span> roc_auc_score(y_test, y_score)</span>
<span id="cb22-21"><a href="#cb22-21" tabindex="-1"></a></span>
<span id="cb22-22"><a href="#cb22-22" tabindex="-1"></a>    ax.plot(fpr, tpr, lw<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb22-23"><a href="#cb22-23" tabindex="-1"></a>    ax.plot([<span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">0</span>, <span class="dv">1</span>], lw<span class="op">=</span><span class="dv">1</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb22-24"><a href="#cb22-24" tabindex="-1"></a></span>
<span id="cb22-25"><a href="#cb22-25" tabindex="-1"></a>    ax.set_xlabel(<span class="st">'False Positive Rate'</span>)</span>
<span id="cb22-26"><a href="#cb22-26" tabindex="-1"></a>    ax.set_ylabel(<span class="st">'True Positive Rate'</span>)</span>
<span id="cb22-27"><a href="#cb22-27" tabindex="-1"></a></span>
<span id="cb22-28"><a href="#cb22-28" tabindex="-1"></a>    label <span class="op">=</span> <span class="st">'</span><span class="sc">{}</span><span class="st"> - AUC: </span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(name, roc_auc)</span>
<span id="cb22-29"><a href="#cb22-29" tabindex="-1"></a>    ax.set_title(label, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb22-30"><a href="#cb22-30" tabindex="-1"></a></span>
<span id="cb22-31"><a href="#cb22-31" tabindex="-1"></a>show()</span></code></pre>
</div>
<figure><img src="../fig/02-improvement-rendered-unnamed-chunk-18-23.png" width="1440" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>The (orange) diagonal represents predictions of the two labels by a
coin toss. To be of value the classifier must reach a ROC curve above
the diagonal.</p>
<p style="text-align: justify;">
This concludes our first steps into classification with scikit-learn.
There are many more aspects of classification. From a practical point of
view, <a href="https://scikit-learn.org/stable/modules/preprocessing.html" class="external-link">data
normalisation</a> and <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.permutation_test_score.html" class="external-link">permutation
test score</a> as well as the workflow report are important. These will
be the topics of our next lesson.
</p>
<p><br></p>
</section><section><h2 class="section-heading" id="exercises">Exercises<a class="anchor" aria-label="anchor" href="#exercises"></a></h2>
<hr class="half-width"><div id="end-of-chapter-exercises" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="end-of-chapter-exercises" class="callout-inner">
<h3 class="callout-title">End of chapter Exercises</h3>
<div class="callout-content">
<p>Take the torus-within-a-torus data generator from the
<strong>Challenge</strong> above.</p>
<ol style="list-style-type: decimal"><li>Create a 3-feature dataset with the <code>make_torus_3D()</code>
function using the following properties:</li>
</ol><ul><li>2000 samples</li>
<li>A factor of 0.8</li>
<li>A noise level of 0.3</li>
</ul><ol start="2" style="list-style-type: decimal"><li>Create a 3-D scatter plot of the newly generated dataset.</li>
</ol><p><em>(Optional: Play around with factor and noise to observe how they
can make the dataset more or less complex)</em>.</p>
<p>3a. Using the classifiers given above, train and score each
model:</p>
<ul><li>Use the stratified-shuffle-split method:
<ul><li>Set the function to generate 5 sets of test/train splits</li>
<li>Set the test size to 0.3 (30%)</li>
</ul></li>
<li>Calculate and store the average score for each classifier</li>
</ul><p>3b. Plot the average score for all classifiers. - What is the best
performing classifier?</p>
<ol start="4" style="list-style-type: decimal"><li>Select the best-performing model from the previous question and
store it as a new instance:</li>
</ol><ul><li>Plot the feature importances obtained from your chosen model to see
how each feature contriutes to the outcome.</li>
</ul><p><em>(Hint: You’ll need to intialise and fit the model again. You can
use the whole dataset for this)</em>.</p>
<ol start="5" style="list-style-type: decimal"><li>OPTIONAL: Explore how noise affects the accuracy of a
classifier.</li>
</ol><ul><li><p>Generate three datasets with noise levels of <strong>0.05, 0.2,
0.6</strong> using <code>make_torus_3D()</code>. Keep the number of
samples and factor the same as in <strong>Question 1</strong>.</p></li>
<li>
<p>Create an ROC AUC plot for each dataset using the same-best
performing model you used in <strong>Question 4</strong> as the model to
train and predict with.</p>
<ul><li>Use the <code>train_test_split()</code> function from scikit-learn
to split your data.</li>
<li><em>Look to the lesson for an example of how to generate ROC curve
plots.</em></li>
</ul></li>
</ul><p><em>(Hint! When calling a for loop on your datasets you will need to
nest them inside a list).</em></p>
<p>Observe and comment on how noise affects the performance of the
classifiers in terms of True and False positives.</p>
<div class="section level2">
<h2 id="recommendation">Recommendation<a class="anchor" aria-label="anchor" href="#recommendation"></a></h2>
<p style="text-align: justify;">
Pick any of the provided (or other) data sets with labels to repeat the
above. Feel free to try and do any testing or plotting that you find
important. This is not an assignment to get the correct answer. Rather
at this stage, we practise to use functionality from scikit-learn to
search for structure in the data that helps to achieve the best
predictions possible.
</p>
</div>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">
  Solutions are provided after assignments are marked.
  </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">

</div>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul><li>Different classification algorithms approach problems
differently.</li>
<li>
<code>train_test_split</code> function tries to preserve the ratio
of labels in the split</li>
<li>Increasing the level of noise in the data makes the task more
complicated.</li>
<li>The potential bias due to splitting could be avoid using stratified
shuffle split.</li>
<li>
<code>StratifiedShuffleSplit</code> is a method that uses
<code>n_splits</code> and <code>test_size</code> parameters.</li>
</ul></div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</section></div> <!-- / div.lesson-content -->
    </main><!-- / main#main-content.main-content --><nav class="bottom-pagination mx-md-4" aria-label="Previous and Next Chapter"><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="../instructor/01-classification_intro.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="../instructor/03-refinement.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="../instructor/01-classification_intro.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Classification
        </a>
        <a class="chapter-link float-end" href="../instructor/03-refinement.html" rel="next">
          Next: Refinement...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
    </nav></div> <!-- / div.primary-content.col-xs-12 -->
<!-- END:   inst/pkgdown/templates/content-instructor.html-->

      </div><!--/div.row-->
      		<footer class="row footer mx-md-3"><hr><div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/carpentries/workbench-template-md/edit/main/episodes/02-improvement.Rmd" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/carpentries/workbench-template-md/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/carpentries/workbench-template-md/" class="external-link">Source</a></p>
				<p><a href="https://github.com/carpentries/workbench-template-md/blob/main/CITATION" class="external-link">Cite</a> | <a href="mailto:team@carpentries.org">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/69aaefee46a17928e2a1694825599e169b9793e3" class="external-link">sandpaper (0.16.10)</a>, <a href="https://github.com/carpentries/pegboard/tree/bad0be19a12f0c6545801b276ddf26c945f8bfd1" class="external-link">pegboard (0.7.7)</a>, and <a href="https://github.com/milanmlft/varnish/tree/milanmlft/sticky-sidebar" class="external-link">varnish (1.0.3.9000)</a></p>
			</div>
		</footer></div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "TrainingMaterial",
  "@id": "https://carpentries.github.io/workbench-template-md/instructor/02-improvement.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries",
  "name": "Improvement",
  "creativeWorkStatus": "active",
  "url": "https://carpentries.github.io/workbench-template-md/instructor/02-improvement.html",
  "identifier": "https://carpentries.github.io/workbench-template-md/instructor/02-improvement.html",
  "dateCreated": "2022-07-05",
  "dateModified": "2025-03-31",
  "datePublished": "2025-03-31"
}

  </script><script>
		feather.replace();
	</script></body></html><!-- END:   inst/pkgdown/templates/layout.html-->

